{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd903cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T19:19:41.935728Z",
     "start_time": "2022-05-03T19:18:42.624305Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from itertools import compress\n",
    "\n",
    "import librosa\n",
    "from scipy.fftpack import fft\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import keras\n",
    "from keras import optimizers, losses, activations, models\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, Bidirectional, Conv1D, GRU\n",
    "from tensorflow.keras.layers import Input, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "\n",
    "random.seed(0)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams[\"figure.figsize\"] = 9,6\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c98eb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T19:19:41.953344Z",
     "start_time": "2022-05-03T19:19:41.940535Z"
    }
   },
   "outputs": [],
   "source": [
    "# parameters settings\n",
    "data_path = '../../data/train/audio'\n",
    "labels = os.listdir(data_path)\n",
    "CLASSES = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go'] \n",
    "SAMPLING_RATE = 16000\n",
    "NOISE_RATIO = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1846a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T19:19:41.977535Z",
     "start_time": "2022-05-03T19:19:41.957550Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad_audio(samples):\n",
    "    return np.pad(samples, pad_width=(SAMPLING_RATE - len(samples), 0), mode='constant', constant_values=(0, 0))\n",
    "\n",
    "\n",
    "def chop_audio(samples):\n",
    "    return samples[:SAMPLING_RATE]\n",
    "\n",
    "\n",
    "def split_audio(sample_rate, samples):\n",
    "    \"\"\" Splits audio file to multiple with (up to) fixed 1s length \"\"\"\n",
    "    duration = float(len(samples)/sample_rate)\n",
    "    n_samples = math.ceil(duration)\n",
    "    return np.array_split(samples, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f417dec7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T19:19:41.986466Z",
     "start_time": "2022-05-03T19:19:41.979519Z"
    }
   },
   "outputs": [],
   "source": [
    "def assign_unknown_label(labels, classes=CLASSES):\n",
    "    new_labels = []\n",
    "    for label in labels:\n",
    "        if label not in classes:\n",
    "            new_labels.append('unknown')\n",
    "        else:\n",
    "            new_labels.append(label)\n",
    "    return new_labels\n",
    "\n",
    "def encode_labels(labels):\n",
    "    return pd.get_dummies(pd.Series(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5733564b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T19:19:42.010772Z",
     "start_time": "2022-05-03T19:19:41.988447Z"
    }
   },
   "outputs": [],
   "source": [
    "def specgram(audio, sample_rate, window_size=20, step_size=10, eps=1e-10):\n",
    "    nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "    noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "    _, _, spec = signal.spectrogram(audio,\n",
    "                                    fs=sample_rate,\n",
    "                                    window='hann',\n",
    "                                    nperseg=nperseg,\n",
    "                                    noverlap=noverlap,\n",
    "                                    detrend=False)\n",
    "    return np.log(spec.T.astype(np.float32) + eps)\n",
    "\n",
    "\n",
    "def plot_model_history(history, title: str) -> None:\n",
    "    \"\"\"\n",
    "    Plotting the learning curve of Keras model, broken down into loss curve and accuracy curve, \n",
    "    for both training and validation data.\n",
    "\n",
    "    Args:\n",
    "        history : Object returned by the .fit method of Keras model.\n",
    "        title (str): Title of the plots.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1,2,figsize=(15,5)) \n",
    "\n",
    "    axs[0].plot(history.history['accuracy']) \n",
    "    axs[0].plot(history.history['val_accuracy']) \n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy') \n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].legend(['train', 'validate'], loc='upper left')\n",
    "\n",
    "    axs[1].plot(history.history['loss']) \n",
    "    axs[1].plot(history.history['val_loss']) \n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss') \n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].legend(['train', 'validate'], loc='upper left')\n",
    "    fig.suptitle(title, size=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46539ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T19:19:42.029737Z",
     "start_time": "2022-05-03T19:19:42.012906Z"
    }
   },
   "outputs": [],
   "source": [
    "def rnn_network(input_dim, output_dim, rnn_layer, optimizer):\n",
    "\n",
    "    input_data = Input(name='input', shape=input_dim, dtype='float32')\n",
    "\n",
    "    x = Conv1D(filters=256, kernel_size=10, strides=4, name='conv1d')(input_data)\n",
    "    x = BatchNormalization(name='b_norm')(x)\n",
    "    x = Activation('relu', name='activation')(x)\n",
    "    x = Dropout(DROPOUT, name='dropout_1')(x)\n",
    "    x = rnn_layer(128, activation='relu', return_sequences=True, dropout=DROPOUT, name='rnn_1')(x)\n",
    "    x = rnn_layer(128, activation='relu', return_sequences=False, dropout=DROPOUT, name='rnn_2')(x)\n",
    "    x = Dense(units=64, activation='relu', name='dense')(x)\n",
    "    x = Dropout(DROPOUT, name='dropout_2')(x)\n",
    "\n",
    "    output_data = Dense(units=output_dim, activation='softmax', name='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=input_data, outputs=output_data, name=str(rnn_layer).split(\".\")[-1].split(\"'\")[0])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a196fc1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T19:19:42.055156Z",
     "start_time": "2022-05-03T19:19:42.031955Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_heatmaps(models_results: list, title: str) -> None:\n",
    "    \"\"\"\n",
    "    Plot heatmaps based on the confusion matrices of a given model (basic version and with 2 different augmentations).)\n",
    "\n",
    "    Args:\n",
    "        models_results (list): List of confusion matrices of the evaluated models.\n",
    "        title (str): Title of the plots.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1,3,figsize=(18,6))\n",
    "    models = ['SimpleRNN', 'LSTM', 'GRU']\n",
    "    for i in range(3):\n",
    "        sns.heatmap(models_results[i], center=0.5, annot=True, cmap=\"coolwarm\", linewidths=1, linecolor='black', ax=axes[i], annot_kws={\"size\": 12})\n",
    "        axes[i].set_title(models[i], size=15)\n",
    "        cax = plt.gcf().axes[-1]\n",
    "        cax.tick_params(labelsize=12)\n",
    "\n",
    "    fig.suptitle(title, size=18)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_confusion_matr(models_results: list, title: str) -> None:\n",
    "    \"\"\"\n",
    "    Plot heatmaps based on the confusion matrices of a given model (basic version and with 2 different augmentations).)\n",
    "\n",
    "    Args:\n",
    "        models_results (list): List of confusion matrices of the evaluated models.\n",
    "        title (str): Title of the plots.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1,3,figsize=(18,6))\n",
    "    models = ['SimpleRNN', 'LSTM', 'GRU']\n",
    "    for i in range(3):\n",
    "        sns.heatmap(models_results[i], annot=True, cmap=\"Blues\", linewidths=1, linecolor='black', ax=axes[i], fmt='d', annot_kws={\"size\": 12})\n",
    "        axes[i].set_title(models[i], size=15)\n",
    "        axes[i].set_xticklabels(axes[i].get_xmajorticklabels(), fontsize = 12)\n",
    "        axes[i].set_yticklabels(axes[i].get_xmajorticklabels(), fontsize = 12)\n",
    "        cax = plt.gcf().axes[-1]\n",
    "        cax.tick_params(labelsize=12)\n",
    "\n",
    "    fig.suptitle(title, size=18)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed72d5e9",
   "metadata": {},
   "source": [
    "# <span style='font-family:Georgia'> 1. Data loading & preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb3636c",
   "metadata": {},
   "source": [
    "## <span style='font-family:Georgia'> 1.1. Noisy data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb8c4e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T19:19:43.194386Z",
     "start_time": "2022-05-03T19:19:42.059166Z"
    }
   },
   "outputs": [],
   "source": [
    "label = '_background_noise_'\n",
    "files = [f for f in os.listdir(data_path + '/'+ label) if f.endswith('.wav')]\n",
    "one_sec_background_noise_specgrams = []\n",
    "\n",
    "for file in tqdm(files):\n",
    "    _, samples = wavfile.read(data_path + \"/\" + label + \"/\" + file)\n",
    "    duration = float(len(samples)/SAMPLING_RATE)\n",
    "\n",
    "    # Do not distinguish between noise classes\n",
    "    one_sec_background_noise = split_audio(\n",
    "            sample_rate=SAMPLING_RATE, samples=samples\n",
    "        )\n",
    "    \n",
    "    for item in one_sec_background_noise:\n",
    "        duration = float(len(item)/SAMPLING_RATE)\n",
    "        if duration < 1: item = pad_audio(item)        \n",
    "        \n",
    "        one_sec_background_noise_specgrams.append(\n",
    "            specgram(item, SAMPLING_RATE)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6aec04",
   "metadata": {},
   "source": [
    "## <span style='font-family:Georgia'> 1.2. Noise-free data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec2f8a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T19:19:43.253189Z",
     "start_time": "2022-05-03T19:19:43.198723Z"
    }
   },
   "outputs": [],
   "source": [
    "validation_list = pd.read_csv('../../data/train/validation_list.txt', sep=\"\\t\", header=None)[0].tolist()\n",
    "testing_list = pd.read_csv('../../data/train/testing_list.txt', sep=\"\\t\", header=None)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c70ae40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T19:19:43.276754Z",
     "start_time": "2022-05-03T19:19:43.255173Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Training: ', 64721 - len(validation_list) - len(testing_list))\n",
    "print('Validation: ', len(validation_list))\n",
    "print('Testing: ', len(testing_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006bce66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T19:47:00.606450Z",
     "start_time": "2022-05-03T19:19:43.279701Z"
    }
   },
   "outputs": [],
   "source": [
    "train_labels = []\n",
    "train_specgrams = []\n",
    "val_labels = []\n",
    "val_specgrams = []\n",
    "test_labels = []\n",
    "test_specgrams = []\n",
    "\n",
    "for label in tqdm([l for l in labels if l != '_background_noise_']):\n",
    "\n",
    "    files = [f for f in os.listdir(data_path + '/'+ label) if f.endswith('.wav')]\n",
    "    \n",
    "    for file in tqdm(files):\n",
    "        _, samples = wavfile.read(data_path + \"/\" + label + \"/\" + file)\n",
    "        duration = float(len(samples)/SAMPLING_RATE)\n",
    "                \n",
    "        if duration < 1: samples = pad_audio(samples)\n",
    "            \n",
    "        if (label + \"/\" + file) in validation_list: \n",
    "            val_labels.append(label)\n",
    "            val_specgrams.append(specgram(samples, SAMPLING_RATE))\n",
    "        elif (label + \"/\" + file) in testing_list: \n",
    "            test_labels.append(label)\n",
    "            test_specgrams.append(specgram(samples, SAMPLING_RATE))\n",
    "        else:\n",
    "            train_labels.append(label)\n",
    "            train_specgrams.append(specgram(samples, SAMPLING_RATE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aff870",
   "metadata": {},
   "source": [
    "## <span style='font-family:Georgia'> 1.3. Data labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a960c2e",
   "metadata": {},
   "source": [
    "### <span style='font-family:Georgia'> 1.3.1. Noise-free data split (for hyperparameters tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a049be4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T19:47:02.736765Z",
     "start_time": "2022-05-03T19:47:00.608443Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = np.array(train_specgrams)\n",
    "X_val = np.array(val_specgrams)\n",
    "X_test = np.array(test_specgrams)\n",
    "\n",
    "Y_train = assign_unknown_label(train_labels)\n",
    "Y_val = assign_unknown_label(val_labels)\n",
    "Y_test = assign_unknown_label(test_labels)\n",
    "\n",
    "Y_train = np.array(encode_labels(Y_train).values)\n",
    "Y_val = np.array(encode_labels(Y_val).values)\n",
    "Y_test = np.array(encode_labels(Y_test).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f68893e",
   "metadata": {},
   "source": [
    "### <span style='font-family:Georgia'> 1.3.2. Approach 1 - taking `background_noise` observations as separate `silence` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0360d3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T19:47:02.836828Z",
     "start_time": "2022-05-03T19:47:02.736765Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split noise into 3 disjoint subsets (train, valid & test sets)\n",
    "N = 64721\n",
    "valid_ratio = np.round(len(validation_list)/N, 2)\n",
    "test_ratio = np.round(len(testing_list)/N, 2)\n",
    "train_ratio = 1 - (valid_ratio+test_ratio)\n",
    "\n",
    "sets_list = [\"train\", \"val\", \"test\"]\n",
    "obs_split = np.array(random.choices(sets_list, weights=(train_ratio, valid_ratio, test_ratio), k=N))\n",
    "train_mask, val_mask, test_mask = obs_split == \"train\", obs_split == \"val\", obs_split == \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3001513",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T19:47:02.853091Z",
     "start_time": "2022-05-03T19:47:02.836828Z"
    }
   },
   "outputs": [],
   "source": [
    "train_noise = list(compress(one_sec_background_noise_specgrams, train_mask))\n",
    "print(f'Number of observations assigned to train set: {len(train_noise)}')\n",
    "\n",
    "val_noise = list(compress(one_sec_background_noise_specgrams, val_mask))\n",
    "print(f'Number of observations assigned to validation set: {len(val_noise)}')\n",
    "\n",
    "test_noise = list(compress(one_sec_background_noise_specgrams, test_mask))\n",
    "print(f'Number of observations assigned to test set: {len(test_noise)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc35b15d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T19:47:02.878816Z",
     "start_time": "2022-05-03T19:47:02.856082Z"
    }
   },
   "outputs": [],
   "source": [
    "train_specgrams_1 = train_specgrams + train_noise\n",
    "train_labels_1 = train_labels + ['silence' for i in range(len(train_noise))]\n",
    "\n",
    "val_specgrams_1 = val_specgrams + val_noise\n",
    "val_labels_1 = val_labels + ['silence' for i in range(len(val_noise))]\n",
    "\n",
    "test_specgrams_1 = test_specgrams + test_noise\n",
    "test_labels_1 = test_labels + ['silence' for i in range(len(test_noise))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b75257",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T19:47:33.027251Z",
     "start_time": "2022-05-03T19:47:02.878816Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train_1 = np.array(train_specgrams_1)\n",
    "print(x_train_1.shape)\n",
    "\n",
    "x_val_1 = np.array(val_specgrams_1)\n",
    "print(x_val_1.shape)\n",
    "\n",
    "x_test_1 = np.array(test_specgrams_1)\n",
    "print(x_test_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b225be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T19:47:34.102375Z",
     "start_time": "2022-05-03T19:47:33.048198Z"
    }
   },
   "outputs": [],
   "source": [
    "classes=CLASSES+['silence']\n",
    "\n",
    "y_train_1 = assign_unknown_label(train_labels_1, classes=classes)\n",
    "print(np.unique(y_train_1))\n",
    "\n",
    "y_val_1 = assign_unknown_label(val_labels_1, classes=classes)\n",
    "print(np.unique(y_val_1))\n",
    "\n",
    "y_test_1 = assign_unknown_label(test_labels_1, classes=classes)\n",
    "print(np.unique(y_test_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4f9ab0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T19:47:36.867031Z",
     "start_time": "2022-05-03T19:47:34.112348Z"
    }
   },
   "outputs": [],
   "source": [
    "del train_specgrams_1, val_specgrams_1, test_specgrams_1\n",
    "del train_labels_1, val_labels_1, test_labels_1\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049945e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T19:47:37.030910Z",
     "start_time": "2022-05-03T19:47:36.867031Z"
    }
   },
   "outputs": [],
   "source": [
    "# Labels encoding\n",
    "y_train_1 = np.array(encode_labels(y_train_1).values)\n",
    "y_val_1 = np.array(encode_labels(y_val_1).values)\n",
    "y_test_1 = np.array(encode_labels(y_test_1).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0703d383",
   "metadata": {},
   "source": [
    "### <span style='font-family:Georgia'> 1.3.2. Approach 2 - noising the training subset(s) with the 'background_noise' class observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edab460",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T19:47:37.361667Z",
     "start_time": "2022-05-03T19:47:37.038890Z"
    }
   },
   "outputs": [],
   "source": [
    "train_labels_summary = pd.Series(train_labels).value_counts()\n",
    "n = math.floor(NOISE_RATIO*train_labels_summary.sum())\n",
    "noised_labels = random.choices(train_labels, k=n)\n",
    "\n",
    "summary = pd.DataFrame(train_labels_summary, columns=[\"obs_cnt\"])\n",
    "summary[\"noise_cnt\"] = pd.Series(noised_labels).value_counts()\n",
    "summary[\"noise_ratio\"] = summary[\"noise_cnt\"] / summary[\"obs_cnt\"]\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5527ab01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T19:47:37.392908Z",
     "start_time": "2022-05-03T19:47:37.361667Z"
    }
   },
   "outputs": [],
   "source": [
    "train_specgrams_2 = train_specgrams + random.choices(one_sec_background_noise_specgrams, k=n)\n",
    "train_labels_2 = train_labels + noised_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0baf74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T19:48:07.364291Z",
     "start_time": "2022-05-03T19:47:37.392908Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train_2 = np.array(train_specgrams_2)\n",
    "print(x_train_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2075ba6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T19:48:09.523031Z",
     "start_time": "2022-05-03T19:48:07.642063Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train_2 = assign_unknown_label(train_labels_2)\n",
    "print(np.unique(y_train_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4eb1e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T19:48:10.368794Z",
     "start_time": "2022-05-03T19:48:09.528017Z"
    }
   },
   "outputs": [],
   "source": [
    "# Labels encoding\n",
    "y_train_2 = np.array(encode_labels(y_train_2).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a2eb99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T19:48:29.414546Z",
     "start_time": "2022-05-03T19:48:10.370168Z"
    }
   },
   "outputs": [],
   "source": [
    "del train_specgrams_2, train_labels_2\n",
    "del train_specgrams, val_specgrams, test_specgrams\n",
    "del train_labels, val_labels, test_labels\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1305a760",
   "metadata": {},
   "source": [
    "# <span style='font-family:Georgia'> 2. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03900c85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T19:48:29.429982Z",
     "start_time": "2022-05-03T19:48:29.414546Z"
    }
   },
   "outputs": [],
   "source": [
    "MIN_DELTA=1e-3\n",
    "DROPOUT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c0f192",
   "metadata": {},
   "source": [
    "## <span style='font-family:Georgia'> 2.1. Tuning hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5875e17b",
   "metadata": {},
   "source": [
    "### <span style='font-family:Georgia'> 2.1.1. Learning rate & batch size tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f534ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T19:48:29.468040Z",
     "start_time": "2022-05-03T19:48:29.430979Z"
    }
   },
   "outputs": [],
   "source": [
    "# hyperparameters set-up\n",
    "INPUT_DIM = (X_train.shape[1], X_train.shape[2])\n",
    "OUTPUT_DIM = Y_train.shape[1]\n",
    "\n",
    "Y_test_classes = np.argmax(Y_test, axis=1)\n",
    "\n",
    "ITER = 5\n",
    "EPOCHS = 100\n",
    "PATIENCE=int(EPOCHS/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fc8da4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T19:48:29.484843Z",
     "start_time": "2022-05-03T19:48:29.468785Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "batch_size = [64, 128, 256, 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d0cbf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T19:48:29.501497Z",
     "start_time": "2022-05-03T19:48:29.485448Z"
    }
   },
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_accuracy', patience=PATIENCE, min_delta=MIN_DELTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfde49c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-05-03T19:19:08.294Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model1_acc_all, model2_acc_all, model3_acc_all = [], [], []\n",
    "\n",
    "for lr in tqdm(learning_rate):\n",
    "    model1_acc_lr, model2_acc_lr, model3_acc_lr = [], [], []\n",
    "    \n",
    "    for size in tqdm(batch_size):\n",
    "        model1_acc_bs, model2_acc_bs, model3_acc_bs = [], [], []        \n",
    "    \n",
    "        for i in tqdm(range(ITER)):\n",
    "            OPT = Adam(learning_rate=lr, clipnorm=1.0)\n",
    "            # SimpleRNN\n",
    "            model1 = rnn_network(INPUT_DIM, OUTPUT_DIM, SimpleRNN, OPT)\n",
    "            K.clear_session()\n",
    "            model1_history = model1.fit(X_train, Y_train,\n",
    "                                        batch_size=size, epochs=EPOCHS, verbose=1,\n",
    "                                        validation_data=(X_val, Y_val),\n",
    "                                        callbacks=[early_stop]\n",
    "                                       )\n",
    "\n",
    "            # LSTM\n",
    "            model2 = rnn_network(INPUT_DIM, OUTPUT_DIM, LSTM, OPT)\n",
    "            K.clear_session()\n",
    "            model2_history = model2.fit(X_train, Y_train,\n",
    "                                        batch_size=size, epochs=EPOCHS, verbose=1,\n",
    "                                        validation_data=(X_val, Y_val),\n",
    "                                        callbacks=[early_stop]\n",
    "                                       )\n",
    "\n",
    "            # GRU\n",
    "            model3 = rnn_network(INPUT_DIM, OUTPUT_DIM, GRU, OPT)\n",
    "            K.clear_session()\n",
    "            model3_history = model3.fit(X_train, Y_train,\n",
    "                                        batch_size=size, epochs=EPOCHS, verbose=1,\n",
    "                                        validation_data=(X_val, Y_val),\n",
    "                                        callbacks=[early_stop]\n",
    "                                       )\n",
    "            \n",
    "            # Save models predictions accuracies for each iteration\n",
    "            y_pred_1 = model1.predict(X_test)\n",
    "            y_pred_2 = model2.predict(X_test)\n",
    "            y_pred_3 = model3.predict(X_test)\n",
    "            y_pred_classes_1 = np.argmax(y_pred_1, axis=-1)\n",
    "            y_pred_classes_2 = np.argmax(y_pred_2, axis=-1)\n",
    "            y_pred_classes_3 = np.argmax(y_pred_3, axis=-1)\n",
    "\n",
    "            model1_acc_bs.append(accuracy_score(Y_test_classes, y_pred_classes_1))\n",
    "            model2_acc_bs.append(accuracy_score(Y_test_classes, y_pred_classes_2))\n",
    "            model3_acc_bs.append(accuracy_score(Y_test_classes, y_pred_classes_3))\n",
    "\n",
    "        model1_acc_lr.append(model1_acc_bs)\n",
    "        model2_acc_lr.append(model2_acc_bs)\n",
    "        model3_acc_lr.append(model3_acc_bs)\n",
    "        \n",
    "    # Save models results for each learning rate\n",
    "    model1_acc_all.append(model1_acc_lr)\n",
    "    model2_acc_all.append(model2_acc_lr)\n",
    "    model3_acc_all.append(model3_acc_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e67f0c",
   "metadata": {},
   "source": [
    "### <span style='font-family:Georgia'> 2.1.2. Tuning results summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac0a5a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T05:28:15.354535Z",
     "start_time": "2022-05-04T05:28:02.716788Z"
    }
   },
   "outputs": [],
   "source": [
    "acc_all = []\n",
    "model_1_acc_lr, model_2_acc_lr, model_3_acc_lr = [], [], []\n",
    "\n",
    "for i in range(len(learning_rate)):\n",
    "    model_1_acc_bs, model_2_acc_bs, model_3_acc_bs = [], [], []\n",
    "    for j in range(len(batch_size)):\n",
    "        # mean over iterations (mean score for each learning_rate-batch_size pair)\n",
    "        model_1_acc_bs.append(np.mean(model1_acc_all[i][j]))\n",
    "        model_2_acc_bs.append(np.mean(model2_acc_all[i][j]))\n",
    "        model_3_acc_bs.append(np.mean(model3_acc_all[i][j]))\n",
    "        \n",
    "    model_1_acc_lr.append(model_1_acc_bs)\n",
    "    model_2_acc_lr.append(model_2_acc_bs)\n",
    "    model_3_acc_lr.append(model_3_acc_bs)\n",
    "        \n",
    "    # mean over batch_sized (mean score for each learning_rate)\n",
    "    acc_all.append([\n",
    "        np.mean(model_1_acc_lr),\n",
    "        np.mean(model_2_acc_lr),\n",
    "        np.mean(model_3_acc_lr)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15436b8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T05:28:15.358494Z",
     "start_time": "2022-05-03T19:19:23.554Z"
    }
   },
   "outputs": [],
   "source": [
    "# Accuracy depending on both: Learning rate & Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f1afc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T05:28:15.360489Z",
     "start_time": "2022-05-03T19:19:23.943Z"
    }
   },
   "outputs": [],
   "source": [
    "models_results = [\n",
    "    pd.DataFrame(model_1_acc_lr, columns=batch_size, index=learning_rate),\n",
    "    pd.DataFrame(model_2_acc_lr, columns=batch_size, index=learning_rate),\n",
    "    pd.DataFrame(model_3_acc_lr, columns=batch_size, index=learning_rate)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9598f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T05:28:15.361487Z",
     "start_time": "2022-05-03T19:19:25.455Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_heatmaps(models_results, title=\"Hyperparameters tuning results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59537ffc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T05:28:15.362487Z",
     "start_time": "2022-05-03T19:19:26.583Z"
    }
   },
   "outputs": [],
   "source": [
    "# Accuracy depending on both: Learning rate (mean over batch size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc1b9f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T05:28:15.363484Z",
     "start_time": "2022-05-03T19:19:26.931Z"
    }
   },
   "outputs": [],
   "source": [
    "results_acc = pd.DataFrame(acc_all)\n",
    "results_acc.columns = ['SimpleRNN', 'LSTM', 'GRU']\n",
    "results_acc.index = learning_rate\n",
    "results_acc.index.name = \"Learning rate\"\n",
    "results_acc_melted = pd.melt(results_acc.reset_index(), id_vars=['Learning rate'], value_vars=['SimpleRNN', 'LSTM', 'GRU'])\n",
    "results_acc_melted.columns=['Learning rate', 'Model', 'Accuracy']\n",
    "results_acc.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24649530",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T05:28:15.365479Z",
     "start_time": "2022-05-03T19:19:27.279Z"
    }
   },
   "outputs": [],
   "source": [
    "for j in range(results_acc_melted.shape[0]):\n",
    "    results_acc_melted.loc[j, 'Learning rate'] = '{:.0e}'.format(results_acc_melted.loc[j, 'Learning rate'])\n",
    "sns.stripplot(x=\"Learning rate\", y=\"Accuracy\", hue=\"Model\",\n",
    "               data=results_acc_melted, palette=\"Set2\", dodge=False, jitter=0.0)    \n",
    "plt.title(\"Accuracy depending on learning rate\", size=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153221a5",
   "metadata": {},
   "source": [
    "### <span style='font-family:Georgia'> 2.1.3. Tuned models overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbf15ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000d983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO BEFORE BELOW CALCULATIONS: select the best learning rate & batch size pair for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0cd5b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadfd154",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T18:10:57.506426Z",
     "start_time": "2022-05-02T17:48:06.066753Z"
    }
   },
   "outputs": [],
   "source": [
    "MODELS = ['SimpleRNN', 'LSTM', 'GRU']\n",
    "LEARNING_RATES = [1e-3, 1e-4, 1e-3]\n",
    "BATCH_SIZES = [128, 128, 128]\n",
    "\n",
    "# SimpleRNN\n",
    "OPT = Adam(learning_rate=LEARNING_RATES[0], clipnorm=1.0)\n",
    "model1 = rnn_network(INPUT_DIM, OUTPUT_DIM, SimpleRNN, OPT)\n",
    "K.clear_session()\n",
    "model1_history = model1.fit(X_train, Y_train,\n",
    "                            batch_size=BATCH_SIZES[0], epochs=EPOCHS, verbose=1,\n",
    "                            validation_data=(X_val, Y_val),\n",
    "                            callbacks=[early_stop]\n",
    "                           )\n",
    "\n",
    "# LSTM\n",
    "OPT = Adam(learning_rate=LEARNING_RATES[1], clipnorm=1.0)\n",
    "model2 = rnn_network(INPUT_DIM, OUTPUT_DIM, LSTM, OPT)\n",
    "K.clear_session()\n",
    "model2_history = model2.fit(X_train, Y_train,\n",
    "                            batch_size=BATCH_SIZES[1], epochs=EPOCHS, verbose=1,\n",
    "                            validation_data=(X_val, Y_val),\n",
    "                            callbacks=[early_stop]\n",
    "                           )\n",
    "\n",
    "# GRU\n",
    "OPT = Adam(learning_rate=LEARNING_RATES[2], clipnorm=1.0)\n",
    "model3 = rnn_network(INPUT_DIM, OUTPUT_DIM, GRU, OPT)\n",
    "K.clear_session()\n",
    "model3_history = model3.fit(X_train, Y_train,\n",
    "                            batch_size=BATCH_SIZES[2], epochs=EPOCHS, verbose=1,\n",
    "                            validation_data=(X_val, Y_val),\n",
    "                            callbacks=[early_stop]\n",
    "                           )\n",
    "\n",
    "# Save models predictions accuracies for each iteration\n",
    "y_pred_1 = model1.predict(X_test)\n",
    "y_pred_2 = model2.predict(X_test)\n",
    "y_pred_3 = model3.predict(X_test)\n",
    "y_pred_classes_1 = np.argmax(y_pred_1, axis=-1)\n",
    "y_pred_classes_2 = np.argmax(y_pred_2, axis=-1)\n",
    "y_pred_classes_3 = np.argmax(y_pred_3, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f360ae9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T18:10:58.128911Z",
     "start_time": "2022-05-02T18:10:57.511515Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convergence analysis\n",
    "models_history = [model1_history, model2_history, model3_history]\n",
    "\n",
    "fig, axes = plt.subplots(1,3,figsize=(18,6))\n",
    "models = ['1', '2', '3']\n",
    "for i in range(3):\n",
    "    model_name=MODELS[i]\n",
    "    history = models_history[i]\n",
    "    data_lr = history.history['loss']\n",
    "    sns.lineplot(data=data_lr, ax=axes[i])\n",
    "    if i == 0 : axes[i].set_ylim([0,2.5]) # due to huge loss values for 1e-2 (to make other lines visible)\n",
    "    axes[i].set_title('Model '+model_name)\n",
    "    axes[i].set_xlabel('Epoch', size=12)\n",
    "    axes[i].set_ylabel('Loss', size=12)\n",
    "fig.suptitle(\"Convergence analysis\", size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67439893",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T18:11:00.644742Z",
     "start_time": "2022-05-02T18:10:58.130906Z"
    }
   },
   "outputs": [],
   "source": [
    "# Models evaluation- confusion matrices\n",
    "models_results = [pd.DataFrame(confusion_matrix(Y_test_classes, y_pred_classes_1)),\n",
    "                  pd.DataFrame(confusion_matrix(Y_test_classes, y_pred_classes_2)),\n",
    "                  pd.DataFrame(confusion_matrix(Y_test_classes, y_pred_classes_3))]\n",
    "\n",
    "plot_confusion_matr(models_results, 'Models evaluation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143daa87",
   "metadata": {},
   "source": [
    "## <span style='font-family:Georgia'> 2.2. Noise data handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52034972",
   "metadata": {},
   "source": [
    "### <span style='font-family:Georgia'> 2.2.1. Approach 1 - taking `background_noise` observations as separate `silence` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0cb6d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T18:11:00.654104Z",
     "start_time": "2022-05-02T18:11:00.646736Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train, x_val, x_test = x_train_1, x_val_1, x_test_1\n",
    "y_train, y_val, y_test = y_train_1, y_val_1, y_test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92ba142",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T19:28:06.404910Z",
     "start_time": "2022-05-02T19:28:06.399958Z"
    }
   },
   "outputs": [],
   "source": [
    "# hyperparameters set-up\n",
    "INPUT_DIM = (x_train.shape[1], x_train.shape[2])\n",
    "OUTPUT_DIM = y_train.shape[1]\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', patience=PATIENCE, min_delta=MIN_DELTA)\n",
    "\n",
    "# TODO: Choose best LR & BATCH_SIZE\n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-4\n",
    "OPT = Adam(learning_rate=LR, clipnorm=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bae78ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T20:13:23.958611Z",
     "start_time": "2022-05-02T19:28:07.569577Z"
    }
   },
   "outputs": [],
   "source": [
    "# Models training\n",
    "model1_stats = []\n",
    "model1_acc = []\n",
    "model2_stats = []\n",
    "model2_acc = []\n",
    "model3_stats = []\n",
    "model3_acc = []\n",
    "\n",
    "for i in tqdm(range(ITER)):\n",
    "    # SimpleRNN\n",
    "    model1 = rnn_network(INPUT_DIM, OUTPUT_DIM, SimpleRNN, OPT)\n",
    "    K.clear_session()\n",
    "    model1_history = model1.fit(x_train, y_train,\n",
    "                                batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1,\n",
    "                                validation_data=(x_val, y_val),\n",
    "                                callbacks=[early_stop]\n",
    "                               )\n",
    "    \n",
    "    # LSTM\n",
    "    model2 = rnn_network(INPUT_DIM, OUTPUT_DIM, LSTM, OPT)\n",
    "    K.clear_session()\n",
    "    model2_history = model2.fit(x_train, y_train,\n",
    "                                batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1,\n",
    "                                validation_data=(x_val, y_val),\n",
    "                                callbacks=[early_stop]\n",
    "                               )\n",
    "    \n",
    "    # GRU\n",
    "    model3 = rnn_network(INPUT_DIM, OUTPUT_DIM, GRU, OPT)\n",
    "    K.clear_session()\n",
    "    model3_history = model3.fit(x_train, y_train,\n",
    "                                batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1,\n",
    "                                validation_data=(x_val, y_val),\n",
    "                                callbacks=[early_stop]\n",
    "                               )\n",
    "    \n",
    "    # Saving results\n",
    "    y_test_classes = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    model1_stats.append(model1_history)\n",
    "    y_pred_1 = model1.predict(x_test)\n",
    "    y_pred_classes_1 = np.argmax(y_pred_1, axis=-1)\n",
    "    model1_acc.append(accuracy_score(y_test_classes, y_pred_classes_1))\n",
    "\n",
    "    model2_stats.append(model2_history)\n",
    "    y_pred_2 = model2.predict(x_test)\n",
    "    y_pred_classes_2 = np.argmax(y_pred_2, axis=-1)\n",
    "    model2_acc.append(accuracy_score(y_test_classes, y_pred_classes_2))\n",
    "    \n",
    "    model3_stats.append(model3_history)\n",
    "    y_pred_3 = model3.predict(x_test)\n",
    "    y_pred_classes_3 = np.argmax(y_pred_3, axis=-1)\n",
    "    model3_acc.append(accuracy_score(y_test_classes, y_pred_classes_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7b8b28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T20:13:24.005050Z",
     "start_time": "2022-05-02T20:13:23.960678Z"
    }
   },
   "outputs": [],
   "source": [
    "# Models summary\n",
    "approach1_acc_results = pd.DataFrame([model1_acc, model2_acc, model3_acc]).T\n",
    "approach1_acc_results.columns=['SimpleRNN', 'LSTM', 'GRU']\n",
    "approach1_acc_results_stats = approach1_acc_results.describe().T[['mean', 'std', 'min', 'max']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aaded6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T20:13:24.034348Z",
     "start_time": "2022-05-02T20:13:24.008561Z"
    }
   },
   "outputs": [],
   "source": [
    "model1_epochs = []\n",
    "model2_epochs = []\n",
    "model3_epochs = []\n",
    "for i in range(ITER):\n",
    "    model1_epochs.append(len(model1_stats[i].history['loss']))\n",
    "    model2_epochs.append(len(model2_stats[i].history['loss']))\n",
    "    model3_epochs.append(len(model3_stats[i].history['loss']))\n",
    "    \n",
    "approach1_acc_results_stats['epochs'] = [np.array(model1_epochs).mean(), np.array(model2_epochs).mean(),\n",
    "                                      np.array(model3_epochs).mean()]\n",
    "approach1_acc_results_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72095d57",
   "metadata": {},
   "source": [
    "### <span style='font-family:Georgia'> 2.2.2. Approach 2 - noising the training subset(s) with the `background_noise` class observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74bf8fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T20:13:24.049735Z",
     "start_time": "2022-05-02T20:13:24.034348Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train, x_val, x_test = x_train_2, X_val, X_test\n",
    "y_train, y_val, y_test = y_train_2, Y_val, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5b2c14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T20:13:24.065694Z",
     "start_time": "2022-05-02T20:13:24.052729Z"
    }
   },
   "outputs": [],
   "source": [
    "# hyperparameters set-up\n",
    "INPUT_DIM = (x_train.shape[1], x_train.shape[2])\n",
    "OUTPUT_DIM = y_train.shape[1]\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', patience=PATIENCE, min_delta=MIN_DELTA)\n",
    "\n",
    "# TODO: Choose best LR & BATCH_SIZE\n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-4\n",
    "OPT = Adam(learning_rate=LR, clipnorm=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a39295",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T21:05:02.607788Z",
     "start_time": "2022-05-02T20:13:24.068684Z"
    }
   },
   "outputs": [],
   "source": [
    "# Models training\n",
    "model4_stats = []\n",
    "model4_acc = []\n",
    "model5_stats = []\n",
    "model5_acc = []\n",
    "model6_stats = []\n",
    "model6_acc = []\n",
    "\n",
    "for i in tqdm(range(ITER)):\n",
    "    # SimpleRNN\n",
    "    model4 = rnn_network(INPUT_DIM, OUTPUT_DIM, SimpleRNN, OPT)\n",
    "    K.clear_session()\n",
    "    model4_history = model4.fit(x_train, y_train,\n",
    "                                batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1,\n",
    "                                validation_data=(x_val, y_val),\n",
    "                                callbacks=[early_stop]\n",
    "                               )\n",
    "    \n",
    "    # LSTM\n",
    "    model5 = rnn_network(INPUT_DIM, OUTPUT_DIM, LSTM, OPT)\n",
    "    K.clear_session()\n",
    "    \n",
    "    model5_history = model5.fit(x_train, y_train,\n",
    "                                batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1,\n",
    "                                validation_data=(x_val, y_val),\n",
    "                                callbacks=[early_stop]\n",
    "                               )\n",
    "    \n",
    "    # GRU\n",
    "    model6 = rnn_network(INPUT_DIM, OUTPUT_DIM, GRU, OPT)\n",
    "    K.clear_session()\n",
    "    model6_history = model6.fit(x_train, y_train,\n",
    "                                batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1,\n",
    "                                validation_data=(x_val, y_val),\n",
    "                                callbacks=[early_stop]\n",
    "                               )\n",
    "    \n",
    "    # Saving results\n",
    "    y_test_classes = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    model4_stats.append(model4_history)\n",
    "    y_pred_4 = model4.predict(x_test)\n",
    "    y_pred_classes_4 = np.argmax(y_pred_4, axis=-1)\n",
    "    model4_acc.append(accuracy_score(y_test_classes, y_pred_classes_4))\n",
    "\n",
    "    model5_stats.append(model5_history)\n",
    "    y_pred_5 = model5.predict(x_test)\n",
    "    y_pred_classes_5 = np.argmax(y_pred_5, axis=-1)\n",
    "    model5_acc.append(accuracy_score(y_test_classes, y_pred_classes_5))\n",
    "    \n",
    "    model6_stats.append(model6_history)\n",
    "    y_pred_6 = model6.predict(x_test)\n",
    "    y_pred_classes_6 = np.argmax(y_pred_6, axis=-1)\n",
    "    model6_acc.append(accuracy_score(y_test_classes, y_pred_classes_6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c2ffb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T21:05:02.696500Z",
     "start_time": "2022-05-02T21:05:02.612806Z"
    }
   },
   "outputs": [],
   "source": [
    "# Models summary\n",
    "approach2_acc_results = pd.DataFrame([model4_acc, model5_acc, model6_acc]).T\n",
    "approach2_acc_results.columns=['SimpleRNN', 'LSTM', 'GRU']\n",
    "approach2_acc_results_stats = approach2_acc_results.describe().T[['mean', 'std', 'min', 'max']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52a9f19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T21:05:02.733475Z",
     "start_time": "2022-05-02T21:05:02.699492Z"
    }
   },
   "outputs": [],
   "source": [
    "model4_epochs = []\n",
    "model5_epochs = []\n",
    "model6_epochs = []\n",
    "for i in range(ITER):\n",
    "    model4_epochs.append(len(model4_stats[i].history['loss']))\n",
    "    model5_epochs.append(len(model5_stats[i].history['loss']))\n",
    "    model6_epochs.append(len(model6_stats[i].history['loss']))\n",
    "    \n",
    "approach2_acc_results_stats['epochs'] = [np.array(model4_epochs).mean(), np.array(model5_epochs).mean(),\n",
    "                                      np.array(model6_epochs).mean()]\n",
    "approach2_acc_results_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2240dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
