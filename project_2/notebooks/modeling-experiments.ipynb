{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bd903cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from itertools import compress\n",
    "\n",
    "import librosa\n",
    "from scipy.fftpack import fft\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import keras\n",
    "from keras import optimizers, losses, activations, models\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, Bidirectional, Conv1D, GRU\n",
    "from tensorflow.keras.layers import Input, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "\n",
    "random.seed(0)\n",
    "sns.set_style('whitegrid')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8c98eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters settings\n",
    "data_path = '../../data/train/audio'\n",
    "labels = os.listdir(data_path)\n",
    "CLASSES = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go'] \n",
    "SAMPLING_RATE = 16000\n",
    "\n",
    "EPOCHS = 30\n",
    "PATIENCE=int(EPOCHS/3)\n",
    "BATCH_SIZE = 128\n",
    "MIN_DELTA=1e-4\n",
    "LR = 1e-4\n",
    "DROPOUT = 0.2\n",
    "NOISE_RATIO = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f1846a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_audio(samples):\n",
    "    return np.pad(samples, pad_width=(SAMPLING_RATE - len(samples), 0), mode='constant', constant_values=(0, 0))\n",
    "\n",
    "\n",
    "def chop_audio(samples):\n",
    "    return samples[:SAMPLING_RATE]\n",
    "\n",
    "\n",
    "def split_audio(sample_rate, samples):\n",
    "    \"\"\" Splits audio file to multiple with (up to) fixed 1s length \"\"\"\n",
    "    duration = float(len(samples)/sample_rate)\n",
    "    n_samples = math.ceil(duration)\n",
    "    return np.array_split(samples, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f417dec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_unknown_label(labels, classes=CLASSES):\n",
    "    new_labels = []\n",
    "    for label in labels:\n",
    "        if label not in classes:\n",
    "            new_labels.append('unknown')\n",
    "        else:\n",
    "            new_labels.append(label)\n",
    "    return new_labels\n",
    "\n",
    "def encode_labels(labels):\n",
    "    return pd.get_dummies(pd.Series(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5733564b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def specgram(audio, sample_rate, window_size=20, step_size=10, eps=1e-10):\n",
    "    nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "    noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "    _, _, spec = signal.spectrogram(audio,\n",
    "                                    fs=sample_rate,\n",
    "                                    window='hann',\n",
    "                                    nperseg=nperseg,\n",
    "                                    noverlap=noverlap,\n",
    "                                    detrend=False)\n",
    "    return np.log(spec.T.astype(np.float32) + eps)\n",
    "\n",
    "\n",
    "def plot_model_history(history, title: str) -> None:\n",
    "    \"\"\"\n",
    "    Plotting the learning curve of Keras model, broken down into loss curve and accuracy curve, \n",
    "    for both training and validation data.\n",
    "\n",
    "    Args:\n",
    "        history : Object returned by the .fit method of Keras model.\n",
    "        title (str): Title of the plots.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1,2,figsize=(15,5)) \n",
    "\n",
    "    axs[0].plot(history.history['accuracy']) \n",
    "    axs[0].plot(history.history['val_accuracy']) \n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy') \n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].legend(['train', 'validate'], loc='upper left')\n",
    "\n",
    "    axs[1].plot(history.history['loss']) \n",
    "    axs[1].plot(history.history['val_loss']) \n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss') \n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].legend(['train', 'validate'], loc='upper left')\n",
    "    fig.suptitle(title, size=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b46539ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_network(input_dim, output_dim, rnn_layer):\n",
    "\n",
    "    input_data = Input(name='input', shape=input_dim, dtype='float32')\n",
    "\n",
    "    x = Conv1D(filters=256, kernel_size=10, strides=4, name='conv1d')(input_data)\n",
    "    x = BatchNormalization(name='b_norm')(x)\n",
    "    x = Activation('relu', name='activation')(x)\n",
    "    x = Dropout(DROPOUT, name='dropout_1')(x)\n",
    "    x = rnn_layer(128, activation='relu', return_sequences=True, dropout=DROPOUT, name='rnn_1')(x)\n",
    "    x = rnn_layer(128, activation='relu', return_sequences=False, dropout=DROPOUT, name='rnn_2')(x)\n",
    "    x = Dense(units=64, activation='relu', name='dense')(x)\n",
    "    x = Dropout(DROPOUT, name='dropout_2')(x)\n",
    "\n",
    "    output_data = Dense(units=output_dim, activation='softmax', name='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=input_data, outputs=output_data, name=str(rnn_layer).split(\".\")[-1].split(\"'\")[0])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=OPT, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a196fc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmaps(models_results: list, title: str=None) -> None:\n",
    "    \"\"\"\n",
    "    Plot heatmaps based on the confusion matrices of a given models.\n",
    "\n",
    "    Args:\n",
    "        models_results (list): List of confusion matrices of the evaluated models.\n",
    "        title (str): Title of the plots.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1,3,figsize=(18,6))\n",
    "    models = ['SimpleRNN', 'LSTM', 'GRU']\n",
    "    for i in range(3):\n",
    "        sns.heatmap(models_results[i], annot=True, cmap=\"Blues\", ax=axes[i], fmt='d', annot_kws={\"size\": 12})\n",
    "        axes[i].set_title(models[i], size=15)\n",
    "        axes[i].set_xticklabels(axes[i].get_xmajorticklabels(), fontsize = 12)\n",
    "        axes[i].set_yticklabels(axes[i].get_xmajorticklabels(), fontsize = 12)\n",
    "        cax = plt.gcf().axes[-1]\n",
    "        cax.tick_params(labelsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed72d5e9",
   "metadata": {},
   "source": [
    "# <span style='font-family:Georgia'> 1. Data loading & preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb3636c",
   "metadata": {},
   "source": [
    "### <span style='font-family:Georgia'> 1.1. Noisy data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9eb8c4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb46a9a07c304c8cb5b584e399f0feef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label = '_background_noise_'\n",
    "files = [f for f in os.listdir(data_path + '/'+ label) if f.endswith('.wav')]\n",
    "one_sec_background_noise_specgrams = []\n",
    "\n",
    "for file in tqdm(files):\n",
    "    _, samples = wavfile.read(data_path + \"/\" + label + \"/\" + file)\n",
    "    duration = float(len(samples)/SAMPLING_RATE)\n",
    "\n",
    "    # Do not distinguish between noise classes\n",
    "    one_sec_background_noise = split_audio(\n",
    "            sample_rate=SAMPLING_RATE, samples=samples\n",
    "        )\n",
    "    \n",
    "    for item in one_sec_background_noise:\n",
    "        duration = float(len(item)/SAMPLING_RATE)\n",
    "        if duration < 1: item = pad_audio(item)        \n",
    "        \n",
    "        one_sec_background_noise_specgrams.append(\n",
    "            specgram(item, SAMPLING_RATE)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6aec04",
   "metadata": {},
   "source": [
    "### <span style='font-family:Georgia'> 1.2. Noise-free data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ec2f8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_list = pd.read_csv('../../data/train/validation_list.txt', sep=\"\\t\", header=None)[0].tolist()\n",
    "testing_list = pd.read_csv('../../data/train/testing_list.txt', sep=\"\\t\", header=None)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c70ae40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  51088\n",
      "Validation:  6798\n",
      "Testing:  6835\n"
     ]
    }
   ],
   "source": [
    "print('Training: ', 64721 - len(validation_list) - len(testing_list))\n",
    "print('Validation: ', len(validation_list))\n",
    "print('Testing: ', len(testing_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006bce66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8b3c3af78e34f2189613213e6b8cc08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b93e69414ac841b0a9b8ad9ed3d97cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1713 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75cfd3b5d0e844599574c599db2e4328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1731 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "812d77091e934a14bf37b3bbaaf52abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1733 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f70dbf2751c04255a1e048ee6c7df16a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1746 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "407e4b05d7c646629e35e0c8952b24b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2359 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d020320fdefc4f559e9e02f50d70472d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab422e3f55c49a88dc45011e0043c0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2357 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b2c735207044256b6e5c9347681fc3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2372 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "371391ec4c7d4ddcb5f45c8db531c0a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2372 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "210bddb827f546559491fc5992fc6413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1742 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ef54cd3c723435bbe4068ced8bbedf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80dff1aa58fd46a2979ef6e671d234a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "382d67870eb14eabba7dbc49aa3e5ca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1746 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236ccb61f3fb4faaab95177cee098c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2364 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c892ecc8fb5403fb5db44736d12eb2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "697f88e69ea146dab11ab4821283a51d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2357 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32c30841264544188af9ba51f366668c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2367 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e873a0dd93c44099963a551ee1529b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2370 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f47a3ca311a746349ea0476e0a79462f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2367 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2644f3e856f4fd99cc321438fb26093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc32e54ce674378a432b50bc2c9926a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0286c901596e49a5bc233b69ed99c3ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2369 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67fa00a9bcf143438ec0ba9d3e6638c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2380 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd4dd37e107a4dd494a126a68082bfce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2356 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb4bdac05e54348aee5d51a9c38f41b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1733 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd53b6b6ba1d46e787261a13df98ab66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2373 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd36cc00871b4ed9ba7d3f3cef264895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_labels = []\n",
    "train_specgrams = []\n",
    "val_labels = []\n",
    "val_specgrams = []\n",
    "test_labels = []\n",
    "test_specgrams = []\n",
    "\n",
    "for label in tqdm([l for l in labels if l != '_background_noise_']):\n",
    "\n",
    "    files = [f for f in os.listdir(data_path + '/'+ label) if f.endswith('.wav')]\n",
    "    \n",
    "    for file in tqdm(files):\n",
    "        _, samples = wavfile.read(data_path + \"/\" + label + \"/\" + file)\n",
    "        duration = float(len(samples)/SAMPLING_RATE)\n",
    "                \n",
    "        if duration < 1: samples = pad_audio(samples)\n",
    "            \n",
    "        if (label + \"/\" + file) in validation_list: \n",
    "            val_labels.append(label)\n",
    "            val_specgrams.append(specgram(samples, SAMPLING_RATE))\n",
    "        elif (label + \"/\" + file) in testing_list: \n",
    "            test_labels.append(label)\n",
    "            test_specgrams.append(specgram(samples, SAMPLING_RATE))\n",
    "        else:\n",
    "            train_labels.append(label)\n",
    "            train_specgrams.append(specgram(samples, SAMPLING_RATE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aff870",
   "metadata": {},
   "source": [
    "### <span style='font-family:Georgia'> 1.3. Data labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f68893e",
   "metadata": {},
   "source": [
    "### <span style='font-family:Georgia'> 1.3.1. Approach 1 - taking `background_noise` observations as separate `silence` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0360d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split noise into 3 disjoint subsets (train, valid & test sets)\n",
    "N = 64721\n",
    "valid_ratio = np.round(len(validation_list)/N, 2)\n",
    "test_ratio = np.round(len(testing_list)/N, 2)\n",
    "train_ratio = 1 - (valid_ratio+test_ratio)\n",
    "\n",
    "sets_list = [\"train\", \"val\", \"test\"]\n",
    "obs_split = np.array(random.choices(sets_list, weights=(train_ratio, valid_ratio, test_ratio), k=N))\n",
    "train_mask, val_mask, test_mask = obs_split == \"train\", obs_split == \"val\", obs_split == \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3001513",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_noise = list(compress(one_sec_background_noise_specgrams, train_mask))\n",
    "print(f'Number of observations assigned to train set: {len(train_noise)}')\n",
    "\n",
    "val_noise = list(compress(one_sec_background_noise_specgrams, val_mask))\n",
    "print(f'Number of observations assigned to validation set: {len(val_noise)}')\n",
    "\n",
    "test_noise = list(compress(one_sec_background_noise_specgrams, test_mask))\n",
    "print(f'Number of observations assigned to test set: {len(test_noise)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc35b15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_specgrams_1 = train_specgrams + train_noise\n",
    "train_labels_1 = train_labels + ['silence' for i in range(len(train_noise))]\n",
    "\n",
    "val_specgrams_1 = val_specgrams + val_noise\n",
    "val_labels_1 = val_labels + ['silence' for i in range(len(val_noise))]\n",
    "\n",
    "test_specgrams_1 = test_specgrams + test_noise\n",
    "test_labels_1 = test_labels + ['silence' for i in range(len(test_noise))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b75257",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_1 = np.array(train_specgrams_1)\n",
    "print(x_train_1.shape)\n",
    "\n",
    "x_val_1 = np.array(val_specgrams_1)\n",
    "print(x_val_1.shape)\n",
    "\n",
    "x_test_1 = np.array(test_specgrams_1)\n",
    "print(x_test_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b225be",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=CLASSES+['silence']\n",
    "\n",
    "y_train_1 = assign_unknown_label(train_labels_1, classes=classes)\n",
    "print(np.unique(y_train_1))\n",
    "\n",
    "y_val_1 = assign_unknown_label(val_labels_1, classes=classes)\n",
    "print(np.unique(y_val_1))\n",
    "\n",
    "y_test_1 = assign_unknown_label(test_labels_1, classes=classes)\n",
    "print(np.unique(y_test_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4f9ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_specgrams_1, val_specgrams_1, test_specgrams_1\n",
    "del train_labels_1, val_labels_1, test_labels_1\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049945e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels encoding\n",
    "y_train_1 = np.array(encode_labels(y_train_1).values)\n",
    "y_val_1 = np.array(encode_labels(y_val_1).values)\n",
    "y_test_1 = np.array(encode_labels(y_test_1).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0703d383",
   "metadata": {},
   "source": [
    "### <span style='font-family:Georgia'> 1.3.2. Approach 2 - noising the training subset(s) with the 'background_noise' class observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edab460",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_summary = pd.Series(train_labels).value_counts()\n",
    "n = math.floor(NOISE_RATIO*train_labels_summary.sum())\n",
    "noised_labels = random.choices(train_labels, k=n)\n",
    "\n",
    "summary = pd.DataFrame(train_labels_summary, columns=[\"obs_cnt\"])\n",
    "summary[\"noise_cnt\"] = pd.Series(noised_labels).value_counts()\n",
    "summary[\"noise_ratio\"] = summary[\"noise_cnt\"] / summary[\"obs_cnt\"]\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5527ab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_specgrams_2 = train_specgrams + random.choices(one_sec_background_noise_specgrams, k=n)\n",
    "train_labels_2 = train_labels + noised_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0baf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_2 = np.array(train_specgrams_2)\n",
    "print(x_train_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2075ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_2 = assign_unknown_label(train_labels_2)\n",
    "print(np.unique(y_train_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4eb1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels encoding\n",
    "y_train_2 = np.array(encode_labels(y_train_2).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a2eb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_specgrams_2, train_labels_2\n",
    "del train_specgrams, val_specgrams, test_specgrams\n",
    "del train_labels, val_labels, test_labels\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1305a760",
   "metadata": {},
   "source": [
    "# <span style='font-family:Georgia'> 2. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143daa87",
   "metadata": {},
   "source": [
    "## <span style='font-family:Georgia'> 2.1. Approach 1 - taking `background_noise` observations as separate `silence` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0cb6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, x_test = x_train_1, x_val_1, x_test_1\n",
    "y_train, y_val, y_test = y_train_1, y_val_1, y_test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92ba142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters set-up\n",
    "INPUT_DIM = (x_train.shape[1], x_train.shape[2])\n",
    "OUTPUT_DIM = y_train.shape[1]\n",
    "\n",
    "OPT = Adam(learning_rate=LR, clipnorm=1.0)\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', patience=PATIENCE, min_delta=MIN_DELTA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fcce09",
   "metadata": {},
   "source": [
    "### <span style='font-family:Georgia'> 2.1.1 SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bae78ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_stats = []\n",
    "model1_acc = []\n",
    "\n",
    "for i in tqdm(range(5)):\n",
    "    print('iteration: '+str(i))\n",
    "    model1 = rnn_network(INPUT_DIM, OUTPUT_DIM, SimpleRNN)\n",
    "    K.clear_session()\n",
    "    model1_history = model1.fit(x_train, y_train,\n",
    "                                batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1,\n",
    "                                validation_data=(x_val, y_val),\n",
    "                                callbacks=[early_stop]\n",
    "                               )\n",
    "    model1_stats.append(model1_history)\n",
    "    y_pred_1 = model1.predict(X_test)\n",
    "    y_pred_classes_1 = np.argmax(y_pred_1, axis=-1)\n",
    "    model1_acc.append(accuracy_score(y_test, y_pred_classes_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162180a5",
   "metadata": {},
   "source": [
    "### <span style='font-family:Georgia'> 2.1.2. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a9947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_stats = []\n",
    "model2_acc = []\n",
    "\n",
    "for i in tqdm(range(5)):\n",
    "    print('iteration: '+str(i))\n",
    "    model2 = rnn_network(INPUT_DIM, OUTPUT_DIM, LSTM)\n",
    "    K.clear_session()\n",
    "    \n",
    "    model2_history = model2.fit(x_train, y_train,\n",
    "                                batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1,\n",
    "                                validation_data=(x_val, y_val),\n",
    "                                callbacks=[early_stop]\n",
    "                               )\n",
    "    \n",
    "    model2_stats.append(model2_history)\n",
    "    y_pred_2 = model2.predict(X_test)\n",
    "    y_pred_classes_2 = np.argmax(y_pred_2, axis=-1)\n",
    "    model2_acc.append(accuracy_score(y_test, y_pred_classes_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef14c03d",
   "metadata": {},
   "source": [
    "### <span style='font-family:Georgia'> 2.1.3. GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13047b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3_stats = []\n",
    "model3_acc = []\n",
    "\n",
    "for i in tqdm(range(5)):\n",
    "    print('iteration: '+str(i))\n",
    "    model3 = rnn_network(INPUT_DIM, OUTPUT_DIM, GRU)\n",
    "    K.clear_session()\n",
    "    model3_history = model3.fit(x_train, y_train,\n",
    "                                batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1,\n",
    "                                validation_data=(x_val, y_val),\n",
    "                                callbacks=[early_stop]\n",
    "                               )\n",
    "    model3_stats.append(model3_history)\n",
    "    y_pred_3 = model3.predict(X_test)\n",
    "    y_pred_classes_3 = np.argmax(y_pred_3, axis=-1)\n",
    "    model3_acc.append(accuracy_score(y_test, y_pred_classes_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f3ef0d",
   "metadata": {},
   "source": [
    "### <span style='font-family:Georgia'> 2.1.4. Models summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7b8b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "approach1_acc_results = pd.DataFrame([model1_acc, model2_acc, model3_acc]).T\n",
    "approach1_acc_results.columns=['SimpleRNN', 'LSTM', 'GRU']\n",
    "approach1_acc_results_stats = approach1_acc_results.describe().T[['mean', 'std', 'min', 'max']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aaded6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_epochs = []\n",
    "model2_epochs = []\n",
    "model3_epochs = []\n",
    "for i in range(5):\n",
    "    model1_epochs.append(len(model1_stats[i].history['loss']))\n",
    "    model2_epochs.append(len(model2_stats[i].history['loss']))\n",
    "    model3_epochs.append(len(model3_stats[i].history['loss']))\n",
    "    \n",
    "approach1_acc_results_stats['epochs'] = [np.array(model1_epochs).mean(), np.array(model2_epochs).mean(),\n",
    "                                      np.array(model3_epochs).mean()]\n",
    "approach1_acc_results_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72095d57",
   "metadata": {},
   "source": [
    "## <span style='font-family:Georgia'> 2.2. Approach 2 - noising the training subset(s) with the 'background_noise' class observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74bf8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, x_test = x_train_2, x_val_1, x_test_1\n",
    "y_train, y_val, y_test = y_train_2, y_val_1, y_test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5b2c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters set-up\n",
    "INPUT_DIM = (x_train.shape[1], x_train.shape[2])\n",
    "OUTPUT_DIM = y_train.shape[1]\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', patience=PATIENCE, min_delta=MIN_DELTA)\n",
    "OPT = Adam(learning_rate=LR, clipnorm=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11f50dc",
   "metadata": {},
   "source": [
    "### <span style='font-family:Georgia'> 2.2.1 SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eb072e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4_stats = []\n",
    "model4_acc = []\n",
    "\n",
    "for i in tqdm(range(5)):\n",
    "    print('iteration: '+str(i))\n",
    "    model4 = rnn_network(INPUT_DIM, OUTPUT_DIM, SimpleRNN)\n",
    "    K.clear_session()\n",
    "    model4_history = model4.fit(x_train, y_train,\n",
    "                                batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1,\n",
    "                                validation_data=(x_val, y_val),\n",
    "                                callbacks=[early_stop]\n",
    "                               )\n",
    "    model4_stats.append(model4_history)\n",
    "    y_pred_4 = model4.predict(X_test)\n",
    "    y_pred_classes_4 = np.argmax(y_pred_4, axis=-1)\n",
    "    model4_acc.append(accuracy_score(y_test, y_pred_classes_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5450d51a",
   "metadata": {},
   "source": [
    "### <span style='font-family:Georgia'> 2.2.2. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7a7b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model5_stats = []\n",
    "model5_acc = []\n",
    "\n",
    "for i in tqdm(range(5)):\n",
    "    print('iteration: '+str(i))\n",
    "    model5 = rnn_network(INPUT_DIM, OUTPUT_DIM, LSTM)\n",
    "    K.clear_session()\n",
    "    \n",
    "    model5_history = model5.fit(x_train, y_train,\n",
    "                                batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1,\n",
    "                                validation_data=(x_val, y_val),\n",
    "                                callbacks=[early_stop]\n",
    "                               )\n",
    "    \n",
    "    model5_stats.append(model5_history)\n",
    "    y_pred_5 = model5.predict(X_test)\n",
    "    y_pred_classes_5 = np.argmax(y_pred_5, axis=-1)\n",
    "    model5_acc.append(accuracy_score(y_test, y_pred_classes_5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aca4dc",
   "metadata": {},
   "source": [
    "### <span style='font-family:Georgia'> 2.2.3. GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06407de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model6_stats = []\n",
    "model6_acc = []\n",
    "\n",
    "for i in tqdm(range(5)):\n",
    "    print('iteration: '+str(i))\n",
    "    model6 = rnn_network(INPUT_DIM, OUTPUT_DIM, GRU)\n",
    "    K.clear_session()\n",
    "    model6_history = model6.fit(x_train, y_train,\n",
    "                                batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1,\n",
    "                                validation_data=(x_val, y_val),\n",
    "                                callbacks=[early_stop]\n",
    "                               )\n",
    "    model6_stats.append(model6_history)\n",
    "    y_pred_6 = model6.predict(X_test)\n",
    "    y_pred_classes_6 = np.argmax(y_pred_6, axis=-1)\n",
    "    model6_acc.append(accuracy_score(y_test, y_pred_classes_6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25d3614",
   "metadata": {},
   "source": [
    "### <span style='font-family:Georgia'> 2.2.4. Models summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c2ffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "approach2_acc_results = pd.DataFrame([model4_acc, model5_acc, model6_acc]).T\n",
    "approach2_acc_results.columns=['SimpleRNN', 'LSTM', 'GRU']\n",
    "approach2_acc_results_stats = approach2_acc_results.describe().T[['mean', 'std', 'min', 'max']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52a9f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4_epochs = []\n",
    "model5_epochs = []\n",
    "model6_epochs = []\n",
    "for i in range(5):\n",
    "    model4_epochs.append(len(model4_stats[i].history['loss']))\n",
    "    model5_epochs.append(len(model5_stats[i].history['loss']))\n",
    "    model6_epochs.append(len(model6_stats[i].history['loss']))\n",
    "    \n",
    "approach2_acc_results_stats['epochs'] = [np.array(model4_epochs).mean(), np.array(model5_epochs).mean(),\n",
    "                                      np.array(model6_epochs).mean()]\n",
    "approach2_acc_results_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2240dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
