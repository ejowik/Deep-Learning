{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DzjYwQHfl6Ih"
   },
   "outputs": [],
   "source": [
    "# !wget -O quora.zip -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1ERtxpdWOgGQ3HOigqAMHTJjmOE_tWvoF\"\n",
    "# !unzip quora.zip\n",
    "# !pip install -q --upgrade nltk gensim bokeh pandas\n",
    "\n",
    "# import numpy as np\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vqAnq1aPi-mo",
    "outputId": "605524cc-2ef4-4be7-9642-ac9142543c44"
   },
   "outputs": [],
   "source": [
    "# !pip install -qq nltk==3.4\n",
    "#!conda install -c anaconda -y gensim\n",
    "# !pip install -qq pandas==0.23.4\n",
    "# !pip install -qq bokeh==1.0.3\n",
    "\n",
    "# !wget -O quora.zip -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1ERtxpdWOgGQ3HOigqAMHTJjmOE_tWvoF\"\n",
    "# !unzip quora.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Ob6cY-EjMWQ",
    "outputId": "404520f0-698b-4d8c-a8ad-d5dbd905d6b8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXl16AOdtlDk"
   },
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ErZ_TOu0vAOR"
   },
   "source": [
    "*NB. This notebook is somewhat based on the YSDA NLP course [notebook](https://github.com/yandexdataschool/nlp_course/tree/master/week01_embeddings).*\n",
    "\n",
    "Guess, you've seen such pictures already:  \n",
    "![embeddings relations](https://www.tensorflow.org/images/linear-relationships.png)\n",
    "*From [Vector Representations of Words, Tensorflow tutorial](https://www.tensorflow.org/tutorials/representation/word2vec)*\n",
    "\n",
    "We are going to use these thingies alo-o-ot.\n",
    "\n",
    "Well, we need a proper introduction, nevertheless. Do you remember how we represented sentences last time?\n",
    "\n",
    "We converted a sentence to the bag-of-words:  \n",
    "![](https://i.ibb.co/Tvw1c8S/BOW.png)\n",
    "\n",
    "And each word was represented using one-hot encoding (a vector with one at the position corresponding to the word's index and zeros at all others positions).\n",
    "\n",
    "These one-hot encoding vectors have extremely high dimensions (like, hundreds of thousands or millions). They fit their purpose - to encode information about words. But they have several disadvantages.\n",
    "\n",
    "First of all, they are almost uninterpretable. I mean, all one-hot encoding vectors are orthonormal, so you cannot say that, e.g. `man` and `men` are more similar words than `man` and `crocodiles`.\n",
    "\n",
    "But we want to. Well, NLP researchers in the past few years wanted to, cannot really speak for you.\n",
    "\n",
    "And we're gonna build vectors, that encode semantics!\n",
    "\n",
    "Look at the first picture. It shows relations encoded in the word embeddings space. Such as male-female or verb tense... whatever, just check these two links: http://bionlp-www.utu.fi/wv_demo/, https://lamyiowce.github.io/word2viz/. Go and play with this relations right now! They are funny and you'll get an insight into what the word embeddings can.\n",
    "\n",
    "There is another disadvantage of one-hot encoding vectors: their size. The word embedding vectors we are going to play with have dimensions from 50 to 600 usually. That is by a few orders of magnitude smaller than one-hot encoding vectors.\n",
    "\n",
    "This is crucial for neural networks - they can work only with sufficiently small dense vectors. Well, we'll speak about it later.\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we are going to work with [gensim](https://radimrehurek.com/gensim/) - somewhat standard word embeddings python library. We'll just superficially discuss how it works, but we'll train our model and apply a pretrained one. As a result, you're (probably) gonna understand how to work with word embeddings.\n",
    "\n",
    "In the next notebook, we'll try to work out how word embeddings work and how to implement a module to train word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkxsGQqZNjxj"
   },
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PaOn69Bg1hH-"
   },
   "source": [
    "Well, nothing is interesting in mere training of the word embeddings model. We are gonna apply it to a very concrete task: [Quora Question Pairs](https://www.kaggle.com/c/quora-question-pairs) from kaggle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "z1N9peq_jx40"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "quora_data = pd.read_csv('train.csv')\n",
    "\n",
    "# quora_data.sample(20)[['question1', 'question2', 'is_duplicate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Er7YDUuVOl7F"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8067</th>\n",
       "      <td>How do I play Pokémon GO in Korea?</td>\n",
       "      <td>How do I play Pokémon GO in China?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368101</th>\n",
       "      <td>What are some of the best side dishes for crab...</td>\n",
       "      <td>What are some good side dishes for buffalo chi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70497</th>\n",
       "      <td>Which is more advisable and better material fo...</td>\n",
       "      <td>What is the best server setup for buddypress?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226567</th>\n",
       "      <td>How do I improve logical programming skills?</td>\n",
       "      <td>How can I improve my logical skills for progra...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73186</th>\n",
       "      <td>How close we are to see 3rd world war?</td>\n",
       "      <td>How close is a World War III?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215105</th>\n",
       "      <td>What do Chinese people think about Donald Trump?</td>\n",
       "      <td>What do Chinese people think of Donald Trump?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253209</th>\n",
       "      <td>How many hours a week do Google employees work?</td>\n",
       "      <td>How many hours a day do Google employees work ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354651</th>\n",
       "      <td>How can we follow a Quora question privately w...</td>\n",
       "      <td>How can we view private Instagram pictures wit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104478</th>\n",
       "      <td>Why are cats so overprotective?</td>\n",
       "      <td>How do you know if your cat is overprotective?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163628</th>\n",
       "      <td>How do I improve logical programming skills?</td>\n",
       "      <td>What is the best way to improve logical skills...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258826</th>\n",
       "      <td>How do I access reddit in Indonesia since the ...</td>\n",
       "      <td>How is Reddit governed?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224862</th>\n",
       "      <td>Is China trying to divert river Brahmaputra? I...</td>\n",
       "      <td>Will India suffer, if China blocks Brahmaputra...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121370</th>\n",
       "      <td>What are the travel API's in India?</td>\n",
       "      <td>What is travel API?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213029</th>\n",
       "      <td>What is the Sahara, and how do the average tem...</td>\n",
       "      <td>What is the Sahara, and how do the average tem...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384975</th>\n",
       "      <td>How do I tell my parents I'm not \"grumpy\", I'm...</td>\n",
       "      <td>Why do meat eaters get grumpy?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245470</th>\n",
       "      <td>Moto g4 screen is all black lights light up bu...</td>\n",
       "      <td>What would you do if you owned a dog with a hu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30812</th>\n",
       "      <td>My face skin is so dry, which cream or facewas...</td>\n",
       "      <td>What is the best face cream available in India...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247307</th>\n",
       "      <td>If you invented and patented a home improvemen...</td>\n",
       "      <td>How much time would it take to break a 128bit ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233652</th>\n",
       "      <td>Are Donald Trump's policies too draconian?</td>\n",
       "      <td>What are Donald Trump's policies?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236251</th>\n",
       "      <td>Are there any family offices that invest in mo...</td>\n",
       "      <td>How Descartes' “cogito ergo sum” affected Kant...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                question1  \\\n",
       "8067                   How do I play Pokémon GO in Korea?   \n",
       "368101  What are some of the best side dishes for crab...   \n",
       "70497   Which is more advisable and better material fo...   \n",
       "226567       How do I improve logical programming skills?   \n",
       "73186              How close we are to see 3rd world war?   \n",
       "215105   What do Chinese people think about Donald Trump?   \n",
       "253209    How many hours a week do Google employees work?   \n",
       "354651  How can we follow a Quora question privately w...   \n",
       "104478                    Why are cats so overprotective?   \n",
       "163628       How do I improve logical programming skills?   \n",
       "258826  How do I access reddit in Indonesia since the ...   \n",
       "224862  Is China trying to divert river Brahmaputra? I...   \n",
       "121370                What are the travel API's in India?   \n",
       "213029  What is the Sahara, and how do the average tem...   \n",
       "384975  How do I tell my parents I'm not \"grumpy\", I'm...   \n",
       "245470  Moto g4 screen is all black lights light up bu...   \n",
       "30812   My face skin is so dry, which cream or facewas...   \n",
       "247307  If you invented and patented a home improvemen...   \n",
       "233652         Are Donald Trump's policies too draconian?   \n",
       "236251  Are there any family offices that invest in mo...   \n",
       "\n",
       "                                                question2  is_duplicate  \n",
       "8067                   How do I play Pokémon GO in China?             0  \n",
       "368101  What are some good side dishes for buffalo chi...             0  \n",
       "70497       What is the best server setup for buddypress?             0  \n",
       "226567  How can I improve my logical skills for progra...             1  \n",
       "73186                       How close is a World War III?             1  \n",
       "215105      What do Chinese people think of Donald Trump?             1  \n",
       "253209  How many hours a day do Google employees work ...             0  \n",
       "354651  How can we view private Instagram pictures wit...             0  \n",
       "104478     How do you know if your cat is overprotective?             1  \n",
       "163628  What is the best way to improve logical skills...             1  \n",
       "258826                            How is Reddit governed?             0  \n",
       "224862  Will India suffer, if China blocks Brahmaputra...             1  \n",
       "121370                                What is travel API?             0  \n",
       "213029  What is the Sahara, and how do the average tem...             1  \n",
       "384975                     Why do meat eaters get grumpy?             0  \n",
       "245470  What would you do if you owned a dog with a hu...             0  \n",
       "30812   What is the best face cream available in India...             0  \n",
       "247307  How much time would it take to break a 128bit ...             0  \n",
       "233652                  What are Donald Trump's policies?             0  \n",
       "236251  How Descartes' “cogito ergo sum” affected Kant...             0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quora_data.sample(20)[['question1', 'question2', 'is_duplicate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "id": "ybl95489HDwA",
    "outputId": "cbc02ce0-00b8-42bf-c8c5-0cea0bcb7cfd"
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 669
    },
    "id": "2kJb3ZuJG90s",
    "outputId": "41dccb1a-d46f-4a07-d43d-b76f9fbb95fc"
   },
   "outputs": [],
   "source": [
    "#quora_data.sample(20)[['question1', 'question2', 'is_duplicate']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p13HdkzWKtKe"
   },
   "source": [
    "You see, the dataset consists of question pairs and you have to determine which of them are duplicates and which are not.\n",
    "\n",
    "Well, I'm not promising that we'll achieve good results right now, but still... Let's train Word2vec gensim model!\n",
    "\n",
    "*Word2vec is the most popular method of building word embeddings. We'll implement it next time, right now let's believe that it just do whatever we want.*\n",
    "\n",
    "First of all, we need to collect available texts to pass them to Word2vec model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Mchv4fS_21OX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the step by step guide to invest in share market in india?',\n",
       " 'What is the story of Kohinoor (Koh-i-Noor) Diamond?',\n",
       " 'How can I increase the speed of my internet connection while using a VPN?',\n",
       " 'Why am I mentally very lonely? How can I solve it?',\n",
       " 'Which one dissolve in water quikly sugar, salt, methane and carbon di oxide?',\n",
       " 'Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?',\n",
       " 'Should I buy tiago?',\n",
       " 'How can I be a good geologist?',\n",
       " 'When do you use シ instead of し?',\n",
       " 'Motorola (company): Can I hack my Charter Motorolla DCX3400?']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "quora_data.question1 = quora_data.question1.replace(np.nan, '', regex=True)\n",
    "quora_data.question2 = quora_data.question2.replace(np.nan, '', regex=True)\n",
    "\n",
    "texts = list(pd.concat([quora_data.question1, quora_data.question2]).unique())\n",
    "texts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hZMFAmvK5b7"
   },
   "source": [
    "Next, we have to tokenize the texts. This time we'll use `nltk` - another great NLP library. \n",
    "\n",
    "It goes this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "LTxolf8nLM-n"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What',\n",
       " 'is',\n",
       " 'the',\n",
       " 'step',\n",
       " 'by',\n",
       " 'step',\n",
       " 'guide',\n",
       " 'to',\n",
       " 'invest',\n",
       " 'in',\n",
       " 'share',\n",
       " 'market',\n",
       " 'in',\n",
       " 'india',\n",
       " '?']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuJceE4JLRxK"
   },
   "source": [
    "**Task** Your turn: lowercase all the texts and tokenize them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "a7XbnSdt4REg"
   },
   "outputs": [],
   "source": [
    "tokenized_texts = [*map(lambda text: word_tokenize(text.lower()), texts)]\n",
    "\n",
    "assert all(isinstance(row, (list, tuple)) for row in tokenized_texts), \\\n",
    "    \"please convert each line into a list of tokens\"\n",
    "assert all(all(isinstance(tok, str) for tok in row) for row in tokenized_texts), \\\n",
    "    \"please convert each line into a list of tokens\"\n",
    "\n",
    "is_latin = lambda tok: all('a' <= x.lower() <= 'z' for x in tok)\n",
    "assert all(not is_latin(token) or token.islower() for tokens in tokenized_texts for token in tokens),\\\n",
    "    \"please lowercase each line\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "irl7RotC5C_B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what is the step by step guide to invest in share market in india ?', 'what is the story of kohinoor ( koh-i-noor ) diamond ?']\n"
     ]
    }
   ],
   "source": [
    "print([' '.join(row) for row in tokenized_texts[:2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kj4dC3iLdwH"
   },
   "source": [
    "And we are ready to train a small model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "9GNuiLio8M25",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\agama\\anaconda3\\envs\\course\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(tokenized_texts, \n",
    "                 vector_size=32,      # embedding vector size\n",
    "                 min_count=5,  # consider words that occured at least 5 times\n",
    "                 window=5,     # define context as a 5-word window around the target word\n",
    "                 seed=0,       # + workers=1 is to make model reproducible\n",
    "                 workers=1).wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JclToDJMNwTy"
   },
   "source": [
    "## Analyzing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBVR2kY7LkCs"
   },
   "source": [
    "Yay, we have our own model, let's play with it!\n",
    "\n",
    "To get word's vector, well, call `get_vector`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Jk6Fgraj-j3c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.6059568 ,  2.460762  , -0.5609757 , -0.6594619 , -3.2099524 ,\n",
       "        2.4934635 ,  0.9850607 ,  0.5102441 , -1.0140994 ,  2.0118055 ,\n",
       "       -1.815692  , -4.2941537 , -1.9720241 , -0.02941428,  3.9537377 ,\n",
       "        0.21149045, -3.5083218 ,  1.6357796 ,  2.3724627 ,  2.1486197 ,\n",
       "        3.8126264 , -3.0769866 , -1.653677  , -0.72145635, -1.6492962 ,\n",
       "       -0.757187  ,  0.23071285,  0.01671845,  1.6631601 ,  0.03714013,\n",
       "        2.1424541 ,  1.8517803 ], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_vector('anything')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qHKRy7HyLuxH"
   },
   "source": [
    "To get most similar words for the given one (guess, what):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "toyNzyTB-p70"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rice', 0.9563511610031128),\n",
       " ('fruit', 0.947577178478241),\n",
       " ('butter', 0.9317622184753418),\n",
       " ('vodka', 0.9248947501182556),\n",
       " ('sauce', 0.9244229197502136),\n",
       " ('beans', 0.914040207862854),\n",
       " ('honey', 0.9098433256149292),\n",
       " ('banana', 0.9076601266860962),\n",
       " ('flour', 0.906358003616333),\n",
       " ('pasta', 0.9051861763000488)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('bread')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A510z5gTL00E"
   },
   "source": [
    "And it can do such magic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "j2A_DF5E-ucq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('photographer', 0.6976268291473389),\n",
       " ('betting', 0.6549664735794067),\n",
       " ('volunteer', 0.6211757063865662),\n",
       " ('writer', 0.6180971264839172),\n",
       " ('youtuber', 0.6097328662872314),\n",
       " ('athlete', 0.597032368183136),\n",
       " ('gifts', 0.5961406230926514),\n",
       " ('conference', 0.5952812433242798),\n",
       " ('basketball', 0.586754560470581),\n",
       " ('tutor', 0.5855691432952881)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['coder', 'money'], negative=['brain'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-_uKG4vNIJv"
   },
   "source": [
    "That is, who is like coder, with money and without brains.\n",
    "\n",
    "And this too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "l_mzQgi4L474"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('romantic', 0.7079866528511047),\n",
       " ('actress', 0.6925302147865295),\n",
       " ('girl/woman', 0.6619561910629272),\n",
       " ('ted', 0.6391326189041138),\n",
       " ('bhatt', 0.6329595446586609),\n",
       " ('alia', 0.615281343460083),\n",
       " ('thoughtful', 0.6080158352851868),\n",
       " ('effortlessly', 0.6065546274185181),\n",
       " ('astrologer', 0.6039962768554688),\n",
       " ('farewell', 0.6033608317375183)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar([model.get_vector('politician') - model.get_vector('power') + model.get_vector('honesty')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5BAEyMyL2kx"
   },
   "source": [
    "Honest politician without power, isn't it just cute.\n",
    "\n",
    "**Task** Play with it. And yes, I'm serious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "4F_3hQxqn16s"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('teacher', 0.7798760533332825),\n",
       " ('therapist', 0.7773643732070923),\n",
       " ('married', 0.7656753063201904),\n",
       " ('teenager', 0.7491468787193298),\n",
       " ('girlfriend', 0.7469215393066406),\n",
       " ('dentist', 0.7425242066383362),\n",
       " ('girl', 0.7387226223945618),\n",
       " ('student', 0.7370373010635376),\n",
       " ('professor', 0.7302892208099365),\n",
       " ('lawyer', 0.7270255088806152)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['doctor','woman'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FYuh4DKN1Fd"
   },
   "source": [
    "## Visualizing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OdT0eIEiN4Ja"
   },
   "source": [
    "Let's now look at the projection of the first 1000 the most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "d1e9yS0zBr6j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['?', 'money', 'study', 's', '6', 'physics', 'says', 'view', 'meet', 'changed']\n"
     ]
    }
   ],
   "source": [
    "words = sorted(list(model.key_to_index), #model.vocab.keys()\n",
    "               key=lambda word: model.get_vecattr(word, \"count\"), #model.vocab[word].count\n",
    "               reverse=True)[:1000]\n",
    "\n",
    "print(words[::100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Yk5pgMXOESS"
   },
   "source": [
    "**Task** Build the matrix from these words' vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "kQu724f2CAh0"
   },
   "outputs": [],
   "source": [
    "word_vectors = model.vectors[[model.key_to_index[word] for word in words]]\n",
    "\n",
    "assert isinstance(word_vectors, np.ndarray)\n",
    "assert word_vectors.shape == (len(words), model.vectors.shape[1])\n",
    "assert np.isfinite(word_vectors).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XA3onWUbcWAo"
   },
   "source": [
    "Now we would try to project this 32 dimensional vectors to the more convenient 2D space to be able to look on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7cgxin-OTvK"
   },
   "source": [
    "### PCA\n",
    "\n",
    "The simplest linear method of dimension reduction is __P__rincipial __C__omponent __A__nalysis.\n",
    "\n",
    "PCA builds so called principal components - set of variables along which our data has the largest variance:  \n",
    "\n",
    "![pca](https://i.stack.imgur.com/Q7HIP.gif)\n",
    "*From the great answer [https://stats.stackexchange.com/a/140579](https://stats.stackexchange.com/a/140579)*\n",
    "\n",
    "For instance, in the picture, the rotating line represents possible variants of the first principal component. If we want to project 2D set of dots to one dimension, we'll probably want to save as much information as possible. The maximum variance position of the rotating line gives us more information about the dots than all other positions.\n",
    "\n",
    "Really nice illustrations of this mechanism live [here](http://setosa.io/ev/principal-component-analysis/).\n",
    "\n",
    "To be short, project multi-dimensional space on the first two or three components and enjoy fast-and-dirty dimensional reduction.\n",
    "\n",
    "**Task** Use [sklearn PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) to project data to 2D. Centre and normalize the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "V0fQKZw2Css4"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def get_pca_projection(word_vectors):\n",
    "    # <implement me>\n",
    "    pca = PCA(n_components=2)\n",
    "    word_vectors = pca.fit_transform(word_vectors)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(word_vectors)\n",
    "    return scaler.transform(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "a_9VtLl9CviW"
   },
   "outputs": [],
   "source": [
    "word_vectors_pca = get_pca_projection(word_vectors)\n",
    "\n",
    "assert word_vectors_pca.shape == (len(word_vectors), 2), \"there must be a 2d vector for each word\"\n",
    "assert max(abs(word_vectors_pca.mean(0))) < 1e-5, \"points must be zero-centered\"\n",
    "assert max(abs(1 - word_vectors_pca.std(0))) < 1e-5, \"points must have unit variance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGyvhrk7Rlyt"
   },
   "source": [
    "Let's visualize the embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "1U58YxF3Cx0W"
   },
   "outputs": [],
   "source": [
    "import bokeh.models as bm, bokeh.plotting as pl\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
    "                 width=600, height=400, show=True, **kwargs):\n",
    "    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n",
    "    output_notebook()\n",
    "    \n",
    "    if isinstance(color, str): \n",
    "        color = [color] * len(x)\n",
    "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
    "\n",
    "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
    "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
    "\n",
    "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
    "    if show: \n",
    "        pl.show(fig)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "qBljy2hCC1qX"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      if (url in hashes) {\n",
       "        element.crossOrigin = \"anonymous\";\n",
       "        element.integrity = \"sha384-\" + hashes[url];\n",
       "      }\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"f7cb12d6-866c-446e-b05a-25afff490016\" data-root-id=\"1003\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"bcb7e8bd-3e50-4c94-8186-6f20ce981c1d\":{\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"1012\"}],\"center\":[{\"id\":\"1015\"},{\"id\":\"1019\"}],\"left\":[{\"id\":\"1016\"}],\"plot_height\":400,\"renderers\":[{\"id\":\"1037\"}],\"title\":{\"id\":\"1042\"},\"toolbar\":{\"id\":\"1027\"},\"x_range\":{\"id\":\"1004\"},\"x_scale\":{\"id\":\"1008\"},\"y_range\":{\"id\":\"1006\"},\"y_scale\":{\"id\":\"1010\"}},\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{},\"id\":\"1013\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1010\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"1006\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"1004\",\"type\":\"DataRange1d\"},{\"attributes\":{\"callback\":null,\"tooltips\":[[\"token\",\"@token\"]]},\"id\":\"1039\",\"type\":\"HoverTool\"},{\"attributes\":{\"formatter\":{\"id\":\"1044\"},\"ticker\":{\"id\":\"1013\"}},\"id\":\"1012\",\"type\":\"LinearAxis\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"field\":\"color\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"field\":\"color\"},\"size\":{\"units\":\"screen\",\"value\":10},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1036\",\"type\":\"Scatter\"},{\"attributes\":{\"axis\":{\"id\":\"1012\"},\"ticker\":null},\"id\":\"1015\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1025\",\"type\":\"HelpTool\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":{\"id\":\"1021\"},\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1020\"},{\"id\":\"1021\"},{\"id\":\"1022\"},{\"id\":\"1023\"},{\"id\":\"1024\"},{\"id\":\"1025\"},{\"id\":\"1039\"}]},\"id\":\"1027\",\"type\":\"Toolbar\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":0.5,\"fill_color\":\"lightgrey\",\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":1.0,\"line_color\":\"black\",\"line_dash\":[4,4],\"line_width\":2,\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"1026\",\"type\":\"BoxAnnotation\"},{\"attributes\":{},\"id\":\"1049\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"axis\":{\"id\":\"1016\"},\"dimension\":1,\"ticker\":null},\"id\":\"1019\",\"type\":\"Grid\"},{\"attributes\":{\"formatter\":{\"id\":\"1046\"},\"ticker\":{\"id\":\"1017\"}},\"id\":\"1016\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"1023\",\"type\":\"SaveTool\"},{\"attributes\":{},\"id\":\"1044\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"1024\",\"type\":\"ResetTool\"},{\"attributes\":{\"data\":{\"color\":[\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\"],\"token\":[\"?\",\"the\",\"what\",\"is\",\"a\",\"i\",\"to\",\"in\",\"how\",\"of\",\"do\",\"are\",\"and\",\"for\",\",\",\"can\",\"you\",\"why\",\"it\",\"my\",\"does\",\"best\",\"on\",\".\",\"or\",\"have\",\"if\",\"be\",\"with\",\"which\",\"that\",\"an\",\"some\",\"should\",\"'s\",\"get\",\"from\",\")\",\"your\",\"(\",\"like\",\"when\",\"at\",\"india\",\"good\",\"who\",\"there\",\"will\",\"as\",\"would\",\"people\",\"not\",\"n't\",\"about\",\"``\",\"''\",\"between\",\"one\",\"did\",\"any\",\"we\",\"me\",\"where\",\"most\",\"was\",\"by\",\"make\",\"so\",\"they\",\"this\",\"am\",\"after\",\"way\",\":\",\"has\",\"use\",\"much\",\"difference\",\"time\",\"life\",\"their\",\"know\",\"work\",\"many\",\"but\",\"than\",\"more\",\"all\",\"want\",\"quora\",\"someone\",\"learn\",\"find\",\"other\",\"think\",\"new\",\"better\",\"job\",\"indian\",\"out\",\"money\",\"mean\",\"become\",\"ever\",\"world\",\"without\",\"he\",\"start\",\"take\",\"us\",\"up\",\"first\",\"feel\",\"year\",\"into\",\"go\",\"online\",\"used\",\"engineering\",\"could\",\"love\",\"'m\",\"person\",\"were\",\"possible\",\"day\",\"buy\",\"things\",\"being\",\"need\",\"business\",\"using\",\"them\",\"really\",\"trump\",\"girl\",\"'\",\"her\",\"his\",\"years\",\"different\",\"long\",\"phone\",\"google\",\"company\",\"been\",\"old\",\"only\",\"no\",\"now\",\"just\",\"2\",\"app\",\"college\",\"facebook\",\"number\",\"free\",\"books\",\"2016\",\"movie\",\"still\",\"its\",\"account\",\"ca\",\"women\",\"book\",\"english\",\"she\",\"while\",\"had\",\"change\",\"ways\",\"computer\",\"thing\",\"examples\",\"data\",\"country\",\"over\",\"see\",\"android\",\"help\",\"science\",\"live\",\"school\",\"software\",\"before\",\"&\",\"language\",\"same\",\"going\",\"bad\",\"sex\",\"student\",\"stop\",\"university\",\"happen\",\"back\",\"made\",\"3\",\"1\",\"study\",\"our\",\"two\",\"system\",\"through\",\"name\",\"say\",\"real\",\"during\",\"prepare\",\"water\",\"iphone\",\"website\",\"top\",\"car\",\"important\",\"questions\",\"men\",\"give\",\"getting\",\"anyone\",\"companies\",\"high\",\"black\",\"card\",\"read\",\"programming\",\"war\",\"learning\",\"10\",\"5\",\"\\u2019\",\"exam\",\"[\",\"then\",\"]\",\"even\",\"movies\",\"china\",\"mobile\",\"cost\",\"donald\",\"right\",\"doing\",\"friend\",\"him\",\"working\",\"under\",\"come\",\"president\",\"own\",\"question\",\"career\",\"experience\",\"bank\",\"true\",\"friends\",\"guy\",\"word\",\"hair\",\"home\",\"video\",\"having\",\"look\",\"usa\",\"tell\",\"man\",\"social\",\"web\",\"interview\",\"very\",\"engineer\",\"write\",\"game\",\"government\",\"weight\",\"earth\",\"girls\",\"service\",\"food\",\"students\",\"play\",\"countries\",\"human\",\"place\",\"future\",\"improve\",\"off\",\"big\",\"days\",\"happens\",\"'ve\",\"done\",\"class\",\"eat\",\"tv\",\"process\",\"got\",\"state\",\"average\",\"s\",\"meaning\",\"relationship\",\"music\",\"too\",\"math\",\"create\",\"instagram\",\"every\",\"history\",\"last\",\"pay\",\"windows\",\"4\",\"white\",\"watch\",\"%\",\"salary\",\"body\",\"power\",\"safe\",\"clinton\",\"ask\",\"age\",\"laptop\",\"makes\",\"$\",\"hard\",\"each\",\"lose\",\"american\",\"delhi\",\"youtube\",\"energy\",\"worth\",\"earn\",\"states\",\"against\",\"win\",\"girlfriend\",\"great\",\"keep\",\"test\",\"god\",\"hillary\",\"compare\",\"market\",\"differences\",\"considered\",\"making\",\"something\",\"answer\",\"apply\",\"myself\",\"mba\",\"tips\",\"around\",\"never\",\"next\",\"always\",\"mechanical\",\"another\",\"united\",\"java\",\"c\",\"increase\",\"such\",\"course\",\"jobs\",\"download\",\"song\",\"parents\",\"kind\",\"internet\",\"common\",\"review\",\"woman\",\"code\",\"design\",\"per\",\"employees\",\"end\",\"development\",\"chinese\",\"series\",\"score\",\"interesting\",\"degree\",\"month\",\"travel\",\"show\",\"management\",\"months\",\"able\",\"believe\",\"open\",\"program\",\"living\",\"looking\",\"because\",\"6\",\"type\",\"bangalore\",\"light\",\"favorite\",\"america\",\"today\",\"actually\",\"major\",\"family\",\"marketing\",\"pakistan\",\"universities\",\"technology\",\"law\",\"call\",\"whatsapp\",\"affect\",\"idea\",\"{\",\"build\",\"}\",\"choose\",\"cat\",\"current\",\"support\",\"problem\",\"popular\",\"speed\",\"air\",\"rid\",\"deal\",\"visa\",\"join\",\"run\",\"games\",\"space\",\"down\",\"both\",\"well\",\"city\",\"2017\",\"house\",\"exist\",\"services\",\"available\",\"civil\",\"u.s.\",\"site\",\"culture\",\"also\",\"normal\",\"given\",\"wrong\",\"apple\",\"7\",\"small\",\"date\",\"order\",\"wear\",\"apps\",\"places\",\"songs\",\"project\",\"differ\",\"product\",\"international\",\"x\",\"main\",\"behind\",\"research\",\"happened\",\"boyfriend\",\"media\",\"public\",\"canada\",\"value\",\"jee\",\"cause\",\"reason\",\"uk\",\"education\",\"email\",\"ms\",\"sleep\",\"-\",\"amazon\",\"instead\",\"these\",\"benefits\",\"startup\",\"less\",\"iit\",\"list\",\"part\",\"based\",\"die\",\"medical\",\"websites\",\"etc\",\"physics\",\"skills\",\"child\",\"form\",\"health\",\"causes\",\"times\",\"called\",\"visit\",\"hate\",\"level\",\"worst\",\"legal\",\"police\",\"control\",\"terms\",\"stay\",\"products\",\"mumbai\",\"post\",\"facts\",\"mind\",\"face\",\"application\",\"rate\",\"sites\",\"field\",\"file\",\"required\",\"seen\",\"sell\",\"humans\",\"private\",\"machine\",\"story\",\"dog\",\"notes\",\"stock\",\"modi\",\"biggest\",\"asked\",\"invest\",\"children\",\"point\",\"writing\",\"move\",\"night\",\"problems\",\"gate\",\"'re\",\"put\",\"single\",\"successful\",\"anything\",\"indians\",\"others\",\"remove\",\"compared\",\"theory\",\"death\",\"ideas\",\"talk\",\"marks\",\"answers\",\"plan\",\"similar\",\"sentence\",\"group\",\"low\",\"side\",\"institute\",\"hours\",\"again\",\"effects\",\"/math\",\"coaching\",\"australia\",\"advantages\",\"solve\",\"correct\",\"function\",\"tax\",\"lot\",\"center\",\"famous\",\"send\",\"germany\",\"guys\",\"add\",\"set\",\"offer\",\"south\",\"force\",\"credit\",\"foreign\",\"information\",\"universe\",\"fall\",\"daily\",\"period\",\"says\",\"developer\",\"training\",\"full\",\"videos\",\"area\",\"industry\",\"oil\",\"model\",\"studying\",\"general\",\"majors\",\"colleges\",\"star\",\"graduate\",\"size\",\"yourself\",\"python\",\"drive\",\"2015\",\"blood\",\"near\",\"enough\",\"search\",\"price\",\"kill\",\"quality\",\"words\",\"8\",\"marriage\",\"cons\",\"found\",\"often\",\"\\u201d\",\"leave\",\"pros\",\"fast\",\"grow\",\"started\",\"courses\",\"week\",\"fix\",\"related\",\"purpose\",\"network\",\"types\",\"russia\",\"advice\",\"those\",\"password\",\"follow\",\"married\",\"taking\",\"mass\",\"male\",\"store\",\"turn\",\"party\",\"left\",\"rs\",\"brain\",\"\\u201c\",\"100\",\"numbers\",\"500\",\"female\",\"share\",\"digital\",\"team\",\"1000\",\"effective\",\"economy\",\"text\",\"term\",\"delete\",\"election\",\"cell\",\"pc\",\"master\",\"office\",\"matter\",\"few\",\"since\",\"role\",\"preparation\",\"fat\",\"second\",\"page\",\"dark\",\"short\",\"said\",\"happy\",\"red\",\"line\",\"away\",\"security\",\"options\",\"effect\",\"easy\",\"three\",\"view\",\"admission\",\"must\",\"chemical\",\"source\",\"lost\",\"try\",\"dream\",\"understand\",\"care\",\"exams\",\"convert\",\"americans\",\"past\",\"dogs\",\"electrical\",\"profile\",\"alcohol\",\"exactly\",\"north\",\"ios\",\"develop\",\"paper\",\"japanese\",\"investment\",\"tech\",\"hotel\",\"difficult\",\"check\",\"beautiful\",\"personal\",\"calculate\",\"uber\",\"japan\",\"letter\",\"eating\",\"religion\",\"explain\",\"20\",\"languages\",\"c++\",\"hyderabad\",\"b\",\"topics\",\"british\",\"basic\",\"twitter\",\"news\",\"self\",\"california\",\"green\",\"political\",\"transfer\",\"fight\",\"wife\",\"12\",\"drug\",\"rank\",\"let\",\"scope\",\"porn\",\"film\",\"army\",\"pune\",\"t\",\"national\",\"once\",\"note\",\"contact\",\"known\",\"avoid\",\"boy\",\"gain\",\"please\",\"address\",\"wants\",\"reasons\",\"paid\",\"engine\",\"obama\",\"europe\",\"ex\",\"currently\",\"bollywood\",\"samsung\",\"smart\",\"season\",\"everyone\",\"phd\",\"income\",\"officer\",\"reduce\",\"chances\",\"option\",\"overcome\",\"everything\",\"across\",\"reading\",\"military\",\"else\",\"meet\",\"users\",\"15\",\"solar\",\"message\",\"knowledge\",\"charge\",\"due\",\"sound\",\"balance\",\"microsoft\",\"internship\",\"!\",\"faster\",\"pregnant\",\"photos\",\"yes\",\"sim\",\"moon\",\"taken\",\"financial\",\"hindi\",\"pass\",\"disadvantages\",\"animals\",\"messages\",\"gay\",\"/\",\"healthy\",\"hack\",\"amount\",\"systems\",\"pain\",\"interested\",\"color\",\"ias\",\"singapore\",\"skin\",\"natural\",\"screen\",\"chance\",\"names\",\"board\",\"case\",\"height\",\"special\",\"passport\",\"structure\",\"break\",\"pro\",\"professional\",\"created\",\"drink\",\"already\",\"percentage\",\"useful\",\"french\",\"crush\",\"sun\",\"dating\",\"likes\",\"presidential\",\"battery\",\"interest\",\"access\",\"coming\",\"provide\",\"muslim\",\"modern\",\"starting\",\"linux\",\"camera\",\"recover\",\"german\",\"expect\",\"vote\",\"blue\",\"muslims\",\"train\",\"studies\",\"install\",\"least\",\"solution\",\"thinking\",\"following\",\"mother\",\"negative\",\"jio\",\"spend\",\"mac\",\"prime\",\"user\",\"written\",\"buying\",\"feeling\",\"running\",\"football\",\"cold\",\"gift\",\"allowed\",\"changed\",\"pressure\",\"brand\",\"character\",\"rich\",\"resources\",\"snapchat\",\"within\",\"speak\",\"islam\",\"impact\",\"yet\",\"non\",\"marry\",\"eyes\",\"society\",\"bill\",\"though\",\"likely\",\"determine\",\"shows\",\"fake\",\"save\",\"capital\",\"final\",\"gmail\",\"picture\",\"middle\",\"laws\",\"nuclear\",\"applications\",\"chemistry\",\"young\",\"kids\",\"prefer\",\"device\",\"iq\",\"father\",\"galaxy\",\"views\",\"manager\",\"doctor\",\"personality\",\"hand\",\"greatest\",\"currency\",\"inside\",\"illegal\",\"strategy\",\"stories\",\"recruit\",\"abroad\",\"pursue\",\"cs\",\"gold\",\"works\",\"consider\",\"projects\",\"crack\",\"blog\",\"memory\",\"example\",\"neet\",\"schools\",\"growth\",\"husband\",\"necessary\",\"insurance\",\"alone\",\"economics\",\"depression\",\"dead\",\"b.tech\",\"higher\",\"cse\",\"30\",\"loss\",\"among\",\"easiest\",\"deleted\",\"art\",\"files\",\"macbook\",\"together\",\"suggest\",\"vs\",\"wifi\",\"gas\",\"late\",\"intelligence\",\"gre\",\"method\",\"head\",\"version\",\"minimum\",\"chennai\",\"tools\",\"easily\",\"cars\",\"policy\"],\"x\":{\"__ndarray__\":\"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\",\"dtype\":\"float32\",\"order\":\"little\",\"shape\":[1000]},\"y\":{\"__ndarray__\":\"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\",\"dtype\":\"float32\",\"order\":\"little\",\"shape\":[1000]}},\"selected\":{\"id\":\"1048\"},\"selection_policy\":{\"id\":\"1049\"}},\"id\":\"1002\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"source\":{\"id\":\"1002\"}},\"id\":\"1038\",\"type\":\"CDSView\"},{\"attributes\":{\"data_source\":{\"id\":\"1002\"},\"glyph\":{\"id\":\"1035\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1036\"},\"selection_glyph\":null,\"view\":{\"id\":\"1038\"}},\"id\":\"1037\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"text\":\"\"},\"id\":\"1042\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"1046\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"overlay\":{\"id\":\"1026\"}},\"id\":\"1022\",\"type\":\"BoxZoomTool\"},{\"attributes\":{},\"id\":\"1048\",\"type\":\"Selection\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.25},\"fill_color\":{\"field\":\"color\"},\"line_alpha\":{\"value\":0.25},\"line_color\":{\"field\":\"color\"},\"size\":{\"units\":\"screen\",\"value\":10},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1035\",\"type\":\"Scatter\"},{\"attributes\":{},\"id\":\"1008\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"1017\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1021\",\"type\":\"WheelZoomTool\"},{\"attributes\":{},\"id\":\"1020\",\"type\":\"PanTool\"}],\"root_ids\":[\"1003\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\n",
       "  var render_items = [{\"docid\":\"bcb7e8bd-3e50-4c94-8186-6f20ce981c1d\",\"root_ids\":[\"1003\"],\"roots\":{\"1003\":\"f7cb12d6-866c-446e-b05a-25afff490016\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1003"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style=\"display: table;\"><div style=\"display: table-row;\"><div style=\"display: table-cell;\"><b title=\"bokeh.plotting.figure.Figure\">Figure</b>(</div><div style=\"display: table-cell;\">id&nbsp;=&nbsp;'1003', <span id=\"1096\" style=\"cursor: pointer;\">&hellip;)</span></div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">above&nbsp;=&nbsp;[],</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">align&nbsp;=&nbsp;'start',</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">aspect_ratio&nbsp;=&nbsp;None,</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">aspect_scale&nbsp;=&nbsp;1,</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">background&nbsp;=&nbsp;None,</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">background_fill_alpha&nbsp;=&nbsp;1.0,</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">background_fill_color&nbsp;=&nbsp;'#ffffff',</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">below&nbsp;=&nbsp;[LinearAxis(id='1012', ...)],</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">border_fill_alpha&nbsp;=&nbsp;1.0,</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">border_fill_color&nbsp;=&nbsp;'#ffffff',</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">center&nbsp;=&nbsp;[Grid(id='1015', ...), Grid(id='1019', ...)],</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">css_classes&nbsp;=&nbsp;[],</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">disabled&nbsp;=&nbsp;False,</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">extra_x_ranges&nbsp;=&nbsp;{},</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">extra_y_ranges&nbsp;=&nbsp;{},</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">frame_height&nbsp;=&nbsp;None,</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">frame_width&nbsp;=&nbsp;None,</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">height&nbsp;=&nbsp;None,</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">height_policy&nbsp;=&nbsp;'auto',</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">hidpi&nbsp;=&nbsp;True,</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">js_event_callbacks&nbsp;=&nbsp;{},</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">js_property_callbacks&nbsp;=&nbsp;{},</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">left&nbsp;=&nbsp;[LinearAxis(id='1016', ...)],</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">lod_factor&nbsp;=&nbsp;10,</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">lod_interval&nbsp;=&nbsp;300,</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">lod_threshold&nbsp;=&nbsp;2000,</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">lod_timeout&nbsp;=&nbsp;500,</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">margin&nbsp;=&nbsp;(0, 0, 0, 0),</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">match_aspect&nbsp;=&nbsp;False,</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">max_height&nbsp;=&nbsp;None,</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">max_width&nbsp;=&nbsp;None,</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">min_border&nbsp;=&nbsp;5,</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">min_border_bottom&nbsp;=&nbsp;None,</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">min_border_left&nbsp;=&nbsp;None,</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">min_border_right&nbsp;=&nbsp;None,</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">min_border_top&nbsp;=&nbsp;None,</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">min_height&nbsp;=&nbsp;None,</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">min_width&nbsp;=&nbsp;None,</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">name&nbsp;=&nbsp;None,</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">outline_line_alpha&nbsp;=&nbsp;1.0,</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">outline_line_cap&nbsp;=&nbsp;'butt',</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">outline_line_color&nbsp;=&nbsp;'#e5e5e5',</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">outline_line_dash&nbsp;=&nbsp;[],</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">outline_line_dash_offset&nbsp;=&nbsp;0,</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">outline_line_join&nbsp;=&nbsp;'bevel',</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">outline_line_width&nbsp;=&nbsp;1,</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">output_backend&nbsp;=&nbsp;'canvas',</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">plot_height&nbsp;=&nbsp;400,</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">plot_width&nbsp;=&nbsp;600,</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">renderers&nbsp;=&nbsp;[GlyphRenderer(id='1037', ...)],</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">reset_policy&nbsp;=&nbsp;'standard',</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">right&nbsp;=&nbsp;[],</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">sizing_mode&nbsp;=&nbsp;None,</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">subscribed_events&nbsp;=&nbsp;[],</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">tags&nbsp;=&nbsp;[],</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">title&nbsp;=&nbsp;Title(id='1042', ...),</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">title_location&nbsp;=&nbsp;'above',</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">toolbar&nbsp;=&nbsp;Toolbar(id='1027', ...),</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">toolbar_location&nbsp;=&nbsp;'right',</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">toolbar_sticky&nbsp;=&nbsp;True,</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">visible&nbsp;=&nbsp;True,</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">width&nbsp;=&nbsp;None,</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">width_policy&nbsp;=&nbsp;'auto',</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">x_range&nbsp;=&nbsp;DataRange1d(id='1004', ...),</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">x_scale&nbsp;=&nbsp;LinearScale(id='1008', ...),</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">y_range&nbsp;=&nbsp;DataRange1d(id='1006', ...),</div></div><div class=\"1095\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">y_scale&nbsp;=&nbsp;LinearScale(id='1010', ...))</div></div></div>\n",
       "<script>\n",
       "(function() {\n",
       "  var expanded = false;\n",
       "  var ellipsis = document.getElementById(\"1096\");\n",
       "  ellipsis.addEventListener(\"click\", function() {\n",
       "    var rows = document.getElementsByClassName(\"1095\");\n",
       "    for (var i = 0; i < rows.length; i++) {\n",
       "      var el = rows[i];\n",
       "      el.style.display = expanded ? \"none\" : \"table-row\";\n",
       "    }\n",
       "    ellipsis.innerHTML = expanded ? \"&hellip;)\" : \"&lsaquo;&lsaquo;&lsaquo;\";\n",
       "    expanded = !expanded;\n",
       "  });\n",
       "})();\n",
       "</script>\n"
      ],
      "text/plain": [
       "Figure(id='1003', ...)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draw_vectors(word_vectors_pca[:, 0], word_vectors_pca[:, 1], token=words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOIU8uXnSItf"
   },
   "source": [
    "### T-SNE\n",
    "\n",
    "There is a more complicated method of data visualization. It's called t-SNE. You can gain an intuition behind it from [this](https://distill.pub/2016/misread-tsne/) article (warning: even more beautiful illustrations).\n",
    "\n",
    "**Task** Well, the same as the previous one: apply [TSNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html), normalize and center the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "F-nlN4_aDF9G"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def get_tsne_projection(word_vectors):\n",
    "    # <fill me>\n",
    "    tsne = TSNE(n_components=2)\n",
    "    word_vectors = tsne.fit_transform(word_vectors)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(word_vectors)\n",
    "    return scaler.transform(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "D_YFR_rYDK2n"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1099\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1099\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      if (url in hashes) {\n",
       "        element.crossOrigin = \"anonymous\";\n",
       "        element.integrity = \"sha384-\" + hashes[url];\n",
       "      }\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1099\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1099\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1099\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"b45f7485-f7ab-4097-aa51-acd18d70356c\" data-root-id=\"1101\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"5b2814d2-e25d-4c61-be31-d3640dece038\":{\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"1110\"}],\"center\":[{\"id\":\"1113\"},{\"id\":\"1117\"}],\"left\":[{\"id\":\"1114\"}],\"plot_height\":400,\"renderers\":[{\"id\":\"1135\"}],\"title\":{\"id\":\"1149\"},\"toolbar\":{\"id\":\"1125\"},\"x_range\":{\"id\":\"1102\"},\"x_scale\":{\"id\":\"1106\"},\"y_range\":{\"id\":\"1104\"},\"y_scale\":{\"id\":\"1108\"}},\"id\":\"1101\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"formatter\":{\"id\":\"1151\"},\"ticker\":{\"id\":\"1111\"}},\"id\":\"1110\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"1155\",\"type\":\"Selection\"},{\"attributes\":{\"text\":\"\"},\"id\":\"1149\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"1102\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"1156\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.25},\"fill_color\":{\"field\":\"color\"},\"line_alpha\":{\"value\":0.25},\"line_color\":{\"field\":\"color\"},\"size\":{\"units\":\"screen\",\"value\":10},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1133\",\"type\":\"Scatter\"},{\"attributes\":{},\"id\":\"1108\",\"type\":\"LinearScale\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":0.5,\"fill_color\":\"lightgrey\",\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":1.0,\"line_color\":\"black\",\"line_dash\":[4,4],\"line_width\":2,\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"1124\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"data\":{\"color\":[\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\"],\"token\":[\"?\",\"the\",\"what\",\"is\",\"a\",\"i\",\"to\",\"in\",\"how\",\"of\",\"do\",\"are\",\"and\",\"for\",\",\",\"can\",\"you\",\"why\",\"it\",\"my\",\"does\",\"best\",\"on\",\".\",\"or\",\"have\",\"if\",\"be\",\"with\",\"which\",\"that\",\"an\",\"some\",\"should\",\"'s\",\"get\",\"from\",\")\",\"your\",\"(\",\"like\",\"when\",\"at\",\"india\",\"good\",\"who\",\"there\",\"will\",\"as\",\"would\",\"people\",\"not\",\"n't\",\"about\",\"``\",\"''\",\"between\",\"one\",\"did\",\"any\",\"we\",\"me\",\"where\",\"most\",\"was\",\"by\",\"make\",\"so\",\"they\",\"this\",\"am\",\"after\",\"way\",\":\",\"has\",\"use\",\"much\",\"difference\",\"time\",\"life\",\"their\",\"know\",\"work\",\"many\",\"but\",\"than\",\"more\",\"all\",\"want\",\"quora\",\"someone\",\"learn\",\"find\",\"other\",\"think\",\"new\",\"better\",\"job\",\"indian\",\"out\",\"money\",\"mean\",\"become\",\"ever\",\"world\",\"without\",\"he\",\"start\",\"take\",\"us\",\"up\",\"first\",\"feel\",\"year\",\"into\",\"go\",\"online\",\"used\",\"engineering\",\"could\",\"love\",\"'m\",\"person\",\"were\",\"possible\",\"day\",\"buy\",\"things\",\"being\",\"need\",\"business\",\"using\",\"them\",\"really\",\"trump\",\"girl\",\"'\",\"her\",\"his\",\"years\",\"different\",\"long\",\"phone\",\"google\",\"company\",\"been\",\"old\",\"only\",\"no\",\"now\",\"just\",\"2\",\"app\",\"college\",\"facebook\",\"number\",\"free\",\"books\",\"2016\",\"movie\",\"still\",\"its\",\"account\",\"ca\",\"women\",\"book\",\"english\",\"she\",\"while\",\"had\",\"change\",\"ways\",\"computer\",\"thing\",\"examples\",\"data\",\"country\",\"over\",\"see\",\"android\",\"help\",\"science\",\"live\",\"school\",\"software\",\"before\",\"&\",\"language\",\"same\",\"going\",\"bad\",\"sex\",\"student\",\"stop\",\"university\",\"happen\",\"back\",\"made\",\"3\",\"1\",\"study\",\"our\",\"two\",\"system\",\"through\",\"name\",\"say\",\"real\",\"during\",\"prepare\",\"water\",\"iphone\",\"website\",\"top\",\"car\",\"important\",\"questions\",\"men\",\"give\",\"getting\",\"anyone\",\"companies\",\"high\",\"black\",\"card\",\"read\",\"programming\",\"war\",\"learning\",\"10\",\"5\",\"\\u2019\",\"exam\",\"[\",\"then\",\"]\",\"even\",\"movies\",\"china\",\"mobile\",\"cost\",\"donald\",\"right\",\"doing\",\"friend\",\"him\",\"working\",\"under\",\"come\",\"president\",\"own\",\"question\",\"career\",\"experience\",\"bank\",\"true\",\"friends\",\"guy\",\"word\",\"hair\",\"home\",\"video\",\"having\",\"look\",\"usa\",\"tell\",\"man\",\"social\",\"web\",\"interview\",\"very\",\"engineer\",\"write\",\"game\",\"government\",\"weight\",\"earth\",\"girls\",\"service\",\"food\",\"students\",\"play\",\"countries\",\"human\",\"place\",\"future\",\"improve\",\"off\",\"big\",\"days\",\"happens\",\"'ve\",\"done\",\"class\",\"eat\",\"tv\",\"process\",\"got\",\"state\",\"average\",\"s\",\"meaning\",\"relationship\",\"music\",\"too\",\"math\",\"create\",\"instagram\",\"every\",\"history\",\"last\",\"pay\",\"windows\",\"4\",\"white\",\"watch\",\"%\",\"salary\",\"body\",\"power\",\"safe\",\"clinton\",\"ask\",\"age\",\"laptop\",\"makes\",\"$\",\"hard\",\"each\",\"lose\",\"american\",\"delhi\",\"youtube\",\"energy\",\"worth\",\"earn\",\"states\",\"against\",\"win\",\"girlfriend\",\"great\",\"keep\",\"test\",\"god\",\"hillary\",\"compare\",\"market\",\"differences\",\"considered\",\"making\",\"something\",\"answer\",\"apply\",\"myself\",\"mba\",\"tips\",\"around\",\"never\",\"next\",\"always\",\"mechanical\",\"another\",\"united\",\"java\",\"c\",\"increase\",\"such\",\"course\",\"jobs\",\"download\",\"song\",\"parents\",\"kind\",\"internet\",\"common\",\"review\",\"woman\",\"code\",\"design\",\"per\",\"employees\",\"end\",\"development\",\"chinese\",\"series\",\"score\",\"interesting\",\"degree\",\"month\",\"travel\",\"show\",\"management\",\"months\",\"able\",\"believe\",\"open\",\"program\",\"living\",\"looking\",\"because\",\"6\",\"type\",\"bangalore\",\"light\",\"favorite\",\"america\",\"today\",\"actually\",\"major\",\"family\",\"marketing\",\"pakistan\",\"universities\",\"technology\",\"law\",\"call\",\"whatsapp\",\"affect\",\"idea\",\"{\",\"build\",\"}\",\"choose\",\"cat\",\"current\",\"support\",\"problem\",\"popular\",\"speed\",\"air\",\"rid\",\"deal\",\"visa\",\"join\",\"run\",\"games\",\"space\",\"down\",\"both\",\"well\",\"city\",\"2017\",\"house\",\"exist\",\"services\",\"available\",\"civil\",\"u.s.\",\"site\",\"culture\",\"also\",\"normal\",\"given\",\"wrong\",\"apple\",\"7\",\"small\",\"date\",\"order\",\"wear\",\"apps\",\"places\",\"songs\",\"project\",\"differ\",\"product\",\"international\",\"x\",\"main\",\"behind\",\"research\",\"happened\",\"boyfriend\",\"media\",\"public\",\"canada\",\"value\",\"jee\",\"cause\",\"reason\",\"uk\",\"education\",\"email\",\"ms\",\"sleep\",\"-\",\"amazon\",\"instead\",\"these\",\"benefits\",\"startup\",\"less\",\"iit\",\"list\",\"part\",\"based\",\"die\",\"medical\",\"websites\",\"etc\",\"physics\",\"skills\",\"child\",\"form\",\"health\",\"causes\",\"times\",\"called\",\"visit\",\"hate\",\"level\",\"worst\",\"legal\",\"police\",\"control\",\"terms\",\"stay\",\"products\",\"mumbai\",\"post\",\"facts\",\"mind\",\"face\",\"application\",\"rate\",\"sites\",\"field\",\"file\",\"required\",\"seen\",\"sell\",\"humans\",\"private\",\"machine\",\"story\",\"dog\",\"notes\",\"stock\",\"modi\",\"biggest\",\"asked\",\"invest\",\"children\",\"point\",\"writing\",\"move\",\"night\",\"problems\",\"gate\",\"'re\",\"put\",\"single\",\"successful\",\"anything\",\"indians\",\"others\",\"remove\",\"compared\",\"theory\",\"death\",\"ideas\",\"talk\",\"marks\",\"answers\",\"plan\",\"similar\",\"sentence\",\"group\",\"low\",\"side\",\"institute\",\"hours\",\"again\",\"effects\",\"/math\",\"coaching\",\"australia\",\"advantages\",\"solve\",\"correct\",\"function\",\"tax\",\"lot\",\"center\",\"famous\",\"send\",\"germany\",\"guys\",\"add\",\"set\",\"offer\",\"south\",\"force\",\"credit\",\"foreign\",\"information\",\"universe\",\"fall\",\"daily\",\"period\",\"says\",\"developer\",\"training\",\"full\",\"videos\",\"area\",\"industry\",\"oil\",\"model\",\"studying\",\"general\",\"majors\",\"colleges\",\"star\",\"graduate\",\"size\",\"yourself\",\"python\",\"drive\",\"2015\",\"blood\",\"near\",\"enough\",\"search\",\"price\",\"kill\",\"quality\",\"words\",\"8\",\"marriage\",\"cons\",\"found\",\"often\",\"\\u201d\",\"leave\",\"pros\",\"fast\",\"grow\",\"started\",\"courses\",\"week\",\"fix\",\"related\",\"purpose\",\"network\",\"types\",\"russia\",\"advice\",\"those\",\"password\",\"follow\",\"married\",\"taking\",\"mass\",\"male\",\"store\",\"turn\",\"party\",\"left\",\"rs\",\"brain\",\"\\u201c\",\"100\",\"numbers\",\"500\",\"female\",\"share\",\"digital\",\"team\",\"1000\",\"effective\",\"economy\",\"text\",\"term\",\"delete\",\"election\",\"cell\",\"pc\",\"master\",\"office\",\"matter\",\"few\",\"since\",\"role\",\"preparation\",\"fat\",\"second\",\"page\",\"dark\",\"short\",\"said\",\"happy\",\"red\",\"line\",\"away\",\"security\",\"options\",\"effect\",\"easy\",\"three\",\"view\",\"admission\",\"must\",\"chemical\",\"source\",\"lost\",\"try\",\"dream\",\"understand\",\"care\",\"exams\",\"convert\",\"americans\",\"past\",\"dogs\",\"electrical\",\"profile\",\"alcohol\",\"exactly\",\"north\",\"ios\",\"develop\",\"paper\",\"japanese\",\"investment\",\"tech\",\"hotel\",\"difficult\",\"check\",\"beautiful\",\"personal\",\"calculate\",\"uber\",\"japan\",\"letter\",\"eating\",\"religion\",\"explain\",\"20\",\"languages\",\"c++\",\"hyderabad\",\"b\",\"topics\",\"british\",\"basic\",\"twitter\",\"news\",\"self\",\"california\",\"green\",\"political\",\"transfer\",\"fight\",\"wife\",\"12\",\"drug\",\"rank\",\"let\",\"scope\",\"porn\",\"film\",\"army\",\"pune\",\"t\",\"national\",\"once\",\"note\",\"contact\",\"known\",\"avoid\",\"boy\",\"gain\",\"please\",\"address\",\"wants\",\"reasons\",\"paid\",\"engine\",\"obama\",\"europe\",\"ex\",\"currently\",\"bollywood\",\"samsung\",\"smart\",\"season\",\"everyone\",\"phd\",\"income\",\"officer\",\"reduce\",\"chances\",\"option\",\"overcome\",\"everything\",\"across\",\"reading\",\"military\",\"else\",\"meet\",\"users\",\"15\",\"solar\",\"message\",\"knowledge\",\"charge\",\"due\",\"sound\",\"balance\",\"microsoft\",\"internship\",\"!\",\"faster\",\"pregnant\",\"photos\",\"yes\",\"sim\",\"moon\",\"taken\",\"financial\",\"hindi\",\"pass\",\"disadvantages\",\"animals\",\"messages\",\"gay\",\"/\",\"healthy\",\"hack\",\"amount\",\"systems\",\"pain\",\"interested\",\"color\",\"ias\",\"singapore\",\"skin\",\"natural\",\"screen\",\"chance\",\"names\",\"board\",\"case\",\"height\",\"special\",\"passport\",\"structure\",\"break\",\"pro\",\"professional\",\"created\",\"drink\",\"already\",\"percentage\",\"useful\",\"french\",\"crush\",\"sun\",\"dating\",\"likes\",\"presidential\",\"battery\",\"interest\",\"access\",\"coming\",\"provide\",\"muslim\",\"modern\",\"starting\",\"linux\",\"camera\",\"recover\",\"german\",\"expect\",\"vote\",\"blue\",\"muslims\",\"train\",\"studies\",\"install\",\"least\",\"solution\",\"thinking\",\"following\",\"mother\",\"negative\",\"jio\",\"spend\",\"mac\",\"prime\",\"user\",\"written\",\"buying\",\"feeling\",\"running\",\"football\",\"cold\",\"gift\",\"allowed\",\"changed\",\"pressure\",\"brand\",\"character\",\"rich\",\"resources\",\"snapchat\",\"within\",\"speak\",\"islam\",\"impact\",\"yet\",\"non\",\"marry\",\"eyes\",\"society\",\"bill\",\"though\",\"likely\",\"determine\",\"shows\",\"fake\",\"save\",\"capital\",\"final\",\"gmail\",\"picture\",\"middle\",\"laws\",\"nuclear\",\"applications\",\"chemistry\",\"young\",\"kids\",\"prefer\",\"device\",\"iq\",\"father\",\"galaxy\",\"views\",\"manager\",\"doctor\",\"personality\",\"hand\",\"greatest\",\"currency\",\"inside\",\"illegal\",\"strategy\",\"stories\",\"recruit\",\"abroad\",\"pursue\",\"cs\",\"gold\",\"works\",\"consider\",\"projects\",\"crack\",\"blog\",\"memory\",\"example\",\"neet\",\"schools\",\"growth\",\"husband\",\"necessary\",\"insurance\",\"alone\",\"economics\",\"depression\",\"dead\",\"b.tech\",\"higher\",\"cse\",\"30\",\"loss\",\"among\",\"easiest\",\"deleted\",\"art\",\"files\",\"macbook\",\"together\",\"suggest\",\"vs\",\"wifi\",\"gas\",\"late\",\"intelligence\",\"gre\",\"method\",\"head\",\"version\",\"minimum\",\"chennai\",\"tools\",\"easily\",\"cars\",\"policy\"],\"x\":{\"__ndarray__\":\"8dKhPgbtxz+XXM0/a28kP0EZrj+6zso/7vqKP6y+cT636zE/3y5ePlhHlT82MiM/dfGqPrfqhT7IYLQ+MiqfP6fZxT9msJY/5vqpP/JV0T9mFZ0/svjfP7hGrz6uUaY/AEOyPnNLZz+kHKg/pODwvz8Aqj72O9E/F8GrPwZbsD/cAbw/v2uePyP3ED/7Sbe/t7uRPvxp2T5yatg/JOjPPlxd8j8M7KU/iByFPkfm3r1bYtU/P1uwPwKXPT8BbaU/cxHGPi27pT9WMLs/5lCVP8+ulz8jr94/EsuqvYJ8Tz9P6B4+SMezPwlxnT9I6aE/+B3JP7Fxvz9jZi8/1aSAP93YKT83qso+DMfev1M6tz9Pf8k/kSWwP3Q5kT9c2JM/eod7vs7nej5RrWk/eg7Qvz/Pgz8ci1I/VY82P1nP4z8/4eA/VwHIv7sr7L+wnrE/HJOmP8qToj68Fb8/IhimP4xQjz96rsC/sf23P9fUxb/ccre/3VmpP3ej174T6Is/TmLNP1cqu70A7ZM+JtyBP+2RRL82Ese9ckLuv/Lvlj+ddZ29iUu2Pk2nvT9bveW/0NC4v6rXBj1h7Hs//8OQP95YXz9DdzE/sPWGPtr/77+60EK/lH0jPw8RFD6pYqI/iIV0P6VAkT+oeZ4/nxUmP19G5T/e8Ts/8sLRv25WwD/v04k/7UWPP72nKr88m5C/SAuyP9o1hj+OqqE+Af+kP53MkT0F+cQ/2+jWP/9LPD8+saI/Ky6DP9aZm7+VTbS/QvYtv2Nmkz8aVZk/S+iKP2BWnz+QFYQ/90yKPy05pj+L7my/+DGZPfCZu78hiIq/U/9Av2TR7r49I/8+wlkuv+tTjj/fKOQ/PhC2vyn0lD8MlbE/v8Duvq2jBz6rfr0/rWqXP8ZBbz+zD+2/RZV7vkBcMb7NBsM/NFCGvvwTmr5QsoW9dzhjP80M179dVpG/U2ymv60NST0osPi/CCiHPZsdxL7u65M/1t++Psb/mL5OJaE/f//zPkJ/Tz/1Qo4/SmQ3PAumBsCMVQY+x9z1vxtjez8EQ1I/zp2oP7j5oT9dVgM+wHziP68xqj8DGKS+ACiyPl3J5b3P0ci/7/8YP44wKD4jQLG/hOE8v/QfnL+di2S/v88RPXNqZb8a4gc/4sq/v63esD/PLrK/oJC7Pn4WtT/rqzm/1GE1P3Qi+z560oq/Kum1v1m3hb9UqgU9j96Nv7+Srj+H9ak/G5IJP6OtsD5gZQ6+8RiLP6YCAL7hSJ4/D1whv6x7Aby34ZK/9fslv5KFiD6Cl2s/VQ3XPpUSpj+Z3cA/Iru/PirTHz5VJAHA5YFLPqdqnr8mXcS/O93sP2ZH7D8BJFm/CEJEPyivpj8ZJqM/pSWCvumKLz9GTie+g2hSv+bRuz5Go+m/9FxYvTt0yr+Ntpo/0K0jv2IJZL+qKP++QS26P5PRPT4tALu/Ef5Av0jz2D3pwyQ/KYtAv17MsT99O1S/zB5Bv/dVmT0Boca/uBZyvTHPID8M5YC+6FN5vvN4AcA2MW8/kUoLP9GPQj9D4QG/79FuP2BVWz9u7Vg+Vu8JwBw+Tr/QbNW+w0J8P3Ahwz2fM+y+hH/ZPaqiDL5O6Ig/Yec/v2pwuj9R5ew9p63bv5utv78pqZM/PZgPPgwdiz8cD6O/nE2Rv4cbqT80/QA/5a/GvwZ+pT9VYjm/L7gcPy9TJr/lBeA/1CmRPsEywb/q0Dg/ZLWVvyfSIz9Dahi/7OjjPxQMtT+uK/q/nOeOPs3CM75k+Vu/1eUsv+RY8D+/t9y//b9qva0w/D7Wiua/+XShP+gBCj875wTAUhm+Ptq78T6TM5E+iXOxvyHHH7/lhFI/wpkiPzUp1z6aeag/ZXTFv7uiub+yLbo/XxEgva4lEb+DiVk/DaySP91hij8nEI0/atDBvTJbqz/BKsg9sAiHvw3bgL+icPy/7bYwP/CkJ7083d69OWbIv3QGHr/lgJ8/d4VcvSLykb9AX/0+AX8LvlNwnD/KUnC/ybzdvrILmz80Onq/T5b5v5Ej1r5Lv04+oOlJv1u+8D7inws/8YrgvG6XNj9G2fe/6LFmv88I875aB0A/XsziP+M2Bb85986/NCvPvS7h5zw/1rQ+knidP97RrT9RE2C9bDpQvvWIM78XA28/K3x/NzW6eT9R24I/FjzyPZRtkj+sAfG+6bQAPZRGHD5UG4e+JPnqPRVKvL//er6/FX/5vy+RGr/gRby9RCPfv3Fju71s4tG/KF+7Pm42s75IR66/EZEKvs337j5kdCe/BY4jv6IJYT8oRAnAyYp+v9cNz7+KXeG/7XxKvwlyNb/kO20/MLeGP9sQRz+a2Ui+YC7yPg/5Rj/tsQa/UBpTv5vCOb/BvqW9zFR4PWuRX7+AEpI+3XOFP4ssTz9Z8kY/K7lrP1jJhL8KdLE/TOkIP0cQWj/36ak+pVbfv4EXXr85MYu+0zEbvzbsrr6366S/nlMav1Eo2LsYYfU+26OhPMup8jvA+5S+ZIxNP4teoD/5ASq/LEITv0PYob0E0fu+QSjEPukFjL5QObk+5gsHvQ/YJL6IKLK/KprXuyYzRj+ffJ0+MSuBvxsmWT5Z560/7X2kvscqKb+KTr8/PMTLPRVOjL+SCR475TKrPj5x7L8FDgG8gK9Uv/s8zz5mBs49DMhZvj8ujT9XauK9G4L0PtM2k74G/kw/zJYiP+pQ/r8RQdK/cp4gPce8bj9539c/bJvkPWI4879B7p++71X3v9lzQb/YpUO+L06Rv2vY8b4uP4E/h7w8P1voZL+iKRG/j5BTvy/rVr7xC3e/yDXVP8sAYz8YSdO/9SogP3YeGr+Vd2m+bDENvz4yOD+vsuq+gWUjv2B29z3weGs/AufGPdh6278MlJ0/1lsPv7Sngj4JmfC/oPUyP0a9970EWb0++Qr1vz0o27+p4gw/96QHP2z6pz/eQQA/zUKvPws53780abW/7xZKviDlTz+G+Q+/s5zJv7qr5j7oQsC/N/P1vgnFFT+e7YG+5/kJu5WfNT9pTC8/QFedvVctRj+9bIA/t5auvtAf0r1QEI8+uwtXvSLOmr4rMvS/djXbO0RKBb5L2lG/2Y5kPwt+iL4x1+4+3wzBv+vyXL1b/7E/cnvPv83tzr/bepW/0mJLPqTTDb920WS/hGDNuyGygb46Y0S/bLLxvxbFKj+4kEs/W8xOP6Iex74/FIi6mMGUPxTFUr9oY7u+ZUUMv6weMr9TbwG/sga4PQAgsTx33pc+vhHjPUk8N7+RPAE93WgGv7l4uD9VYIa/bablvwO4AD89KiA/NdAgPrTKyz81Ece/Xzkiv6sS6b8Q1U2/rzGYvpXDrD9r0ow/ibGcvpVrUj9adUc/6VtPPyF55r++0Ju+1w4yPzhE8r/UEms/gxogvbKTOz9vEeC/Xz8OPyFA773Hp1S/ElONvmgjnDxzNUU+DVOyP6lBur/52dK/PnB9P9mw2j72oR+/BD1lP3tnhr9gZN+/oulsPnPoVj+rrQa/gWwSP9GEqr3TLaU/YkCKvwe39b4Kd2Q/eSGwv6lS6L655Q2/7Mj3vtAhGT9YJcW+iYzKv8q2x76HCde/fBQWPm77SL/1Ro6/meCcvaP4i79wswi/nsypP/wWlz9MhhO+GiaZPpyOJj9GAIw/3h6tvz1eET+Vqx0/IqFOP0SmaT/4rwg/N3cYv25lfj+XDEi/0CDlvgnct77Lwtw/dUupP4Qwp79q2x4+QEqlP41nhr1ZWK++zRR7P/+Z7L9W2Xw/1ofKvxX/bj9W2q0+gsrJvy5Z/z5nhYc/ZKMiP+vg0b2qya6/uLYiv0imT73Qd0w+aJiNv7g14r9Didc+Q+ZIPlGDMr/5Bha/WEGZvuOl4D/s2ca/R63jPgq8A7+oiADA7Kt9v/0+srxJzSK+6hdAPw4p2T6D1PO/1xmhP/qymb6BFIS/wbJ3vsx/ej1ZDoq+bip8PtEtIb3fPba/cJMuvxaAGL4gRjW+O6z0PgDuAD5FTce/KP/nv8SImD9wfp0/LJwiv8Zt3z6S43Y/wVs1vkGSLr8XJyq/Gw4VPi0gYb7Ws9k9vQxPPj5GjT8j3qO/iPW3v4karj5WPwXABPqVP2co+L8h6gE/JSKwvyDWhD+l8bc+jGstv8YnbL+J8Xg+z0GbvI+ZmD9oHBk/JQglvwmNn79gXeI+vHU9vw0nrz/7+Fc8bCdLv8Tacj5aY/m/o2quPtGJ+L5EDAbAq1mrPzKpxz51YoQ+t3H1PeMmgT8Yy9y//yanvx4MoT8EMzW/Yi7Gvz/sZb7n4aa/+/SEvjifDj+Lrhm/+nCAv+41OL19v/Y+QirGP593fj+KLr6/krOkPzYij78yazO/ZvxWP2bVC7+4X+89lHa0v8E0mb4ZvCA//LPCv8tCaT+exJM+s2A7P8+k0L9lEwm/ZASOvmQPSz/MyA0/p4cQP8M7mT6iolC9T44vPyysMjw/wIK/y6W4PiZMZr23q+k+76DPvS4DLT/nLqk+VCqBv74mkr0nyu6/FhScv2256r6+EUQ/JP0JwGzEjD+4e+O+o64QPyKeLD7RUpo/zp4zv9Yk1D7UmXw/RR4aPsZrfL/A1b6+IzvMv4+xAsBrP7K/hxG2PpE5hD60le4+wrKHv32Ifb8209m//zYrPrXKtb+PCgk+n9YHPzTA9j4lkda/FutrPWgM1L+4r54/q5g1vqrYaj8bTFY9wO2SP8sMp76dgY+/gA7Xvxzkh79Txu++P6mrvwvw3z3Mmz+/V2tmP6RwAT/KuQ6/cTM2P+i1Tj8W2ts/f2R0P4SaGr9vgEO/qiodv43iXj+f3pO+SFy7vyViVz7FOcm/Da7iPn3/ur7PKoY/5JuQPtve3r8XOTE/puzHPrvS8L6X1Z0/LqnUP7ZBAMAxZWG/8j7EPlB22b/1aRC/3EkCPwEeub8C1rS/1fWNPlhbLT5MRiO+FxyfvlQtvT2BTV8/r3KdPxK21r+mKoq/Inamvh0ikj/jXaC/qgelvxt9/r4FXYI/5v17P7lWQT8m1m4/pjvXvk7MMz/PVNg/LpH2vhwyB78eAaS/suedvZSzzr+v5Bc9KgIpv42laz4FAte/wWWgvkvssb8Ql2O/zBJ2v6wSRL6YuNU+BSfOPT8H9r5smpg/PfTbP+a1/r6of14/6LU5PUUnWD/rvkU/JbuOPcBORj+2Xq89drKiP5YEED8WuT0+MnPgPy4kwr+eY9k97/GAv958nL+k4V4/H2Ssv8gehz5r7Yu/dr8qv567WD/EV3e+rtK4Pu9mSb6ACT4//4cpvwHd7b4x412+a7mrvqgLSD94yTu/QNfYvg==\",\"dtype\":\"float32\",\"order\":\"little\",\"shape\":[1000]},\"y\":{\"__ndarray__\":\"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\",\"dtype\":\"float32\",\"order\":\"little\",\"shape\":[1000]}},\"selected\":{\"id\":\"1155\"},\"selection_policy\":{\"id\":\"1156\"}},\"id\":\"1100\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"formatter\":{\"id\":\"1153\"},\"ticker\":{\"id\":\"1115\"}},\"id\":\"1114\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"1104\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"1106\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"1123\",\"type\":\"HelpTool\"},{\"attributes\":{},\"id\":\"1111\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1151\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"axis\":{\"id\":\"1110\"},\"ticker\":null},\"id\":\"1113\",\"type\":\"Grid\"},{\"attributes\":{\"axis\":{\"id\":\"1114\"},\"dimension\":1,\"ticker\":null},\"id\":\"1117\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1153\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"1115\",\"type\":\"BasicTicker\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":{\"id\":\"1119\"},\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1118\"},{\"id\":\"1119\"},{\"id\":\"1120\"},{\"id\":\"1121\"},{\"id\":\"1122\"},{\"id\":\"1123\"},{\"id\":\"1137\"}]},\"id\":\"1125\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"1119\",\"type\":\"WheelZoomTool\"},{\"attributes\":{},\"id\":\"1118\",\"type\":\"PanTool\"},{\"attributes\":{\"overlay\":{\"id\":\"1124\"}},\"id\":\"1120\",\"type\":\"BoxZoomTool\"},{\"attributes\":{},\"id\":\"1121\",\"type\":\"SaveTool\"},{\"attributes\":{},\"id\":\"1122\",\"type\":\"ResetTool\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"field\":\"color\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"field\":\"color\"},\"size\":{\"units\":\"screen\",\"value\":10},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1134\",\"type\":\"Scatter\"},{\"attributes\":{\"callback\":null,\"tooltips\":[[\"token\",\"@token\"]]},\"id\":\"1137\",\"type\":\"HoverTool\"},{\"attributes\":{\"data_source\":{\"id\":\"1100\"},\"glyph\":{\"id\":\"1133\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1134\"},\"selection_glyph\":null,\"view\":{\"id\":\"1136\"}},\"id\":\"1135\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"source\":{\"id\":\"1100\"}},\"id\":\"1136\",\"type\":\"CDSView\"}],\"root_ids\":[\"1101\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\n",
       "  var render_items = [{\"docid\":\"5b2814d2-e25d-4c61-be31-d3640dece038\",\"root_ids\":[\"1101\"],\"roots\":{\"1101\":\"b45f7485-f7ab-4097-aa51-acd18d70356c\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1101"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style=\"display: table;\"><div style=\"display: table-row;\"><div style=\"display: table-cell;\"><b title=\"bokeh.plotting.figure.Figure\">Figure</b>(</div><div style=\"display: table-cell;\">id&nbsp;=&nbsp;'1101', <span id=\"1203\" style=\"cursor: pointer;\">&hellip;)</span></div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">above&nbsp;=&nbsp;[],</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">align&nbsp;=&nbsp;'start',</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">aspect_ratio&nbsp;=&nbsp;None,</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">aspect_scale&nbsp;=&nbsp;1,</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">background&nbsp;=&nbsp;None,</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">background_fill_alpha&nbsp;=&nbsp;1.0,</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">background_fill_color&nbsp;=&nbsp;'#ffffff',</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">below&nbsp;=&nbsp;[LinearAxis(id='1110', ...)],</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">border_fill_alpha&nbsp;=&nbsp;1.0,</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">border_fill_color&nbsp;=&nbsp;'#ffffff',</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">center&nbsp;=&nbsp;[Grid(id='1113', ...), Grid(id='1117', ...)],</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">css_classes&nbsp;=&nbsp;[],</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">disabled&nbsp;=&nbsp;False,</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">extra_x_ranges&nbsp;=&nbsp;{},</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">extra_y_ranges&nbsp;=&nbsp;{},</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">frame_height&nbsp;=&nbsp;None,</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">frame_width&nbsp;=&nbsp;None,</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">height&nbsp;=&nbsp;None,</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">height_policy&nbsp;=&nbsp;'auto',</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">hidpi&nbsp;=&nbsp;True,</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">js_event_callbacks&nbsp;=&nbsp;{},</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">js_property_callbacks&nbsp;=&nbsp;{},</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">left&nbsp;=&nbsp;[LinearAxis(id='1114', ...)],</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">lod_factor&nbsp;=&nbsp;10,</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">lod_interval&nbsp;=&nbsp;300,</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">lod_threshold&nbsp;=&nbsp;2000,</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">lod_timeout&nbsp;=&nbsp;500,</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">margin&nbsp;=&nbsp;(0, 0, 0, 0),</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">match_aspect&nbsp;=&nbsp;False,</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">max_height&nbsp;=&nbsp;None,</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">max_width&nbsp;=&nbsp;None,</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">min_border&nbsp;=&nbsp;5,</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">min_border_bottom&nbsp;=&nbsp;None,</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">min_border_left&nbsp;=&nbsp;None,</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">min_border_right&nbsp;=&nbsp;None,</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">min_border_top&nbsp;=&nbsp;None,</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">min_height&nbsp;=&nbsp;None,</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">min_width&nbsp;=&nbsp;None,</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">name&nbsp;=&nbsp;None,</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">outline_line_alpha&nbsp;=&nbsp;1.0,</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">outline_line_cap&nbsp;=&nbsp;'butt',</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">outline_line_color&nbsp;=&nbsp;'#e5e5e5',</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">outline_line_dash&nbsp;=&nbsp;[],</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">outline_line_dash_offset&nbsp;=&nbsp;0,</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">outline_line_join&nbsp;=&nbsp;'bevel',</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">outline_line_width&nbsp;=&nbsp;1,</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">output_backend&nbsp;=&nbsp;'canvas',</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">plot_height&nbsp;=&nbsp;400,</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">plot_width&nbsp;=&nbsp;600,</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">renderers&nbsp;=&nbsp;[GlyphRenderer(id='1135', ...)],</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">reset_policy&nbsp;=&nbsp;'standard',</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">right&nbsp;=&nbsp;[],</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">sizing_mode&nbsp;=&nbsp;None,</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">subscribed_events&nbsp;=&nbsp;[],</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">tags&nbsp;=&nbsp;[],</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">title&nbsp;=&nbsp;Title(id='1149', ...),</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">title_location&nbsp;=&nbsp;'above',</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">toolbar&nbsp;=&nbsp;Toolbar(id='1125', ...),</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">toolbar_location&nbsp;=&nbsp;'right',</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">toolbar_sticky&nbsp;=&nbsp;True,</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">visible&nbsp;=&nbsp;True,</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">width&nbsp;=&nbsp;None,</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">width_policy&nbsp;=&nbsp;'auto',</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">x_range&nbsp;=&nbsp;DataRange1d(id='1102', ...),</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">x_scale&nbsp;=&nbsp;LinearScale(id='1106', ...),</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">y_range&nbsp;=&nbsp;DataRange1d(id='1104', ...),</div></div><div class=\"1202\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">y_scale&nbsp;=&nbsp;LinearScale(id='1108', ...))</div></div></div>\n",
       "<script>\n",
       "(function() {\n",
       "  var expanded = false;\n",
       "  var ellipsis = document.getElementById(\"1203\");\n",
       "  ellipsis.addEventListener(\"click\", function() {\n",
       "    var rows = document.getElementsByClassName(\"1202\");\n",
       "    for (var i = 0; i < rows.length; i++) {\n",
       "      var el = rows[i];\n",
       "      el.style.display = expanded ? \"none\" : \"table-row\";\n",
       "    }\n",
       "    ellipsis.innerHTML = expanded ? \"&hellip;)\" : \"&lsaquo;&lsaquo;&lsaquo;\";\n",
       "    expanded = !expanded;\n",
       "  });\n",
       "})();\n",
       "</script>\n"
      ],
      "text/plain": [
       "Figure(id='1101', ...)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tsne = get_tsne_projection(word_vectors)\n",
    "draw_vectors(word_tsne[:, 0], word_tsne[:, 1], color='green', token=words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sz6uHYMbSjuE"
   },
   "source": [
    "## Using Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4GEypE4SokX"
   },
   "source": [
    "We can also use a pretrained embeddings model. There are a number of such models in gensim, you can call `api.info()` to get the list.\n",
    "\n",
    "Let's load a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "GUDRtumXXF3S"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "model = api.load('glove-twitter-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6CST4OCyCBF"
   },
   "source": [
    "## Building Phrase Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eypMhlOSXFWN"
   },
   "source": [
    "The simplest way to obtain a phrase embedding is to average embeddings of the words in the phrase.\n",
    "\n",
    "*You are probably thinking, 'What a dumb idea, why on earth the average of embedding should contain any useful information'. Well, check [this paper](https://arxiv.org/pdf/1805.09843.pdf).*\n",
    "\n",
    "Let's do it: tokenize and lowercase the texts, calc the mean embedding for the words with known embeddings.\n",
    "\n",
    "**Task** Implement the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "F76HGRGDEPVq"
   },
   "outputs": [],
   "source": [
    "def get_phrase_embedding(model, phrase):\n",
    "    \"\"\" Calcs phrase embedding as a mean of known word embeddings in the phrase. \n",
    "    If all the words are unknown, returns zero vector.\n",
    "    :param model: KeyedVectors instance\n",
    "    :param phrase: str or list of str (tokenized text)\n",
    "    \"\"\"    \n",
    "    embedding = np.zeros([model.vector_size], dtype='float32')\n",
    "    \n",
    "    if isinstance(phrase, str):\n",
    "        words = word_tokenize(phrase.lower())\n",
    "    else:\n",
    "        words = phrase\n",
    "    \n",
    "    # <implement me>\n",
    "    words = [*filter(lambda word: word in model.key_to_index, words)]\n",
    "    \n",
    "    if len(words) == 0:\n",
    "        embedding = np.zeros([model.vector_size], dtype='float32')\n",
    "    else:\n",
    "        embedding = np.mean([*map(model.get_vector, words)], axis = 0, dtype='float32')\n",
    "    \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "i12XsqnIXfko"
   },
   "outputs": [],
   "source": [
    "vector = get_phrase_embedding(model, \"I'm very sure. This never happened to me before...\")\n",
    "\n",
    "assert np.allclose(vector[::10],\n",
    "                   np.array([ 0.30757686, -0.05861897,  0.143751  , -0.11104885, -0.96929336,\n",
    "                             -0.21928601,  0.21652265,  0.14978765,  1.4842536 ,  0.017826  ],\n",
    "                              dtype=np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yl2nBZG0WAUx"
   },
   "source": [
    "Well, we are ready to embed all the sentences in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "40LezFEJFwv3"
   },
   "outputs": [],
   "source": [
    "text_vectors = np.array([get_phrase_embedding(model, phrase) for phrase in tokenized_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(537361, 100)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBj8XMZvWFTv"
   },
   "source": [
    "What can we do with it? Now we are able perform search of the nearest neighbours to the given phrase in our base!\n",
    "\n",
    "How are we going to define the distance?\n",
    "\n",
    "We'll use cosine similarity of two vectors:\n",
    "$$\\text{cosine_similarity}(x, y) = \\frac{x^{T} y}{||x||\\cdot ||y||}$$\n",
    "\n",
    "*It's not a [distance](https://www.encyclopediaofmath.org/index.php/Metric) strictly speaking but we still can use it to search for the vectors.*\n",
    "\n",
    "**Task** Calc the similarity between `query` embedding and `text_vectors` using `cosine_similarity` function. Find `k` vectors with highest scores and return corresponding texts from `texts` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 93960, 449642, 181904, 492921, 193900, 171348, 273668, 119429,\n",
       "        30950, 187447], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "k=10\n",
    "query = 'How do i enter the matrix?'\n",
    "query_vector = get_phrase_embedding(model, query)\n",
    "similarities = cosine_similarity(text_vectors, [query_vector])\n",
    "nearest = np.argsort(similarities, axis=None)[-k:]\n",
    "nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"How do I download the Mengto's Design-Code book?\",\n",
       "       'How do I get to the dark web?',\n",
       "       'What should I do to enter hollywood?',\n",
       "       'How do I use the Greenify app?',\n",
       "       'What can I do to save the world?', 'How do I win this?',\n",
       "       'How do I think out of the box? How do I learn to think out of the box?',\n",
       "       'How do I find the 5th dimension?', 'How do I use the pad in MMA?',\n",
       "       'How do I estimate the competition?'], dtype='<U1169')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.take(texts, nearest[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "fjw7kTQ-FP11"
   },
   "outputs": [],
   "source": [
    "def find_nearest(model, text_vectors, texts, query, k=10):\n",
    "    # <implement me too>\n",
    "    query_vector = get_phrase_embedding(model, query)\n",
    "    similarities = cosine_similarity(text_vectors, [query_vector])\n",
    "    nearest = np.argsort(similarities, axis=None)[-k:]\n",
    "    return np.take(texts, nearest[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "c2yK0twNGWaQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do I download the Mengto's Design-Code book?\n",
      "How do I get to the dark web?\n",
      "What should I do to enter hollywood?\n",
      "How do I use the Greenify app?\n",
      "What can I do to save the world?\n",
      "How do I win this?\n",
      "How do I think out of the box? How do I learn to think out of the box?\n",
      "How do I find the 5th dimension?\n",
      "How do I use the pad in MMA?\n",
      "How do I estimate the competition?\n"
     ]
    }
   ],
   "source": [
    "results = find_nearest(model, text_vectors, texts, query=\"How do i enter the matrix?\", k=10)\n",
    "\n",
    "print('\\n'.join(results))\n",
    "\n",
    "assert len(results) == 10 and isinstance(results[0], str)\n",
    "assert results[1] == 'How do I get to the dark web?'\n",
    "assert results[4] == 'What can I do to save the world?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "AutTfxbDGhku"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['What does Donald Trump think about Israel?',\n",
       "       'What books does Donald Trump like?',\n",
       "       'What does Donald Trump think of India?',\n",
       "       'What does India think of Donald Trump?',\n",
       "       'What does Donald Trump think of China?',\n",
       "       'What does Donald Trump think about Pakistan?',\n",
       "       'What companies does Donald Trump own?',\n",
       "       'What does Dushka Zapata think about Donald Trump?',\n",
       "       'Does Donald Trump have dementia/alzheimers?',\n",
       "       'How does it feel to date Ivanka Trump?'], dtype='<U1169')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest(model, text_vectors, texts, query=\"How does Trump?\", k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "47Ff7heKGiGV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"Why do you always answer a question with a question? I don't, or do I?\",\n",
       "       'Why do I ask this question?', 'How do I ask a question?',\n",
       "       'How do I ask a question on this?',\n",
       "       \"Why do I have to ask a girl out? Why can't she ask me?\",\n",
       "       'How do I downvote a question?', 'How do I ask someone on a date?',\n",
       "       'Why do I question myself about this?',\n",
       "       \"How do I ask a girl I don't know to fuck?\",\n",
       "       'How do you ask a question?'], dtype='<U1169')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest(model, text_vectors, texts, query=\"Why don't i ask a question myself?\", k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BhDEF11ZnTY"
   },
   "source": [
    "## Starting Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2D_LEV8cm0z3"
   },
   "source": [
    "### Bag-of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xlZZKWOE2ME"
   },
   "source": [
    "Finally, we are ready to return to the classification task.\n",
    "\n",
    "We have two sentences and we are going calculate their similarity and compare it with some threshold. If the value is higher than the threshold then we'll call the sentences similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGge73gDid99"
   },
   "source": [
    "Let's start with tokenization of the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "FJTjqdgiih7O"
   },
   "outputs": [],
   "source": [
    "tokenized_question1 = [word_tokenize(question.lower()) for question in quora_data.question1]\n",
    "tokenized_question2 = [word_tokenize(question.lower()) for question in quora_data.question2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "sT0GPAFnJv-8"
   },
   "outputs": [],
   "source": [
    "assert tokenized_question1[0] == ['what', 'is', 'the', 'step', 'by', 'step', 'guide', 'to', 'invest', 'in', 'share', 'market', 'in', 'india', '?']\n",
    "assert tokenized_question2[2] == ['how', 'can', 'internet', 'speed', 'be', 'increased', 'by', 'hacking', 'through', 'dns', '?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0VBcipE7Ztkc"
   },
   "source": [
    "**Task** Calc the cosine similarity between the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ItJLR_ENHGtI"
   },
   "outputs": [],
   "source": [
    "question1_vectors = <calc vectors for tokenized_question1>\n",
    "question2_vectors = <calc vectors for tokenized_question2>\n",
    "# 1:37 \n",
    "cosine_similarities = <calc similarities between the vectors in question1_vectors and question2_vectors>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5rdNs01vgoI"
   },
   "outputs": [],
   "source": [
    "assert cosine_similarities.shape == (len(quora_data),), 'Check the shapes'\n",
    "\n",
    "target_similarity = cosine_similarity([get_phrase_embedding(model, tokenized_question1[1])], \n",
    "                                      [get_phrase_embedding(model, tokenized_question2[1])])[0, 0]\n",
    "assert np.allclose(cosine_similarities[1], target_similarity), 'Check your calculations'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NgCVueJ4Z3DQ"
   },
   "source": [
    "Let's find the texts' similarity threshold.\n",
    "\n",
    "We are going to optimize accuracy of the similarity prediction. For instance, accuracy with threshold equal to 0 would be equal to the fraction ones in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nEzgi-o5OieT"
   },
   "outputs": [],
   "source": [
    "(quora_data.is_duplicate == 1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJvse7gpO4-E"
   },
   "source": [
    "**Task** Implement the `accuracy` function that calculates accuracy with the given threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41hRIb-KSA3F"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def accuracy(cosine_similarities, threshold, labels):\n",
    "    return <implement me>\n",
    "\n",
    "thresholds = np.linspace(0, 1, 100, endpoint=False)\n",
    "plt.plot(thresholds, [accuracy(cosine_similarities, th, quora_data.is_duplicate) for th in thresholds])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOxJSE38msmX"
   },
   "source": [
    "Let's optimize over this function to find the optimal threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R3LVCJwqSgZz"
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "res = minimize_scalar(\n",
    "    lambda th: -accuracy(cosine_similarities, th, quora_data.is_duplicate), bounds=(0.5, 0.99), method='bounded'\n",
    ")\n",
    "\n",
    "best_threshold = res.x\n",
    "best_accuracy = accuracy(cosine_similarities, best_threshold, quora_data.is_duplicate)\n",
    "print('Threshold = {:.5f}, Accuracy = {:.2%}'.format(best_threshold, best_accuracy))\n",
    "\n",
    "assert best_accuracy > 0.65, 'Check yourself'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R31CunmePSLY"
   },
   "source": [
    "Well, we are a bit better than random :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8yFlaffm34A"
   },
   "source": [
    "### Tf-idf Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cpeSa-4m64H"
   },
   "source": [
    "The averaging of vectors is boring. We can use weighted average - with tf-idf weights.\n",
    "\n",
    "Let's use `TfidfVectorizer` for this task.\n",
    "\n",
    "You see, `TfidfVectorizer` returns matrix `(samples_count, words_count)`. Our embeddings is a matrix `(words_count, embedding_dim)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sjwLoOHx7ChZ"
   },
   "outputs": [],
   "source": [
    "model.vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VaVrx3j7I9l"
   },
   "source": [
    "The embedding of a sequence of words $w_1, \\ldots, w_k$, as we defined, it is vector $\\sum_i \\text{idf}(w_i) \\cdot \\text{embedding}(w_i)$.\n",
    "\n",
    "That means that we can multiply matrices `(samples_count, words_count) x (words_count, embedding_dim)` to obtain the embeddings for all phrases we have.\n",
    "\n",
    "But we need to have corresponding words in both matrices. That is i-th row in the first matrix correspond to the i-th column in the second matrix.\n",
    "\n",
    "To achieve it, we are going to use `vocabulary` argument of `TfidfVectorizer`.\n",
    "\n",
    "We can extract the vocabulary this way from the gensim model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCFfdtf25ukF"
   },
   "outputs": [],
   "source": [
    "vocabulary = {word: vocab_element.index for word, vocab_element in model.vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDo4xkQl9mND"
   },
   "source": [
    "Initialize the vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I_tM8LMk5jgc"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(vocabulary=vocabulary)\n",
    "\n",
    "vectorizer.fit(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uu5gILFElhx"
   },
   "source": [
    "**Task** Apply `vectorizer` to the `quora_data` questions and obtain the phrase vectors by multiplying them on `model.vectors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UoYjVExHd_OC"
   },
   "outputs": [],
   "source": [
    "tfidf_question1 = <calc it>\n",
    "tfidf_question2 = <and it>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZGM1HvnwwwYj"
   },
   "outputs": [],
   "source": [
    "assert tfidf_question1.shape == tfidf_question2.shape == (len(quora_data), len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34Di8jXxoBxg"
   },
   "source": [
    "Check, that the text in matrices is correctly encoded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k52NtVJKf1Wl"
   },
   "outputs": [],
   "source": [
    "for col in tfidf_question1[0].tocoo().col:\n",
    "    print(model.index2word[col], end=' ')\n",
    "\n",
    "print('\\n' + ' '.join(tokenized_question1[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNBcFog40fa8"
   },
   "source": [
    "Now we are able to convert the vectors matrices to vectors. That is, multiply tfidf and word2vec matrices and nomalize the result by the number of words in each sentence.\n",
    "\n",
    "**Task** Build the question vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EWGN_z0UxBNL"
   },
   "outputs": [],
   "source": [
    "EPS = 1e-9\n",
    "\n",
    "question1_elements_count = <calc it, add EPS to ensure you don't divide by zero>\n",
    "question2_elements_count = <and it too>\n",
    "\n",
    "assert question1_elements_count.shape == question2_elements_count.shape == (len(quora_data), 1)\n",
    "assert np.all(question1_elements_count > 0) and np.all(question2_elements_count > 0.)\n",
    "\n",
    "question1_vectors = <calc mean tfidf-weighted vectors>\n",
    "question2_vectors = <and these too>\n",
    "\n",
    "assert question1_vectors.shape == question2_vectors.shape == (len(quora_data), model.vectors.shape[1])\n",
    "\n",
    "assert np.allclose(question1_vectors[0][:10], [ 0.04672134, -0.00910798,  0.06817335,  0.00792347,  0.00907249,\n",
    "                                                0.05163505,  0.02648487, -0.05109346,  0.04752091, -0.01203835])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fMqY5tOoL9r"
   },
   "source": [
    "**Task** Evaluate the quality of these embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FP46USWDy2Ah"
   },
   "outputs": [],
   "source": [
    "cosine_similarities = <calc them>\n",
    "assert cosine_similarities.shape == (len(quora_data),), 'Check the shapes'\n",
    "assert np.allclose(cosine_similarities[:5], [0.99604267, 0.9558047 , 0.973884  , 0.79243606, 0.92760015])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Chf2qy7uhbPF"
   },
   "outputs": [],
   "source": [
    "res = minimize_scalar(\n",
    "    lambda th: -accuracy(cosine_similarities, th, quora_data.is_duplicate), bounds=(0.5, 0.99), method='bounded'\n",
    ")\n",
    "\n",
    "best_threshold = res.x\n",
    "best_accuracy = accuracy(cosine_similarities, best_threshold, quora_data.is_duplicate)\n",
    "print('Threshold = {:.5f}, Accuracy = {:.2%}'.format(best_threshold, best_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvqrFUS6vVhh"
   },
   "source": [
    "## Implementing Word-level Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1CXcr-ypzGXg"
   },
   "outputs": [],
   "source": [
    "!wget -O ukr_rus.train.txt -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1vAK0SWXUqei4zTimMvIhH3ufGPsbnC_O\"\n",
    "!wget -O ukr_rus.test.txt -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1W9R2F8OeKHXruo2sicZ6FgBJUTJc8Us_\"\n",
    "!wget -O fairy_tale.txt -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1sq8zSroFeg_afw-60OmY8RATdu_T1tej\"\n",
    "\n",
    "# Install the PyDrive wrapper & import libraries.\n",
    "# This only needs to be done once per notebook.\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# Authenticate and create the PyDrive client.\n",
    "# This only needs to be done once per notebook.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "downloaded = drive.CreateFile({'id': '1d7OXuil646jUeDS1JNhP9XWlZogv6rbu'})\n",
    "downloaded.GetContentFile('cc.ru.300.vec.zip')\n",
    "\n",
    "downloaded = drive.CreateFile({'id': '1yAqwqgUHtMSfGS99WLGe5unSCyIXfIxi'})\n",
    "downloaded.GetContentFile('cc.uk.300.vec.zip')\n",
    "\n",
    "!unzip cc.ru.300.vec.zip\n",
    "!unzip cc.uk.300.vec.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RqUeOXxws8y"
   },
   "source": [
    "Let's implement a simple machine translator.\n",
    "\n",
    "The idea is based on the paper [Word Translation Without Parallel Data](https://arxiv.org/pdf/1710.04087.pdf). There are lots of interesting things in the repo: [https://github.com/facebookresearch/MUSE](https://github.com/facebookresearch/MUSE).\n",
    "\n",
    "And we are going to translate from Ukrainian to Russian. They are quite similar languages with similar syntax. This is why we can substitute words from one language with words from another and expect something coherent in the result.\n",
    "\n",
    "That is, we are going to learn how embeddings from one language correspond to embeddings from another, like this:\n",
    "\n",
    "![](https://raw.githubusercontent.com/facebookresearch/MUSE/master/outline_all.png)\n",
    "\n",
    "Than we will simply map the source word (the word in the sentence we want to translate) to the target embedding space and take the word with the nearest embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjPj9FTRry0U"
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "ru_emb = KeyedVectors.load_word2vec_format(\"cc.ru.300.vec\")\n",
    "uk_emb = KeyedVectors.load_word2vec_format(\"cc.uk.300.vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rGx4TXWFJ65"
   },
   "source": [
    "Look at the pair `серпень-август` (which are translation, means august)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FkHer36xyh4n"
   },
   "outputs": [],
   "source": [
    "ru_emb.most_similar([ru_emb[\"август\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1RSDixWvylEP"
   },
   "outputs": [],
   "source": [
    "uk_emb.most_similar([uk_emb[\"серпень\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iwmm3YQ1yl1U"
   },
   "outputs": [],
   "source": [
    "ru_emb.most_similar([uk_emb[\"серпень\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lAsW7oxszE_I"
   },
   "outputs": [],
   "source": [
    "def load_word_pairs(filename):\n",
    "    uk_ru_pairs = []\n",
    "    uk_vectors = []\n",
    "    ru_vectors = []\n",
    "    with open(filename, \"r\", encoding='utf8') as inpf:\n",
    "        for line in inpf:\n",
    "            uk, ru = line.rstrip().split(\"\\t\")\n",
    "            if uk not in uk_emb or ru not in ru_emb:\n",
    "                continue\n",
    "            uk_ru_pairs.append((uk, ru))\n",
    "            uk_vectors.append(uk_emb[uk])\n",
    "            ru_vectors.append(ru_emb[ru])\n",
    "    return uk_ru_pairs, np.array(uk_vectors), np.array(ru_vectors)\n",
    "\n",
    "\n",
    "uk_ru_train, X_train, Y_train = load_word_pairs(\"ukr_rus.train.txt\")\n",
    "uk_ru_test, X_test, Y_test = load_word_pairs(\"ukr_rus.test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9z6ts7DC0XmN"
   },
   "source": [
    "### Learning the mapping from the embedding spaces\n",
    "\n",
    "We have pairs of corresponding words. So we have to find a mapping which would map their embeddings to be as near as possible.\n",
    "\n",
    "$$W^*= \\arg\\min_W ||WX - Y||_F, \\text{where} ||*||_F - \\text{Frobenius norm}$$\n",
    "\n",
    "This function is similar to the linear regression (without bias).\n",
    "\n",
    "**Task** Implement it - use `LinearRegression` from sklearn with `fit_intercept=False`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fraTOQtu1YWI"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "mapping = LinearRegression(fit_intercept=False).fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PrzRk3ja1b_6"
   },
   "source": [
    "Check it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Quax6HnF1aON"
   },
   "outputs": [],
   "source": [
    "august = mapping.predict(uk_emb[\"серпень\"].reshape(1, -1))\n",
    "ru_emb.most_similar(august)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ih1GLNZt1nZX"
   },
   "source": [
    "Expected that the top contains different months, but `август` is not the first.\n",
    "\n",
    "We are going to evaluate the mapping by precision@k metric with k = 1, 5, 10.\n",
    "\n",
    "**Task** Implement following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JnmrLp9y2gNI"
   },
   "outputs": [],
   "source": [
    "def precision(pairs, mapped_vectors, topn=1):\n",
    "    \"\"\"\n",
    "    :args:\n",
    "        pairs = list of right word pairs [(uk_word_0, ru_word_0), ...]\n",
    "        mapped_vectors = list of embeddings after mapping from source embedding space to destination embedding space\n",
    "        topn = the number of nearest neighbours in destination embedding space to choose from\n",
    "    :returns:\n",
    "        precision_val, float number, total number of words for those we can find right translation at top K.\n",
    "    \"\"\"\n",
    "    assert len(pairs) == len(mapped_vectors)\n",
    "    <implement it>\n",
    "    return precision_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-1NIvhSH2olG"
   },
   "outputs": [],
   "source": [
    "assert precision([(\"серпень\", \"август\")], august, topn=5) == 0.0\n",
    "assert precision([(\"серпень\", \"август\")], august, topn=9) == 1.0\n",
    "assert precision([(\"серпень\", \"август\")], august, topn=10) == 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Ml_w1Tl2r7Y"
   },
   "outputs": [],
   "source": [
    "assert precision(uk_ru_test, X_test) == 0.0\n",
    "assert precision(uk_ru_test, Y_test) == 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-d9KQHMr2tx8"
   },
   "outputs": [],
   "source": [
    "precision_top1 = precision(uk_ru_test, mapping.predict(X_test), 1)\n",
    "precision_top5 = precision(uk_ru_test, mapping.predict(X_test), 5)\n",
    "\n",
    "assert precision_top1 >= 0.635\n",
    "assert precision_top5 >= 0.813"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNbDTP502urT"
   },
   "source": [
    "### Improving Mapping\n",
    "\n",
    "It can be proven that the mapping with orthogonal constraint is better:\n",
    "$$W^*= \\arg\\min_W ||WX - Y||_F \\text{, where: } W^TW = I$$\n",
    "\n",
    "You can find it using SVD:\n",
    "$$X^TY=U\\Sigma V^T\\text{, singular value decompostion}$$\n",
    "\n",
    "$$W^*=UV^T$$\n",
    "\n",
    "**Task** Implement the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9de8XZ_F3v53"
   },
   "outputs": [],
   "source": [
    "def learn_transform(X_train, Y_train):\n",
    "    \"\"\" \n",
    "    :returns: W* : float matrix[emb_dim x emb_dim] as defined in formulae above\n",
    "    \"\"\"    \n",
    "    <calculate it>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8WeCadzN382y"
   },
   "outputs": [],
   "source": [
    "W = learn_transform(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p6qaMb0E3-f9"
   },
   "outputs": [],
   "source": [
    "ru_emb.most_similar([np.matmul(uk_emb[\"серпень\"], W)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Nn58crh4AH0"
   },
   "outputs": [],
   "source": [
    "assert precision(uk_ru_test, np.matmul(X_test, W)) >= 0.653\n",
    "assert precision(uk_ru_test, np.matmul(X_test, W), 5) >= 0.824"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqgcYk-c4DE5"
   },
   "source": [
    "### Writing the translator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwi70fP6FaAN"
   },
   "source": [
    "Now we are ready to implement the translation function. It should find the nearest vector in the target (Russian) embedding space and return the source word if it is not in the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0etAHUks4JOr"
   },
   "outputs": [],
   "source": [
    "with open(\"fairy_tale.txt\", \"r\") as in f:\n",
    "    uk_sentences = [line.rstrip().lower() for line in in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JK_FJGmn4N7V"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    \"\"\"\n",
    "    :args:\n",
    "        sentence - sentence in Ukrainian (str)\n",
    "    :returns:\n",
    "        translation - sentence in Russian (str)\n",
    "\n",
    "    * find ukrainian embedding for each word in sentence\n",
    "    * transform ukrainian embedding vector\n",
    "    * find nearest russian word and replace\n",
    "    \"\"\"\n",
    "    <implement it>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H47pbFyk4P6D"
   },
   "outputs": [],
   "source": [
    "assert translate(\".\") == \".\"\n",
    "assert translate(\"1 , 3\") == \"1 , 3\"\n",
    "assert translate(\"кіт зловив мишу\") == \"кот поймал мышку\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PAVWK7mE4RYU"
   },
   "outputs": [],
   "source": [
    "for sentence in uk_sentences:\n",
    "    print(\"src: {}\\ndst: {}\\n\".format(sentence, translate(sentence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5GrChTeFqIg"
   },
   "source": [
    "# Supplementary Materials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwffxpbmFwDh"
   },
   "source": [
    "## To read\n",
    "### Basic knowledge:  \n",
    "[On word embeddings - Part 1, Sebastian Ruder](http://ruder.io/word-embeddings-1/)  \n",
    "[Deep Learning, NLP, and Representations, Christopher Olah](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)  \n",
    "\n",
    "### How to clusterize embeddings:  \n",
    "[Making Sense of Word Embeddings (2016), Pelevina et al](http://anthology.aclweb.org/W16-1620)    \n",
    "\n",
    "### How to evaluate embeddings:\n",
    "[Evaluation methods for unsupervised word embeddings (2015), T. Schnabel](http://www.aclweb.org/anthology/D15-1036)  \n",
    "[Intrinsic Evaluation of Word Vectors Fails to Predict Extrinsic Performance (2016), B. Chiu](https://www.aclweb.org/anthology/W/W16/W16-2501.pdf)  \n",
    "[Problems With Evaluation of Word Embeddings Using Word Similarity Tasks (2016), M. Faruqui](https://arxiv.org/pdf/1605.02276.pdf)  \n",
    "[Improving Reliability of Word Similarity Evaluation by Redesigning Annotation Task and Performance Measure (2016), Oded Avraham, Yoav Goldberg](https://arxiv.org/pdf/1611.03641.pdf)  \n",
    "[Evaluating Word Embeddings Using a Representative Suite of Practical Tasks (2016), N. Nayak](https://cs.stanford.edu/~angeli/papers/2016-acl-veceval.pdf)  \n",
    "\n",
    "\n",
    "## To watch\n",
    "[Word Vector Representations: word2vec, Lecture 2, cs224n](https://www.youtube.com/watch?v=ERibwqs9p38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9vacV4BIFI8l"
   },
   "outputs": [],
   "source": [
    "!pip3 -qq install torch==1.1\n",
    "!pip -qq install nltk==3.2.5\n",
    "!pip -qq install gensim==3.6.0\n",
    "!pip -qq install bokeh==1.0.4\n",
    "\n",
    "!wget -O quora.zip -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1ERtxpdWOgGQ3HOigqAMHTJjmOE_tWvoF\"\n",
    "!unzip quora.zip\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XIFSTdJG95SZ"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim \n",
    "\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbpWIAreB6ky"
   },
   "source": [
    "# Introduction to PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_M0mMOadG8aZ"
   },
   "source": [
    "PyTorch is one of the most well-known frameworks for building neural networks (which is what we're gonna do in this course).\n",
    "\n",
    "The most obvious alternative is Tensorflow, but right now (at fall of 2018) it's much less user-friendly so we'll stick to pytorch.\n",
    "\n",
    "And come on, if you learn one of them, you'll be able to learn another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsScdJ7DLZCm"
   },
   "source": [
    "## Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1bSmJbXOPbk"
   },
   "source": [
    "Let's start with one of the fundamental pytorch concepts - automatic differentiation (autograd)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bY9FHLM-M4aW"
   },
   "source": [
    "### Computational Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkvCloDpNXdH"
   },
   "source": [
    "Computational graphs provide a very convenient way to represent functions and calculate their gradients.\n",
    "\n",
    "For instance,\n",
    "$$f = (x + y) \\cdot z$$\n",
    "\n",
    "Can be represented with this graph:  \n",
    "![graph](https://github.com/DanAnastasyev/DeepNLP-Course/raw/master/Week%2003/Images/Circuit.png)  \n",
    "*From [Backpropagation, Intuitions - CS231n](http://cs231n.github.io/optimization-2/)*\n",
    "\n",
    "*The forward pass* computes value of the function (green numbers). It starts from the inputs (on the left) and applies the sequence of functions.\n",
    "\n",
    "*The backward pass* (or *back propagation*) is designed to compute gradients of the function. That is $\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}, \\frac{\\partial f}{\\partial z}$ in our case. It starts from the output and applies *chain rule* to compute them.\n",
    "\n",
    "For instance, for $f = q \\cdot z$, we have $\\frac{\\partial f}{\\partial q} = z$ and $\\frac{\\partial f}{\\partial z} = q$.  \n",
    "For $q = x + y$, we have $\\frac{\\partial q}{\\partial x} = \\frac{\\partial q}{\\partial y} = 1$.  \n",
    "Finally, we can apply the chain rule: $\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial q} \\frac{\\partial q}{\\partial x} = z$.\n",
    "\n",
    "*If you had problems with understanding the stuff above (and even if didn't), check this great tutorial: [Backpropagation, Intuitions - CS231n](http://cs231n.github.io/optimization-2/).*\n",
    "\n",
    "Well, such calculations in pytorch are fairly simple. You just have to describe your function as a sequence of operations, like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lw4ASRktLdO4"
   },
   "outputs": [],
   "source": [
    "x = torch.tensor(-2., requires_grad=True)\n",
    "y = torch.tensor(5., requires_grad=True)\n",
    "z = torch.tensor(-4., requires_grad=True)\n",
    "\n",
    "q = x + y\n",
    "f = q * z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-78COM99N8YL"
   },
   "source": [
    "Call it with some arguments and then ask it like, \"Hey, calc your grads, please\". And the magic happens:\n",
    "\n",
    "![graph](https://raw.githubusercontent.com/pytorch/pytorch/master/docs/source/_static/img/dynamic_graph.gif)  \n",
    "*From [github.com/pytorch/pytorch](https://github.com/pytorch/pytorch)*\n",
    "\n",
    "Pytorch builds graph and performs backward pass - all by itself.\n",
    "\n",
    "If you already know tensorflow, you'll see the main difference: graph is built dynamically, it hasn't be compiled and stuff. \n",
    "\n",
    "Basically, it means that you can debug your code much more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9FOlPMIQMfbq"
   },
   "outputs": [],
   "source": [
    "f.backward()\n",
    "\n",
    "print('df/dz =', z.grad)\n",
    "print('df/dx =', x.grad)\n",
    "print('df/dy =', y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JotDf1naGU-R"
   },
   "source": [
    "The call of the `backward()` method calculates gradients for all tensors in graph (except the subgraphs where `requires_grad == False`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSiB1CGyJMzt"
   },
   "source": [
    "*Read about autograd in pytorch in depth here: [Autograd mechanics](https://pytorch.org/docs/stable/notes/autograd.html).*\n",
    "\n",
    "Well, the nicest thing about pytorch is that you can use it like you used numpy. You use `ndarray` all the time, right.\n",
    "\n",
    "There is an analog for it - `tensor`. We just created few of them actually.\n",
    "\n",
    "It stores data, like `ndarray`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DY2CcCw2Gmgq"
   },
   "outputs": [],
   "source": [
    "x.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYxD8N_9GpJl"
   },
   "source": [
    "And gradient (unlike `ndarray`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wYCD5P24GufX"
   },
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwLx4szvGwMb"
   },
   "source": [
    "Add function used to compute the gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TTfGdUF_GzV8"
   },
   "outputs": [],
   "source": [
    "q.grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgK1Esa6HHAB"
   },
   "source": [
    "And lots of meta-info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nazaer0AG4pL"
   },
   "outputs": [],
   "source": [
    "x.type(), x.shape, x.device, x.layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Be6fwAky0pr"
   },
   "source": [
    "Check this tutorial to learn more about tensors: [Deep Learning with PyTorch: A 60 Minute Blitz > What is PyTorch?](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py)\n",
    "\n",
    "Sometimes we don't want to compute the gradients. To handle this cases (we'll discuss particular examples very soon), you can use context managers ([Locally disabling gradient computation](https://pytorch.org/docs/stable/autograd.html#locally-disabling-gradient-computation)):\n",
    "```python\n",
    "torch.autograd.no_grad()\n",
    "torch.autograd.enable_grad()\n",
    "torch.autograd.set_grad_enabled(mode)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQEJeqfnJPpA"
   },
   "outputs": [],
   "source": [
    "with torch.autograd.no_grad():\n",
    "    x = torch.tensor(-2., requires_grad=True)\n",
    "    y = torch.tensor(5., requires_grad=True)\n",
    "    q = x + y\n",
    "\n",
    "z = torch.tensor(-4., requires_grad=True)\n",
    "f = q * z\n",
    "\n",
    "f.backward()\n",
    "\n",
    "print('df/dz =', z.grad)\n",
    "print('df/dx =', x.grad)\n",
    "print('df/dy =', y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvLFlc4iQOQv"
   },
   "source": [
    "Well, the question is why on earth you would want it to use :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlhLBWwHG3Xe"
   },
   "source": [
    "### Warm-up Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kaqtIIvJOEut"
   },
   "source": [
    "To understand it, let's write a simple linear regression implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QDZpEHF8AKH2"
   },
   "outputs": [],
   "source": [
    "w_orig, b_orig = 2.6, -0.4\n",
    "\n",
    "X = np.random.rand(100) * 10. - 5.\n",
    "y_orig = w_orig * X + b_orig\n",
    "\n",
    "y = y_orig + np.random.randn(100)\n",
    "\n",
    "plt.plot(X, y, '.')\n",
    "plt.plot(X, y_orig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2K5MVtiSGuC"
   },
   "source": [
    "There are two parameters $w$ and $b$. We want them to be as near to $w_{orig}, b_{orig}$ as possible.\n",
    "\n",
    "What are we going to optimize? Let's optimize MSE loss:\n",
    "$$J(w, b) = \\frac{1}{N} \\sum_{i=1}^N || \\hat y_i - y_i(w, b)||^2 =\\frac{1}{N} \\sum_{i=1}^N || \\hat y_i - (w \\cdot x_i + b)||^2. $$\n",
    "\n",
    "We can use *gradient descent* algorithm to optimize it (not even stohastic right now):\n",
    "$$w_{t+1} := w_t - \\alpha \\cdot \\frac{\\partial J}{\\partial w}(w_t, b_t)$$\n",
    "$$b_{t+1} := w_t - \\alpha \\cdot \\frac{\\partial J}{\\partial b}(w_t, b_t)$$\n",
    "\n",
    "*You see, it would be nice to use backpropagation here.*\n",
    "\n",
    "**Task** Implement the optimization using pure numpy.\n",
    "\n",
    "You'll need:\n",
    "1. Perform the forward pass: $y(w, b) = w \\cdot x + b$;\n",
    "2. Find the gradients $\\frac{\\partial J}{\\partial w}, \\frac{\\partial J}{\\partial b}$ using backward pass;\n",
    "3. Move $w, b$ in the anti-gradients direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VKbqTNVXFB3A"
   },
   "outputs": [],
   "source": [
    "def display_progress(epoch, loss, w, b, X, y, y_pred):\n",
    "    clear_output(True)\n",
    "    print('Epoch = {}, Loss = {}, w = {}, b = {}'.format(epoch, loss, w, b))\n",
    "    plt.plot(X, y, '.')\n",
    "    plt.plot(X, y_pred)\n",
    "    plt.show()\n",
    "    time.sleep(1)\n",
    "\n",
    "\n",
    "w = np.random.randn()\n",
    "b = np.random.randn()\n",
    "\n",
    "alpha = 0.01\n",
    "\n",
    "for i in range(100):\n",
    "    <calculate model's output>\n",
    "    \n",
    "    <calculate loss>\n",
    "    \n",
    "    <calculate gradients>\n",
    "    \n",
    "    <update w and b>\n",
    "    \n",
    "    if (i + 1) % 5 == 0:\n",
    "        display_progress(i + 1, loss, w, b, X, y, y_pred)\n",
    "        \n",
    "assert np.abs(w - w_orig) < 0.1, 'Something went wrong :('"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8WgWrF4C2WK"
   },
   "source": [
    "It's much simpler to implement the same thing using pytorch.\n",
    "\n",
    "The forward pass will look almost the same. And we've already learnt how to perform the backward pass! Just call `loss.backward()`.\n",
    "\n",
    "But there are a number of caveats you have to know about. \n",
    "\n",
    "First of all, one doen't simply update `w` and `b`. Try this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zx4DoGeBMJd4"
   },
   "outputs": [],
   "source": [
    "w = torch.randn(1, requires_grad=True)\n",
    "\n",
    "w -= 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OjoUh-SMPBt"
   },
   "source": [
    "It should fail with a message like:  \n",
    "`RuntimeError: a leaf Variable that requires grad has been used in an in-place operation.`\n",
    "\n",
    "The issue is in the support of in-place operations in autograd: [In place operations with autograd](https://pytorch.org/docs/stable/notes/autograd.html#in-place-operations-with-autograd).\n",
    "\n",
    "But actually we are not going to perform an operation that requires gradients. We're just updating the value of the note.\n",
    "\n",
    "To fight this problem, we can either use `no_grad` context or update the underline data used by the tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zegkKd-cMOMj"
   },
   "outputs": [],
   "source": [
    "w.data -= 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVlaIdvHNXR_"
   },
   "source": [
    "Another thing you should be aware of is that the gradients are accumulating by default. So you have to update them yourself between `loss.backward()` calls:\n",
    "```python\n",
    "w.grad.zero_()\n",
    "b.grad.zero_()\n",
    "```\n",
    "\n",
    "**Task** Implement the linear regression on pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VRqxypuEU2ig"
   },
   "outputs": [],
   "source": [
    "X = torch.as_tensor(X).float()\n",
    "y = torch.as_tensor(y).float()\n",
    "\n",
    "w = torch.randn(1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "\n",
    "for i in range(100):\n",
    "    <copy forward pass and add backward pass + parameters updates>\n",
    "    \n",
    "    if (i + 1) % 5 == 0:\n",
    "        display_progress(i + 1, loss, w.item(), b.item(), \n",
    "                         X.data.numpy(), y.data.numpy(), y_pred.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KaKTKN_fOvo-"
   },
   "source": [
    "Much simpler, isn't it? :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZNq6ujzPtvd"
   },
   "source": [
    "## Word Embeddings (via High-Level PyTorch API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITLgcVz66AfV"
   },
   "source": [
    "Let's move now to more high-level API, where all the good neural network parts are implemented. Quite comprehensive description of it is given in this tutorial: [What is torch.nn really?](https://pytorch.org/tutorials/beginner/nn_tutorial.html)\n",
    "\n",
    "Last time we used gensim to train word2vec model. Now we're ready to implement our own model.\n",
    "\n",
    "Well, almost ready. We still haven't discussed what word2vec is.\n",
    "\n",
    "The key idea is simple: you can understand the meaning of the word by his neighbours (the words that appear frequently in its context):  \n",
    "![contexts](https://image.ibb.co/mnQ2uz/2018_09_17_21_07_08.png)\n",
    "*From [cs224n, Lecture 2](http://web.stanford.edu/class/cs224n/lectures/lecture2.pdf)*\n",
    "\n",
    "The first idea is just to use counts of the words in context as a meaningful word vector.\n",
    "\n",
    "For instance, for such simple corpus:\n",
    "\n",
    "```\n",
    "The red fox jumped\n",
    "The brown fox jumped\n",
    "```\n",
    "\n",
    "we'll have following count vectors:\n",
    "```\n",
    "        the fox jumped red brown\n",
    "red   = (1   1    1     0    0)\n",
    "brown = (1   1    1     0    0)\n",
    "```\n",
    "\n",
    "You see, `red` and `brown` have similar vector! The problem is almost solved. But we have to obtain much smaller embedding vectors.\n",
    "\n",
    "And here is what word2vec algorithms do. They build embedding vectors based on the neighbours of the word in corpus.\n",
    "\n",
    "A nice introduction is given in this post: [king - man + woman is queen; but why?](http://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html)\n",
    "\n",
    "Let's do some preparation work before moving to the interesting stuff.\n",
    "\n",
    "**Task** Tokenize and lower-case texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hKKb9Ya8hzIb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "quora_data = pd.read_csv('train.csv')\n",
    "\n",
    "quora_data.question1 = quora_data.question1.replace(np.nan, '', regex=True)\n",
    "quora_data.question2 = quora_data.question2.replace(np.nan, '', regex=True)\n",
    "\n",
    "texts = list(pd.concat([quora_data.question1, quora_data.question2]).unique())\n",
    "\n",
    "tokenized_texts = [<do it there>]\n",
    "\n",
    "assert len(tokenized_texts) == len(texts)\n",
    "assert isinstance(tokenized_texts[0], list)\n",
    "assert isinstance(tokenized_texts[0][0], str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYoj91iDDDfT"
   },
   "source": [
    "Collect the indices of the most frequent words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5PL471pGjuVN"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "MIN_COUNT = 5\n",
    "\n",
    "words_counter = Counter(token for tokens in tokenized_texts for token in tokens)\n",
    "word2index = {\n",
    "    '<unk>': 0\n",
    "}\n",
    "\n",
    "for word, count in words_counter.most_common():\n",
    "    if count < MIN_COUNT:\n",
    "        break\n",
    "        \n",
    "    word2index[word] = len(word2index)\n",
    "    \n",
    "index2word = [word for word, _ in sorted(word2index.items(), key=lambda x: x[1])]\n",
    "    \n",
    "print('Vocabulary size:', len(word2index))\n",
    "print('Tokens count:', sum(len(tokens) for tokens in tokenized_texts))\n",
    "print('Unknown tokens appeared:', sum(1 for tokens in tokenized_texts for token in tokens if token not in word2index))\n",
    "print('Most freq words:', index2word[1:21])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF5mYpCsE9Uh"
   },
   "source": [
    "### Skip-Gram Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "om1IG5XEMGRa"
   },
   "source": [
    "Word2vec is actually a set of models used to build word embeddings.\n",
    "\n",
    "We are going to start with the *skip-gram model*.\n",
    "\n",
    "It's a very simple neural network with just two layers. It aims to build word vectors that encode information about the co-occurring words:  \n",
    "![](https://i.ibb.co/nL0LLD2/Word2vec-Example.jpg)  \n",
    "*From cs224n, Lecture 2*\n",
    "\n",
    "More precisely, it models the probabilities $\\{P(w_{c+j}|w_c):  j = c-k, ..., c+k, j \\neq c\\}$, where $k$ is the context window size, $c$ is index of the central word (which embedding we are trying to optimize).\n",
    "\n",
    "The learnable parameters of the model are following: matrix $U$ (embeddings' matrix that is used in all downstream tasks. In gensim it's called `syn0`) and matrix $V$ - output layer of the model (in gensim it's called `syn1`).\n",
    "\n",
    "Two vectors correspond to each word: a row in $U$ and a column in $V$. That is $U \\in \\mathbb{R}^{|V|, d}$ and $V \\in \\mathbb{R}^{d, |V|}$, where $d$ is embedding size and $|V|$ is the vocabulary size.\n",
    "\n",
    "As a result, the neural network looks this way:  \n",
    "![skip-gram](https://i.ibb.co/F54XzDC/SkipGram.png)\n",
    "\n",
    "What's going on and how it is connected to probability and word context?\n",
    "\n",
    "Well, the word is mapped to its embedding $u_c$. Then this embedding is multiplied to matrix $V$. \n",
    "\n",
    "As a result, we obtain the set of scores $\\{v_j^T u_c : j \\in {0, \\ldots, |V|}\\}$. Each corresponds to the similarity between the word $w_j$ vector and our word vector. It's very similar to the cosine similarity we calculated in the previous lesson, but without normalization.\n",
    "\n",
    "This similarities show how likely $w_j$ can be in context of word $w_c$. That means, that they can be converted to probability using the softmax function:\n",
    "$$P(w_j | w_c) = \\frac{\\exp(v_{j}^T u_c)}{\\sum_{i=1}^{|V|} \\exp(v_i^T u_c)}.$$\n",
    "\n",
    "So for each word we calculate such probability distribution over our vocabulary. It's shown in using blue bars in the picture above. More likely word - bluer is the corresponding cell.\n",
    "\n",
    "The model learns to distribute the probabilities between the co-occuring words for the given one. We'll use cross-entropy loss for it:\n",
    "$$-\\sum_{-k \\leq j \\leq k, j \\neq 0} \\log \\frac{\\exp(v_{c+j}^T u_c)}{\\sum_{i=1}^{|V|} \\exp(v_i^T u_c)} \\to \\min_{U, V}.$$\n",
    "\n",
    "For instance, for the sample from the picture model will be penalized if it outputs a low probability of word `over`.\n",
    "\n",
    "Please, notice that we calculate the similarity between vectors from different vector spaces. $u_c$ is the vector from the input embeddings and $v_j$ is the vector from the output embeddings. A high similarity between them means that they co-occur frequently, not that they are similar in the syntactic role or their semantics.\n",
    "\n",
    "On the other hand, the similarity between $u_k$ and $u_m$ means that their output distributions are similar. And that means exactly that the similarity of the count vectors we discussed before and also most probably means their syntactic or semantic similarity.\n",
    "\n",
    "Check this demo to understand what's going on in more depth: [https://ronxin.github.io/wevi/](https://ronxin.github.io/wevi/).\n",
    "\n",
    "Let's implement it now!\n",
    "\n",
    "#### Batches Generations\n",
    "\n",
    "First of all, we need to collect all the contexts from our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ocrsXgaynYPG"
   },
   "outputs": [],
   "source": [
    "def build_contexts(tokenized_texts, window_size):\n",
    "    contexts = []\n",
    "    for tokens in tokenized_texts:\n",
    "        for i in range(len(tokens)):\n",
    "            central_word = tokens[i]\n",
    "            context = [tokens[i + delta] for delta in range(-window_size, window_size + 1) \n",
    "                       if delta != 0 and i + delta >= 0 and i + delta < len(tokens)]\n",
    "\n",
    "            contexts.append((central_word, context))\n",
    "            \n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQBa6yQ9BXjp"
   },
   "outputs": [],
   "source": [
    "contexts = build_contexts(tokenized_texts, window_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8o_ePiZ7wfpT"
   },
   "source": [
    "Check, what you got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KyQNK-9SBdb9"
   },
   "outputs": [],
   "source": [
    "contexts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbQKln_6yC4l"
   },
   "source": [
    "Let's convert words to indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hOPRlKlLvUBA"
   },
   "outputs": [],
   "source": [
    "contexts = [(word2index.get(central_word, 0), [word2index.get(word, 0) for word in context]) \n",
    "            for central_word, context in contexts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GYmrAi9gyIe-"
   },
   "source": [
    "Neural networks are optimized using stochastic gradient descent methods. Which means, we need a batch generator - a function that generates samples to optimize neural network with.\n",
    "\n",
    "A simple batch generator looks this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wC9SifFU5iQP"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def make_skip_gram_batches_iter(contexts, window_size, num_skips, batch_size):\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * window_size\n",
    "    \n",
    "    central_words = [word for word, context in contexts if len(context) == 2 * window_size and word != 0]\n",
    "    contexts = [context for word, context in contexts if len(context) == 2 * window_size and word != 0]\n",
    "    \n",
    "    batch_size = int(batch_size / num_skips)\n",
    "    batches_count = int(math.ceil(len(contexts) / batch_size))\n",
    "    \n",
    "    print('Initializing batches generator with {} batches per epoch'.format(batches_count))\n",
    "    \n",
    "    while True:\n",
    "        indices = np.arange(len(contexts))\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        for i in range(batches_count):\n",
    "            batch_begin, batch_end = i * batch_size, min((i + 1) * batch_size, len(contexts))\n",
    "            batch_indices = indices[batch_begin: batch_end]\n",
    "\n",
    "            batch_data, batch_labels = [], []\n",
    "\n",
    "            for data_ind in batch_indices:\n",
    "                central_word, context = central_words[data_ind], contexts[data_ind]\n",
    "                \n",
    "                words_to_use = random.sample(context, num_skips)\n",
    "                batch_data.extend([central_word] * num_skips)\n",
    "                batch_labels.extend(words_to_use)\n",
    "            \n",
    "            yield {\n",
    "                'tokens': torch.cuda.LongTensor(batch_data), \n",
    "                'labels': torch.cuda.LongTensor(batch_labels)\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmRGZ-vG5iQR"
   },
   "source": [
    "Check it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L_Yeowx15iQS"
   },
   "outputs": [],
   "source": [
    "batch = next(make_skip_gram_batches_iter(contexts, window_size=2, num_skips=2, batch_size=32))\n",
    "\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DXjZS3JyQZh"
   },
   "source": [
    "#### nn.Sequential\n",
    "\n",
    "The simplest way to implement a model on pytorch is to use `nn.Sequential`. Just define the order of layers and it will apply them sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WRw9Z4G__46O"
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Embedding(len(word2index), 32),\n",
    "    nn.Linear(32, len(word2index))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ysn0DDpLyj1S"
   },
   "source": [
    "Yep, we've just defined our skip-gram model! \n",
    "\n",
    "The construction above says that we need a `nn.Embedding` layer (a layer that maps index to a vector. In our case it would be an index from the `range(len(word2index))` and a 32-dimensional vector) following by a `nn.Linear` layer (just a dot-product of an input vector to the learnable matrix with addition of a learnable bias).\n",
    "\n",
    "There is another pytorch's feature we haven't discussed yet. It's computations on GPU. Neural networks are usually trained on GPUs because it's much faster.\n",
    "\n",
    "It's extremely easy to ask pytorch to perform calculations on the GPU. Just call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EfmaUi3Uy9YT"
   },
   "outputs": [],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3c3UEa2zHhk"
   },
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OHxAg5ZWzEKT"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prP1avp-5DjZ"
   },
   "source": [
    "We'll apply it to the data in the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jsa-P9VS49RD"
   },
   "outputs": [],
   "source": [
    "tokens, labels = batch['tokens'], batch['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtLMvOO2z3c8"
   },
   "source": [
    "Now, to perform the forward pass just call the model with its input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N9wTpewTz3Dk"
   },
   "outputs": [],
   "source": [
    "logits = model(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWJmDy_uzJgD"
   },
   "source": [
    "We can calculate the loss now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h7rlD62_ykYl"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "loss = criterion(logits, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAwx-pck0RxX"
   },
   "source": [
    "And, of course, perform the backward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JWt6gL0_0Npp"
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJkDOl6szRLm"
   },
   "source": [
    "Finally, we have to update the parameters. We can do it as before (like, `w.data -= alpha * w.grad`). But pytorch contains predefined optimizers that can do the same things (and more complicated stuff).\n",
    "\n",
    "We are going to use Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-b5CIARzQ6m"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ju5lO0Xi0hsV"
   },
   "source": [
    "To update the weights just call `optimizer.step()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_9QK7nHu0Zw8"
   },
   "outputs": [],
   "source": [
    "print(model[1].weight)\n",
    "\n",
    "optimizer.step()\n",
    "\n",
    "print(model[1].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hnxyk1ew0pSk"
   },
   "source": [
    "And finally, don't forget to zero grads!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uMsuvEP90svi"
   },
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXSzfnLf3hDa"
   },
   "source": [
    "You can ask optimizer to do it for you, you see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PibTw33Azg7q"
   },
   "source": [
    "#### Train Cycle\n",
    "\n",
    "We are ready to implement the training cycle - just like we did for our linear regression.\n",
    "\n",
    "**Task** Implement the cycle. Train the model for a few epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ewGMgYTXANzz"
   },
   "outputs": [],
   "source": [
    "loss_every_nsteps = 1000\n",
    "total_loss = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for step, (batch, labels) in enumerate(make_skip_gram_batches_iter(contexts, window_size=2, num_skips=4, batch_size=128)):\n",
    "    <1. convert data to tensors>\n",
    "    \n",
    "    <2. make forward pass>\n",
    "\n",
    "    <3. make backward pass>\n",
    "\n",
    "    <4. apply optimizer>\n",
    "    \n",
    "    <5. zero grads>\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    \n",
    "    if step != 0 and step % loss_every_nsteps == 0:\n",
    "        print(\"Step = {}, Avg Loss = {:.4f}, Time = {:.2f}s\".format(step, total_loss / loss_every_nsteps, \n",
    "                                                                    time.time() - start_time))\n",
    "        total_loss = 0\n",
    "        start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pqq9kee41L4P"
   },
   "source": [
    "#### Analysis\n",
    "\n",
    "To get the embedding matrix, cast the following magic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uWsYkNn-Hnl_"
   },
   "outputs": [],
   "source": [
    "embeddings = model[0].weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fvE2za248_A"
   },
   "source": [
    "That is, we get the weights from the first layer of the model, move them from gpu to cpu and convert to numpy array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZtxY2D01RB6"
   },
   "source": [
    "Let's check how adequate are similarities that the model learnt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bhDwuhDSHEDm"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def most_similar(embeddings, index2word, word2index, word):\n",
    "    word_emb = embeddings[word2index[word]]\n",
    "    \n",
    "    similarities = cosine_similarity([word_emb], embeddings)[0]\n",
    "    top10 = np.argsort(similarities)[-10:]\n",
    "    \n",
    "    return [index2word[index] for index in reversed(top10)]\n",
    "\n",
    "most_similar(embeddings, index2word, word2index, 'warm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0VS1x-mO1WKS"
   },
   "source": [
    "And visualizations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yuXv2HxsAecb"
   },
   "outputs": [],
   "source": [
    "import bokeh.models as bm, bokeh.plotting as pl\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "\n",
    "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
    "                 width=600, height=400, show=True, **kwargs):\n",
    "    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n",
    "    output_notebook()\n",
    "    \n",
    "    if isinstance(color, str): \n",
    "        color = [color] * len(x)\n",
    "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
    "\n",
    "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
    "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
    "\n",
    "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
    "    if show: \n",
    "        pl.show(fig)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def get_tsne_projection(word_vectors):\n",
    "    tsne = TSNE(n_components=2, verbose=100)\n",
    "    return scale(tsne.fit_transform(word_vectors))\n",
    "    \n",
    "    \n",
    "def visualize_embeddings(embeddings, index2word, word_count):\n",
    "    word_vectors = embeddings[1: word_count + 1]\n",
    "    words = index2word[1: word_count + 1]\n",
    "    \n",
    "    word_tsne = get_tsne_projection(word_vectors)\n",
    "    draw_vectors(word_tsne[:, 0], word_tsne[:, 1], color='green', token=words)\n",
    "    \n",
    "    \n",
    "visualize_embeddings(embeddings, index2word, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGfhLR6x8D3r"
   },
   "source": [
    "### Continuous Bag of Words (CBoW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UuVr2IsaYhX"
   },
   "source": [
    "Here is an alternative word2vec model:\n",
    "\n",
    "![](https://i.ibb.co/StXTMFH/CBOW.png)\n",
    "\n",
    "Now we have to predict the word by its context. The context is represented as a sum of context vectors.\n",
    "\n",
    "**Task** Implement the batch generator. It should output a context matrix `(samples_count, 2 * window_size)` which contains the context word indices and a target matrix `(samples_count)` with the central word indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VNaP0uaU7T2-"
   },
   "outputs": [],
   "source": [
    "def make_cbow_batches_iter(contexts, window_size, batch_size):\n",
    "    data = np.array([context for word, context in contexts if len(context) == 2 * window_size and word != 0])\n",
    "    labels = np.array([word for word, context in contexts if len(context) == 2 * window_size and word != 0])\n",
    "        \n",
    "    batches_count = int(math.ceil(len(data) / batch_size))\n",
    "    \n",
    "    print('Initializing batches generator with {} batches per epoch'.format(batches_count))\n",
    "    \n",
    "    while True:\n",
    "        indices = np.arange(len(data))\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        for i in range(batches_count):\n",
    "            <implement the generator>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBF7xiik7ZaN"
   },
   "source": [
    "Check it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0IVrQl8S4L9j"
   },
   "outputs": [],
   "source": [
    "window_size = 2\n",
    "batch_size = 32\n",
    "\n",
    "batch = next(make_cbow_batches_iter(contexts, window_size=window_size, batch_size=batch_size))\n",
    "\n",
    "assert isinstance(batch, dict)\n",
    "assert 'labels' in batch and 'tokens' in batch\n",
    "\n",
    "assert isinstance(batch['tokens'], torch.cuda.LongTensor)\n",
    "assert isinstance(batch['labels'], torch.cuda.LongTensor)\n",
    "\n",
    "assert batch['tokens'].shape == (batch_size, 2 * window_size)\n",
    "assert batch['labels'].shape == (batch_size,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nbKbZ_4E7T3U"
   },
   "source": [
    "The alternative way to define a model is to inherit it from `nn.Module`. It's a more flexible approach than defining a `nn.Sequential` model so we are going to use it mostly\n",
    "\n",
    "```python\n",
    "class MyNetModel(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MyNetModel, self).__init__()\n",
    "        <initialize layers>\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        <apply layers>\n",
    "        return final_output\n",
    "```\n",
    "\n",
    "You have to create all the learnable parameters (usually - layers) of the model in the `__init__` method and you have to apply them in the `forward` method.\n",
    "\n",
    "**Task** Build the `CBoWModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mkawxwe77T3V"
   },
   "outputs": [],
   "source": [
    "class CBoWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self._out_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        <apply the layers>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHhTDxya7a3S"
   },
   "source": [
    "Check it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nh_mNh__6lG2"
   },
   "outputs": [],
   "source": [
    "model = CBoWModel(vocab_size=len(word2index), embedding_dim=32).cuda()\n",
    "\n",
    "outputs = model(batch['tokens'])\n",
    "\n",
    "assert isinstance(outputs, torch.cuda.FloatTensor)\n",
    "assert outputs.shape == (batch_size, len(word2index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmn56yki7T3a"
   },
   "source": [
    "**Task** Train the model in the same way as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OzKLP9bs7T3b"
   },
   "outputs": [],
   "source": [
    "model = CBoWModel(vocab_size=len(word2index), embedding_dim=32).cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "loss_every_nsteps = 1000\n",
    "total_loss = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for step, batch in enumerate(make_cbow_batches_iter(contexts, window_size=2, batch_size=128)):\n",
    "    <copy-paste the learning cycle>\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    \n",
    "    if step != 0 and step % loss_every_nsteps == 0:\n",
    "        print(\"Step = {}, Avg Loss = {:.4f}, Time = {:.2f}s\".format(step, total_loss / loss_every_nsteps, \n",
    "                                                                    time.time() - start_time))\n",
    "        total_loss = 0\n",
    "        start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQW4PBdF96xC"
   },
   "source": [
    "Let's visualize what we got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fTEWcyYmvips"
   },
   "outputs": [],
   "source": [
    "visualize_embeddings(model.embeddings.weight.data.cpu().numpy(), index2word, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CON4VOyG3iET"
   },
   "source": [
    "### Negative Sampling\n",
    "\n",
    "What is the most computationally hard part of the model optimization? It's computation of softmax function over the vocabulary.\n",
    "\n",
    "To improve the model's performance *negative sampling* can be used.\n",
    "\n",
    "The idea is fairly obvious: let's predict the probability that the word $w$ can be in the context $c$: $P(D=1|w,c)$.\n",
    "\n",
    "The probability (again!) would be a function of the similarity between vectors: \n",
    "$$P(D=1|w, c) = \\sigma(v_w^T u_c) = \\frac 1 {1 + \\exp(-v^T_w u_c)}.$$\n",
    "\n",
    "The sigmoid function just maps the similarity to [0, 1] range.\n",
    "\n",
    "Well, we have positive samples (word-context pairs from the corpus), but we need some negative samples too to train our model. And we can generate them!  \n",
    "![Negative Sampling](https://i.ibb.co/D7rTdbr/Negative-Sampling.png)\n",
    "\n",
    "Just sample random word instead of the correct one and hope that they don't fit the context too well.\n",
    "\n",
    "The loss function (in CBoW setup) will be following:\n",
    "$$-\\log \\sigma(v_c^T u_c) - \\sum_{k=1}^K \\log \\sigma(-\\tilde v_k^T u_c),$$\n",
    "where $v_c$ - central word vector, $u_c$ - context vector (sum of vectors from the context), $\\tilde v_1, \\ldots, \\tilde v_K$ - vectors of the negative samples.\n",
    "\n",
    "Compare it with ordinary CBoW:\n",
    "$$-v_c^T u_c + \\log \\sum_{i=1}^{|V|} \\exp(v_i^T u_c).$$\n",
    "\n",
    "You see, it's quite similar, but the sum is over a much smaller number of samples.\n",
    "\n",
    "The sampling is performed from $U^{3/4}$, where $U$ - unigram distribution (word frequencies).\n",
    "\n",
    "We've already calculated the word counts (they were obtained when we called `Counter(words)`). \n",
    "\n",
    "Just convert them to frequencies and raise them to the power of $\\frac 3 4$. Why exactly $\\frac 3 4$? It's just a good constant, but the intuition is following:\n",
    "\n",
    "$$P(\\text{is}) = 0.9, \\ P(\\text{is})^{3/4} = 0.92$$\n",
    "$$P(\\text{Constitution}) = 0.09, \\ P(\\text{Constitution})^{3/4} = 0.16$$\n",
    "$$P(\\text{bombastic}) = 0.01, \\ P(\\text{bombastic})^{3/4} = 0.032$$\n",
    "\n",
    "The probability of high-frequent words stayed the same (approximately), while low-frequent words now are more likely.\n",
    "\n",
    "Nice description of this algorithm can be found in [cs224n lecture notes](https://github.com/maxim5/cs224n-winter-2017/blob/master/lecture_notes/cs224n-2017-notes1.pdf).\n",
    "\n",
    "**Task** Implement Negative Sampling.\n",
    "\n",
    "Define distribution first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zcX4vRBLlXy6"
   },
   "outputs": [],
   "source": [
    "words_sum_count = sum(words_counter.values())\n",
    "word_distribution = np.array([(words_counter[word] / words_sum_count) ** (3 / 4) for word in index2word])\n",
    "word_distribution /= word_distribution.sum()\n",
    "\n",
    "indices = np.arange(len(word_distribution))\n",
    "\n",
    "np.random.choice(indices, p=word_distribution, size=(32, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_o2pzsue16Lu"
   },
   "outputs": [],
   "source": [
    "class NegativeSamplingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self._out_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, inputs, targets, num_samples):\n",
    "        '''\n",
    "        inputs: (batch_size, context_size)\n",
    "        targets: (batch_size)\n",
    "        num_samples: int\n",
    "        Returns loss calculated like in the formula above\n",
    "        '''\n",
    "        \n",
    "        <calc u_c (using self._embedding) & v_c (using self._out_layer)>\n",
    "        \n",
    "        <obtain negative sample indices with shape (inputs.shape[0], num_samples) using np.random.choice>\n",
    "        \n",
    "        <calculate v_tilde - embeddings of the negative samples (using self._out_layer)>\n",
    "        \n",
    "        <calculate logsigmoid outputs (use F.logsigmoid)>\n",
    "\n",
    "        <return mean loss>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "265oljMhKdBp"
   },
   "source": [
    "Train and visualize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6wz2iRanqzlq"
   },
   "outputs": [],
   "source": [
    "model = NegativeSamplingModel(vocab_size=len(word2index), embedding_dim=32).cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)  \n",
    "\n",
    "negative_samples = 20\n",
    "loss_every_nsteps = 1000\n",
    "total_loss = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for step, batch in enumerate(make_cbow_batches_iter(contexts, window_size=2, batch_size=128)):\n",
    "    <update the learning cycle accordingly>\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    \n",
    "    if step != 0 and step % loss_every_nsteps == 0:\n",
    "        print(\"Step = {}, Avg Loss = {:.4f}, Time = {:.2f}s\".format(step, total_loss / loss_every_nsteps, \n",
    "                                                                    time.time() - start_time))\n",
    "        total_loss = 0\n",
    "        start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CFik_6djvg3F"
   },
   "outputs": [],
   "source": [
    "visualize_embeddings(model.embeddings.weight.data.cpu().numpy(), index2word, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4G2X-TTpzwz"
   },
   "source": [
    "### Structured Word2Vec\n",
    "\n",
    "In the paper [Two/Too Simple Adaptations of Word2Vec for Syntax Problems (2015), Ling, Wang, et al.](https://www.aclweb.org/anthology/N/N15/N15-1142.pdf) \n",
    "two ways of improvement of the embeddings are discussed: *Structured Skip-gram Model* and *Continuous Window Model*:   \n",
    "![](https://i.ibb.co/56w8MC8/Structured-Word2vec.png)  \n",
    "*From Two/Too Simple Adaptations of Word2Vec for Syntax Problems*\n",
    "\n",
    "In contract to the classic word2vec, each context word has its own embedding matrix. The idea is that the words order is meaningful and by learning the order embeddings learn syntax better.\n",
    "\n",
    "The disadvantage of such approach is that you have to learn much more parameters in the model.\n",
    "\n",
    "**Task** Read the paper and implement at least one of the described methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OEazfh1s9eki"
   },
   "outputs": [],
   "source": [
    "class CWindowModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, window_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        <create layers>\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        <apply 'em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4uF6iF6A9uGQ"
   },
   "outputs": [],
   "source": [
    "model = CWindowModel(vocab_size=len(word2index), embedding_dim=32, window_size=2).cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)  \n",
    "\n",
    "loss_every_nsteps = 1000\n",
    "total_loss = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for step, batch in enumerate(make_cbow_batches_iter(contexts, window_size=2, batch_size=128)):\n",
    "    <copy-paste the cycle yet again>\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    \n",
    "    if step != 0 and step % loss_every_nsteps == 0:\n",
    "        print(\"Step = {}, Avg Loss = {:.4f}, Time = {:.2f}s\".format(step, total_loss / loss_every_nsteps, \n",
    "                                                                    time.time() - start_time))\n",
    "        total_loss = 0\n",
    "        start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqDmuu7m_PB5"
   },
   "source": [
    "# Supplementary Materials\n",
    "\n",
    "## To read\n",
    "### Blogs\n",
    "[On word embeddings - Part 1, Sebastian Ruder](http://ruder.io/word-embeddings-1/)  \n",
    "[On word embeddings - Part 2: Approximating the Softmax, Sebastian Ruder](http://ruder.io/word-embeddings-softmax/index.html)  \n",
    "[Word2Vec Tutorial - The Skip-Gram Model, Chris McCormick](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)  \n",
    "[Word2Vec Tutorial Part 2 - Negative Sampling, Chris McCormick](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/) \n",
    "\n",
    "### Papers\n",
    "[Word2vec Parameter Learning Explained (2014), Xin Rong](https://arxiv.org/abs/1411.2738)  \n",
    "[Neural word embedding as implicit matrix factorization (2014), Levy, Omer, and Yoav Goldberg](http://u.cs.biu.ac.il/~nlp/wp-content/uploads/Neural-Word-Embeddings-as-Implicit-Matrix-Factorization-NIPS-2014.pdf)  \n",
    "\n",
    "### Enhancing Embeddings\n",
    "[Two/Too Simple Adaptations of Word2Vec for Syntax Problems (2015), Ling, Wang, et al.](https://www.aclweb.org/anthology/N/N15/N15-1142.pdf)  \n",
    "[Not All Neural Embeddings are Born Equal (2014)](https://arxiv.org/pdf/1410.0718.pdf)  \n",
    "[Retrofitting Word Vectors to Semantic Lexicons (2014), M. Faruqui, et al.](https://arxiv.org/pdf/1411.4166.pdf)  \n",
    "[All-but-the-top: Simple and Effective Postprocessing for Word Representations (2017), Mu, et al.](https://arxiv.org/pdf/1702.01417.pdf)  \n",
    "\n",
    "### Sentence Embeddings\n",
    "[Skip-Thought Vectors (2015), Kiros, et al.](https://arxiv.org/pdf/1506.06726)  \n",
    "\n",
    "### Backpropagation\n",
    "[Backpropagation, Intuitions, cs231n + next parts in the Module 1](http://cs231n.github.io/optimization-2/)   \n",
    "[Calculus on Computational Graphs: Backpropagation, Christopher Olah](http://colah.github.io/posts/2015-08-Backprop/)\n",
    "\n",
    "## To watch\n",
    "[cs224n \"Lecture 2 - Word Vector Representations: word2vec\"](https://www.youtube.com/watch?v=ERibwqs9p38&index=2&list=PLqdrfNEc5QnuV9RwUAhoJcoQvu4Q46Lja&t=0s)  \n",
    "[cs224n \"Lecture 5 - Backpropagation\"](https://www.youtube.com/watch?v=isPiE-DBagM&index=5&list=PLqdrfNEc5QnuV9RwUAhoJcoQvu4Q46Lja&t=0s)   \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "English_Workshop_NN2_NLP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
