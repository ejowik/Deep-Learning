{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d524ad8f-d024-4ba3-8bbc-2435ec3d0dba",
    "_uuid": "7bb30a6f2a18be45a491092a6e2dfcdcce71a2a0"
   },
   "source": [
    "## Preface\n",
    "This notebooks aims to build a CNN + LSTM model.\n",
    "\n",
    "It uses specgrams of wav files(rate 16000) as inputs.\n",
    "\n",
    "\n",
    "## File Structure\n",
    "This script assumes data are stored in following strcuture:\n",
    "\n",
    "speech\n",
    "\n",
    "├── test            \n",
    "\n",
    "│   └── audio #test wavfiles\n",
    "\n",
    "├── train           \n",
    "\n",
    "│   ├── audio #train wavfiles\n",
    "\n",
    "└── model #store models\n",
    "\n",
    "│\n",
    "\n",
    "└── out #store sub.csv\n",
    "\n",
    "## Improve This Script\n",
    "Here are some ways to improve it's performance.\n",
    "1. Use audio data augmentation techniques.\n",
    "2. Create more 'silence' wav files using chop_audio.\n",
    "3. Build deeper CNN or  RNN.\n",
    "4. Train for longer epochs\n",
    "\n",
    "## After Words\n",
    "After the submission the score I'm getting is 0.74, feedback is welcome.\n",
    "\n",
    "Feel free to share your ideas in the comment sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "f7fd8bcb-4451-4d47-bfe8-491c94b3b4eb",
    "_uuid": "712710f20b00f97271136cfeab9937a4c6a2458b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.fftpack import fft\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "from glob import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import gc\n",
    "from scipy.io import wavfile\n",
    "\n",
    "from keras import optimizers, losses, activations, models\n",
    "from keras.layers import Convolution2D, Dense, Input, Flatten, Dropout, MaxPooling2D, BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fb35a2f1-9301-4693-a9ef-9d180b630f05",
    "_uuid": "4b1ba61998e14e15c822c605dbe5961bfed36014"
   },
   "source": [
    "The original sample rate is 16000, and we will keep it the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "dc66e1df-f1eb-4df4-ba1a-65b9f1675953",
    "_uuid": "4cc586519523b28d1d595716d8709ace9f27ac9c"
   },
   "outputs": [],
   "source": [
    "L = 16000\n",
    "legal_labels = 'yes no up down left right on off stop go silence unknown'.split()\n",
    "\n",
    "#src folders\n",
    "root_path = r'.'\n",
    "out_path = r'.'\n",
    "model_path = r'.'\n",
    "train_data_path = os.path.join(root_path, 'data', 'train', 'audio')\n",
    "test_data_path = os.path.join(root_path, 'data', 'test', 'audio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\\\data\\\\train\\\\audio'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e53561e4-1c98-44c0-9245-d87f7957faa5",
    "_uuid": "d9a08781f22e574bb1eb0dc29adeb8dddebc8b51"
   },
   "source": [
    "Here is the log_specgram function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "0fd0b579-8b6f-4253-bf3a-7f75115a42d6",
    "_uuid": "e7ea2c277b6459e532721452ec3cd80d585eae1e"
   },
   "outputs": [],
   "source": [
    "def log_specgram(audio, sample_rate, window_size=20,\n",
    "                 step_size=10, eps=1e-10):\n",
    "    nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "    noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "    freqs, times, spec = signal.spectrogram(audio,\n",
    "                                    fs=sample_rate,\n",
    "                                    window='hann',\n",
    "                                    nperseg=nperseg,\n",
    "                                    noverlap=noverlap,\n",
    "                                    detrend=False)\n",
    "    return freqs, times, np.log(spec.T.astype(np.float32) + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c54cda36-777e-4129-bac1-af2d1ed2706e",
    "_uuid": "5a04e71fe7e66e1a31835feebdfef4c63920faf8"
   },
   "source": [
    "Following is the utility function to grab all wav files inside train data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "956f3150-544d-46ed-b0ec-da1c1fb142b4",
    "_uuid": "964d71a229e9d4560b9118fa1c80804ebf8d6be8"
   },
   "outputs": [],
   "source": [
    "def list_wavs_fname(dirpath, ext='wav'):\n",
    "    print(dirpath)\n",
    "    fpaths = glob(os.path.join(dirpath, r'*/*' + ext))\n",
    "    pat = r'.+\\\\(\\w+)\\\\\\w+\\.' + ext + '$'\n",
    "    labels = []\n",
    "    for fpath in fpaths:\n",
    "        r = re.match(pat, fpath)\n",
    "        if r:\n",
    "            labels.append(r.group(1))\n",
    "    pat = r'.+\\\\(\\w+\\.' + ext + ')$'\n",
    "    fnames = []\n",
    "    for fpath in fpaths:\n",
    "        r = re.match(pat, fpath)\n",
    "        if r:\n",
    "            fnames.append(r.group(1))\n",
    "    return labels, fnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "41025a55-8497-43cf-b316-003af7d9d19f",
    "_uuid": "fc18e87793888952e81a867dd95b1dcc455f9932"
   },
   "source": [
    "__pad_audio__ will pad audios that are less than 16000(1 second) with 0s to make them all have the same length.\n",
    "\n",
    "__chop_audio__ will chop audios that are larger than 16000(eg. wav files in background noises folder) to 16000 in length. In addition, it will create several chunks out of one large wav files given the parameter 'num'.\n",
    "\n",
    "__label_transform__ transform labels into dummies values. It's used in combination with softmax to predict the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "200c34a1-851a-4447-9ff7-b4e541f090c6",
    "_uuid": "94e40aef3899acfd3ed85557caa66fee5dd47db2"
   },
   "outputs": [],
   "source": [
    "def pad_audio(samples):\n",
    "    if len(samples) >= L: return samples\n",
    "    else: return np.pad(samples, pad_width=(L - len(samples), 0), mode='constant', constant_values=(0, 0))\n",
    "\n",
    "def chop_audio(samples, L=16000, num=20):\n",
    "    for i in range(num):\n",
    "        beg = np.random.randint(0, len(samples) - L)\n",
    "        yield samples[beg: beg + L]\n",
    "\n",
    "def label_transform(labels):\n",
    "    nlabels = []\n",
    "    for label in labels:\n",
    "        if label == '_background_noise_':\n",
    "            nlabels.append('silence')\n",
    "        elif label not in legal_labels:\n",
    "            nlabels.append('unknown')\n",
    "        else:\n",
    "            nlabels.append(label)\n",
    "    return pd.get_dummies(pd.Series(nlabels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dae2a45a-f7ab-4e84-bc73-688eda6eca8e",
    "_uuid": "267314ef41c459c8b6ab903d721980fdd62b4106"
   },
   "source": [
    "Next, we use functions declared above to generate x_train and y_train.\n",
    "label_index is the index used by pandas to create dummy values, we need to save it for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "4c8d9fdf-ea3e-45fa-b7ef-52542c70b9db",
    "_uuid": "81bc9722dfb036c73721ae44829d429489662e75",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\data\\train\\audio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64702it [03:26, 311.67it/s]C:\\Users\\agama\\AppData\\Local\\Temp\\ipykernel_18204\\1534510680.py:8: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  sample_rate, samples = wavfile.read(os.path.join(train_data_path, label, fname))\n",
      "64727it [03:26, 313.28it/s]\n"
     ]
    }
   ],
   "source": [
    "labels, fnames = list_wavs_fname(train_data_path)\n",
    "\n",
    "new_sample_rate = 16000\n",
    "y_train = []\n",
    "x_train = []\n",
    "\n",
    "for label, fname in tqdm(zip(labels, fnames)):\n",
    "    sample_rate, samples = wavfile.read(os.path.join(train_data_path, label, fname))\n",
    "    samples = pad_audio(samples)\n",
    "    if len(samples) > 16000:\n",
    "        n_samples = chop_audio(samples)\n",
    "    else: n_samples = [samples]\n",
    "    for samples in n_samples:\n",
    "        resampled = signal.resample(samples, int(new_sample_rate / sample_rate * samples.shape[0]))\n",
    "        _, _, specgram = log_specgram(resampled, sample_rate=new_sample_rate)\n",
    "        y_train.append(label)\n",
    "        x_train.append(specgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.array(x_train)\n",
    "# x_train = x_train.reshape(tuple(list(x_train.shape) + [1]))\n",
    "y_train = label_transform(y_train)\n",
    "label_index = y_train.columns.values\n",
    "y_train = y_train.values\n",
    "y_train = np.array(y_train)\n",
    "del labels, fnames\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64841, 99, 161)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64841, 12)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "56921cf3-1269-4b29-876d-abdd31eb150a",
    "_uuid": "a87a77b76c42da61ca0bec395c71bef795a9e928"
   },
   "source": [
    "RNN declared below.\n",
    "The specgram created will be of shape (99, 161), but in order to fit into Conv2D layer, we need to reshape it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64841, 99, 161)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "b97e8887-b593-4d88-95c8-fc8f1dd5ca72",
    "_uuid": "60af394ad8e91fb868ea32dbb6ac6a725b5935c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " the_input (InputLayer)      [(None, 99, 161)]         0         \n",
      "                                                                 \n",
      " conv_1 (Conv1D)             (None, 23, 256)           412416    \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 23, 256)          1024      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 23, 256)           0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 23, 256)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 23, 128)           197120    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " fc (Dense)                  (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " softmax (Dense)             (None, 12)                780       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 751,180\n",
      "Trainable params: 750,668\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, Bidirectional, TimeDistributed, Conv1D, ZeroPadding1D, GRU\n",
    "from tensorflow.keras.layers import Lambda, Input, Dropout, Masking, BatchNormalization, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def cnn_lstm(input_dim, output_dim, dropout=0.2, n_layers=1):\n",
    "\n",
    "#     # Input data type\n",
    "    dtype = 'float32'\n",
    "\n",
    "    # ---- Network model ----\n",
    "    input_data = Input(name='the_input', shape=input_dim, dtype=dtype)\n",
    "\n",
    "    # 1 x 1D convolutional layers with strides 4\n",
    "    x = Conv1D(filters=256, kernel_size=10, strides=4, name='conv_1')(input_data)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dropout, name='dropout_1')(x)\n",
    "        \n",
    "    x = LSTM(128, activation='relu', return_sequences=True,\n",
    "             dropout=dropout, name='lstm_1')(x)\n",
    "    x = LSTM(128, activation='relu', return_sequences=False,\n",
    "             dropout=dropout, name='lstm_2')(x)\n",
    "\n",
    "#     # 1 fully connected layer DNN ReLu with default 20% dropout\n",
    "    x = Dense(units=64, activation='relu', name='fc')(x)\n",
    "    x = Dropout(dropout, name='dropout_2')(x)\n",
    "\n",
    "    # Output layer with softmax\n",
    "    y_pred = Dense(units=output_dim, activation='softmax', name='softmax')(x)\n",
    "\n",
    "    network_model = Model(inputs=input_data, outputs=y_pred)\n",
    "    \n",
    "    return network_model\n",
    "\n",
    "input_dim = (99, 161)\n",
    "classes = len(legal_labels)\n",
    "K.clear_session()\n",
    "model = cnn_lstm(input_dim, classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "507/507 [==============================] - 196s 370ms/step - loss: 1.2806 - accuracy: 0.6427\n",
      "Epoch 2/10\n",
      "507/507 [==============================] - 153s 303ms/step - loss: 0.7383 - accuracy: 0.7672\n",
      "Epoch 3/10\n",
      "507/507 [==============================] - 149s 294ms/step - loss: 0.5107 - accuracy: 0.8398\n",
      "Epoch 4/10\n",
      "507/507 [==============================] - 143s 283ms/step - loss: 0.4107 - accuracy: 0.8719\n",
      "Epoch 5/10\n",
      "507/507 [==============================] - 140s 276ms/step - loss: 0.3504 - accuracy: 0.8917\n",
      "Epoch 6/10\n",
      "507/507 [==============================] - 136s 268ms/step - loss: 0.3091 - accuracy: 0.9053\n",
      "Epoch 7/10\n",
      "507/507 [==============================] - 143s 282ms/step - loss: 0.2783 - accuracy: 0.9163\n",
      "Epoch 8/10\n",
      "507/507 [==============================] - 139s 274ms/step - loss: 0.2518 - accuracy: 0.9220\n",
      "Epoch 9/10\n",
      "507/507 [==============================] - 137s 271ms/step - loss: 0.2333 - accuracy: 0.9279\n",
      "Epoch 10/10\n",
      "507/507 [==============================] - 135s 267ms/step - loss: 0.2165 - accuracy: 0.9335\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "sgd = SGD(lr=0.00001, clipnorm=1.0)\n",
    "adam = Adam(lr=1e-4, clipnorm=1.0)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=128, epochs=10,\n",
    "#                     validation_data=(X_val, Y_val),\n",
    "                    callbacks=[TensorBoard(log_dir='logs',\n",
    "                                           histogram_freq=1,\n",
    "                                           update_freq='epoch')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " the_input (InputLayer)      [(None, 99, 161)]         0         \n",
      "                                                                 \n",
      " conv_1 (Conv1D)             (None, 23, 256)           412416    \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 23, 256)          1024      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 23, 256)           0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 23, 256)           0         \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 23, 128)           148224    \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (None, 128)               99072     \n",
      "                                                                 \n",
      " fc (Dense)                  (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " softmax (Dense)             (None, 12)                780       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669,772\n",
      "Trainable params: 669,260\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def cnn_gru(input_dim, output_dim, dropout=0.2, n_layers=1):\n",
    "\n",
    "#     # Input data type\n",
    "    dtype = 'float32'\n",
    "\n",
    "    # ---- Network model ----\n",
    "    input_data = Input(name='the_input', shape=input_dim, dtype=dtype)\n",
    "\n",
    "    # 1 x 1D convolutional layers with strides 4\n",
    "    x = Conv1D(filters=256, kernel_size=10, strides=4, name='conv_1')(input_data)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dropout, name='dropout_1')(x)\n",
    "        \n",
    "    x = GRU(128, activation='relu', return_sequences=True,\n",
    "             dropout=dropout, name='gru_1')(x)\n",
    "    x = GRU(128, activation='relu', return_sequences=False,\n",
    "             dropout=dropout, name='gru_2')(x)\n",
    "\n",
    "#     # 1 fully connected layer DNN ReLu with default 20% dropout\n",
    "    x = Dense(units=64, activation='relu', name='fc')(x)\n",
    "    x = Dropout(dropout, name='dropout_2')(x)\n",
    "\n",
    "    # Output layer with softmax\n",
    "    y_pred = Dense(units=output_dim, activation='softmax', name='softmax')(x)\n",
    "\n",
    "    network_model = Model(inputs=input_data, outputs=y_pred)\n",
    "    \n",
    "    return network_model\n",
    "\n",
    "input_dim = (99, 161)\n",
    "classes = len(legal_labels)\n",
    "K.clear_session()\n",
    "model = cnn_gru(input_dim, classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "507/507 [==============================] - 120s 220ms/step - loss: 1.3099 - accuracy: 0.6452\n",
      "Epoch 2/10\n",
      "507/507 [==============================] - 126s 249ms/step - loss: 0.6673 - accuracy: 0.7858\n",
      "Epoch 3/10\n",
      "507/507 [==============================] - 136s 267ms/step - loss: 0.4485 - accuracy: 0.8585\n",
      "Epoch 4/10\n",
      "507/507 [==============================] - 132s 260ms/step - loss: 0.3613 - accuracy: 0.8867\n",
      "Epoch 5/10\n",
      "507/507 [==============================] - 126s 249ms/step - loss: 0.3085 - accuracy: 0.9049\n",
      "Epoch 6/10\n",
      "507/507 [==============================] - 135s 266ms/step - loss: 0.2763 - accuracy: 0.9152\n",
      "Epoch 7/10\n",
      "507/507 [==============================] - 131s 257ms/step - loss: 0.2523 - accuracy: 0.9234\n",
      "Epoch 8/10\n",
      "507/507 [==============================] - 132s 260ms/step - loss: 0.2348 - accuracy: 0.9281\n",
      "Epoch 9/10\n",
      "507/507 [==============================] - 126s 248ms/step - loss: 0.2145 - accuracy: 0.9349\n",
      "Epoch 10/10\n",
      "507/507 [==============================] - 127s 252ms/step - loss: 0.2038 - accuracy: 0.9374\n"
     ]
    }
   ],
   "source": [
    "sgd = SGD(lr=0.00001, clipnorm=1.0)\n",
    "adam = Adam(lr=1e-4, clipnorm=1.0)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=128, epochs=10,\n",
    "#                     validation_data=(X_val, Y_val),\n",
    "                    callbacks=[TensorBoard(log_dir='logs',\n",
    "                                           histogram_freq=1,\n",
    "                                           update_freq='epoch')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " the_input (InputLayer)      [(None, 99, 161)]         0         \n",
      "                                                                 \n",
      " conv_1 (Conv1D)             (None, 23, 256)           412416    \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 23, 256)          1024      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 23, 256)           0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 23, 256)           0         \n",
      "                                                                 \n",
      " rnn_1 (SimpleRNN)           (None, 23, 128)           49280     \n",
      "                                                                 \n",
      " rnn_2 (SimpleRNN)           (None, 128)               32896     \n",
      "                                                                 \n",
      " fc (Dense)                  (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " softmax (Dense)             (None, 12)                780       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 504,652\n",
      "Trainable params: 504,140\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def cnn_rnn(input_dim, output_dim, dropout=0.2, n_layers=1):\n",
    "\n",
    "#     # Input data type\n",
    "    dtype = 'float32'\n",
    "\n",
    "    # ---- Network model ----\n",
    "    input_data = Input(name='the_input', shape=input_dim, dtype=dtype)\n",
    "\n",
    "    # 1 x 1D convolutional layers with strides 4\n",
    "    x = Conv1D(filters=256, kernel_size=10, strides=4, name='conv_1')(input_data)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dropout, name='dropout_1')(x)\n",
    "        \n",
    "    x = SimpleRNN(128, activation='relu', return_sequences=True,\n",
    "             dropout=dropout, name='rnn_1')(x)\n",
    "    x = SimpleRNN(128, activation='relu', return_sequences=False,\n",
    "             dropout=dropout, name='rnn_2')(x)\n",
    "\n",
    "#     # 1 fully connected layer DNN ReLu with default 20% dropout\n",
    "    x = Dense(units=64, activation='relu', name='fc')(x)\n",
    "    x = Dropout(dropout, name='dropout_2')(x)\n",
    "\n",
    "    # Output layer with softmax\n",
    "    y_pred = Dense(units=output_dim, activation='softmax', name='softmax')(x)\n",
    "\n",
    "    network_model = Model(inputs=input_data, outputs=y_pred)\n",
    "    \n",
    "    return network_model\n",
    "\n",
    "input_dim = (99, 161)\n",
    "classes = len(legal_labels)\n",
    "K.clear_session()\n",
    "model = cnn_rnn(input_dim, classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "507/507 [==============================] - 100s 179ms/step - loss: 1.5228 - accuracy: 0.6055\n",
      "Epoch 2/10\n",
      "507/507 [==============================] - 90s 177ms/step - loss: 1.2318 - accuracy: 0.6368\n",
      "Epoch 3/10\n",
      "507/507 [==============================] - 112s 221ms/step - loss: 1.0549 - accuracy: 0.6668\n",
      "Epoch 4/10\n",
      "507/507 [==============================] - 124s 245ms/step - loss: 0.8947 - accuracy: 0.7042\n",
      "Epoch 5/10\n",
      "507/507 [==============================] - 121s 239ms/step - loss: 0.7347 - accuracy: 0.7618\n",
      "Epoch 6/10\n",
      "507/507 [==============================] - 116s 228ms/step - loss: 0.6109 - accuracy: 0.8045\n",
      "Epoch 7/10\n",
      "507/507 [==============================] - 127s 250ms/step - loss: 0.5303 - accuracy: 0.8295\n",
      "Epoch 8/10\n",
      "507/507 [==============================] - 123s 243ms/step - loss: 0.4642 - accuracy: 0.8544\n",
      "Epoch 9/10\n",
      "507/507 [==============================] - 120s 237ms/step - loss: 0.4164 - accuracy: 0.8716\n",
      "Epoch 10/10\n",
      "507/507 [==============================] - 120s 237ms/step - loss: 0.3858 - accuracy: 0.8835\n"
     ]
    }
   ],
   "source": [
    "sgd = SGD(lr=0.00001, clipnorm=1.0)\n",
    "adam = Adam(lr=1e-4, clipnorm=1.0)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=128, epochs=10,\n",
    "#                     validation_data=(X_val, Y_val),\n",
    "                    callbacks=[TensorBoard(log_dir='logs',\n",
    "                                           histogram_freq=1,\n",
    "                                           update_freq='epoch')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " the_input (InputLayer)      [(None, 99, 161)]         0         \n",
      "                                                                 \n",
      " conv_1 (Conv1D)             (None, 23, 256)           412416    \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 23, 256)          1024      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 23, 256)           0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 23, 256)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 23, 128)           197120    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " fc (Dense)                  (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " softmax (Dense)             (None, 12)                780       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 751,180\n",
      "Trainable params: 750,668\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, Bidirectional, TimeDistributed, Conv1D, ZeroPadding1D, GRU\n",
    "from tensorflow.keras.layers import Lambda, Input, Dropout, Masking, BatchNormalization, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def cnn_lstm(input_dim, output_dim, dropout=0.2, n_layers=1, method=GRU):\n",
    "\n",
    "#     # Input data type\n",
    "    dtype = 'float32'\n",
    "\n",
    "    # ---- Network model ----\n",
    "    input_data = Input(name='the_input', shape=input_dim, dtype=dtype)\n",
    "\n",
    "    # 1 x 1D convolutional layers with strides 4\n",
    "    x = Conv1D(filters=256, kernel_size=10, strides=4, name='conv_1')(input_data)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dropout, name='dropout_1')(x)\n",
    "        \n",
    "    x = method(128, activation='relu', return_sequences=True,\n",
    "             dropout=dropout, name='lstm_1')(x)\n",
    "    x = method(128, activation='relu', return_sequences=False,\n",
    "             dropout=dropout, name='lstm_2')(x)\n",
    "\n",
    "#     # 1 fully connected layer DNN ReLu with default 20% dropout\n",
    "    x = Dense(units=64, activation='relu', name='fc')(x)\n",
    "    x = Dropout(dropout, name='dropout_2')(x)\n",
    "\n",
    "    # Output layer with softmax\n",
    "    y_pred = Dense(units=output_dim, activation='softmax', name='softmax')(x)\n",
    "\n",
    "    network_model = Model(inputs=input_data, outputs=y_pred)\n",
    "    \n",
    "    return network_model\n",
    "\n",
    "input_dim = (99, 161)\n",
    "classes = len(legal_labels)\n",
    "K.clear_session()\n",
    "model = cnn_lstm(input_dim, classes, method=LSTM)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join(model_path, 'rnn.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n",
      "['rnn.model', '__output__.json', 'logs', '__notebook__.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('rnn.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "060811f5-34ac-4fc2-92ba-c4276606c2a0",
    "_uuid": "2e3fa8d9706f47e69d0b74afcb68280f8a5de706"
   },
   "source": [
    "Test data is way too large to fit in RAM, we need to process them one by one.\n",
    "Generator test_data_generator will create batches of test wav files to feed into CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "7dfe0801-a636-4123-8367-ab2f19c97800",
    "_uuid": "646b6bcfbde7eae53cd8822b8838c575859e51ce"
   },
   "outputs": [],
   "source": [
    "def test_data_generator(batch=16):\n",
    "    fpaths = glob(os.path.join(test_data_path, '*wav'))\n",
    "    i = 0\n",
    "    for path in fpaths:\n",
    "        if i == 0:\n",
    "            imgs = []\n",
    "            fnames = []\n",
    "        i += 1\n",
    "        rate, samples = wavfile.read(path)\n",
    "        samples = pad_audio(samples)\n",
    "        resampled = signal.resample(samples, int(new_sample_rate / rate * samples.shape[0]))\n",
    "        _, _, specgram = log_specgram(resampled, sample_rate=new_sample_rate)\n",
    "        imgs.append(specgram)\n",
    "        fnames.append(path.split('\\\\')[-1])\n",
    "        if i == batch:\n",
    "            i = 0\n",
    "            imgs = np.array(imgs)\n",
    "            imgs = imgs.reshape(tuple(list(imgs.shape) + [1]))\n",
    "            yield fnames, imgs\n",
    "    if i < batch:\n",
    "        imgs = np.array(imgs)\n",
    "        imgs = imgs.reshape(tuple(list(imgs.shape) + [1]))\n",
    "        yield fnames, imgs\n",
    "    raise StopIteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "22992a27-deda-4a35-b34c-4aa87ad173ec",
    "_uuid": "c6d2516a6d5bd3c6a108d7e28565edaa65830958"
   },
   "source": [
    "We use the trained model to predict the test data's labels.\n",
    "However, since Kaggle doesn't provide test data, the following sections won't be executed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "7fa8feb8-236e-46c5-8432-014f7e27484d",
    "_uuid": "56194039cac16f5d86a322e67641cbeafda9857d"
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'imgs' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3f4af0a28322>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_data_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mpredicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mpredicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-8857457ef1bc>\u001b[0m in \u001b[0;36mtest_data_generator\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mfnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mfnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'imgs' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# exit() #delete this\n",
    "del x_train, y_train\n",
    "gc.collect()\n",
    "\n",
    "index = []\n",
    "results = []\n",
    "for fnames, imgs in test_data_generator(batch=32):\n",
    "    predicts = model.predict(imgs)\n",
    "    predicts = np.argmax(predicts, axis=1)\n",
    "    predicts = [label_index[p] for p in predicts]\n",
    "    index.extend(fnames)\n",
    "    results.extend(predicts)\n",
    "\n",
    "df = pd.DataFrame(columns=['fname', 'label'])\n",
    "df['fname'] = index\n",
    "df['label'] = results\n",
    "df.to_csv(os.path.join(out_path, 'sub.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
